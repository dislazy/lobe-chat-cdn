"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[8303],{8303:e=>{e.exports=JSON.parse('{"01-ai/yi-1.5-34b-chat":{"description":"Yi 1.5, das neueste Open-Source-Fine-Tuning-Modell mit 34 Milliarden Parametern, unterst\xfctzt verschiedene Dialogszenarien mit hochwertigen Trainingsdaten, die auf menschliche Pr\xe4ferenzen abgestimmt sind."},"01-ai/yi-1.5-9b-chat":{"description":"Yi 1.5, das neueste Open-Source-Fine-Tuning-Modell mit 9 Milliarden Parametern, unterst\xfctzt verschiedene Dialogszenarien mit hochwertigen Trainingsdaten, die auf menschliche Pr\xe4ferenzen abgestimmt sind."},"360/deepseek-r1":{"description":"【360 Deployment Version】DeepSeek-R1 nutzt in der Nachtrainingsphase umfangreiche Techniken des verst\xe4rkenden Lernens, um die Modellinferenzf\xe4higkeit erheblich zu verbessern, selbst bei minimalen gekennzeichneten Daten. In Aufgaben wie Mathematik, Programmierung und nat\xfcrlicher Sprachverarbeitung erreicht es Leistungen auf Augenh\xf6he mit der offiziellen Version von OpenAI o1."},"360gpt-pro":{"description":"360GPT Pro ist ein wichtiger Bestandteil der 360 AI-Modellreihe und erf\xfcllt mit seiner effizienten Textverarbeitungsf\xe4higkeit vielf\xe4ltige Anwendungen der nat\xfcrlichen Sprache, unterst\xfctzt das Verst\xe4ndnis langer Texte und Mehrfachdialoge."},"360gpt-pro-trans":{"description":"Ein auf \xdcbersetzungen spezialisiertes Modell, das durch tiefes Feintuning optimiert wurde und f\xfchrende \xdcbersetzungsergebnisse liefert."},"360gpt-turbo":{"description":"360GPT Turbo bietet leistungsstarke Berechnungs- und Dialogf\xe4higkeiten, mit hervorragendem semantischen Verst\xe4ndnis und Generierungseffizienz, und ist die ideale intelligente Assistentenl\xf6sung f\xfcr Unternehmen und Entwickler."},"360gpt-turbo-responsibility-8k":{"description":"360GPT Turbo Responsibility 8K betont semantische Sicherheit und verantwortungsbewusste Ausrichtung, speziell f\xfcr Anwendungen mit hohen Anforderungen an die Inhaltssicherheit konzipiert, um die Genauigkeit und Robustheit der Benutzererfahrung zu gew\xe4hrleisten."},"360gpt2-o1":{"description":"360gpt2-o1 verwendet Baumsuche zur Konstruktion von Denkketten und f\xfchrt einen Reflexionsmechanismus ein, der durch verst\xe4rkendes Lernen trainiert wird. Das Modell verf\xfcgt \xfcber die F\xe4higkeit zur Selbstreflexion und Fehlerkorrektur."},"360gpt2-pro":{"description":"360GPT2 Pro ist ein fortschrittliches Modell zur Verarbeitung nat\xfcrlicher Sprache, das von der 360 Company entwickelt wurde und \xfcber au\xdfergew\xf6hnliche Textgenerierungs- und Verst\xe4ndnisf\xe4higkeiten verf\xfcgt, insbesondere im Bereich der Generierung und Kreativit\xe4t, und in der Lage ist, komplexe Sprachumwandlungs- und Rollendarstellungsaufgaben zu bew\xe4ltigen."},"360zhinao2-o1":{"description":"360zhinao2-o1 verwendet Baumsuche zur Konstruktion von Denkketten und f\xfchrt einen Reflexionsmechanismus ein, der durch verst\xe4rkendes Lernen trainiert wird. Das Modell verf\xfcgt \xfcber die F\xe4higkeit zur Selbstreflexion und Fehlerkorrektur."},"4.0Ultra":{"description":"Spark4.0 Ultra ist die leistungsst\xe4rkste Version der Spark-Gro\xdfmodellreihe, die die Online-Suchverbindung aktualisiert und die F\xe4higkeit zur Textverst\xe4ndnis und -zusammenfassung verbessert. Es ist eine umfassende L\xf6sung zur Steigerung der B\xfcroproduktivit\xe4t und zur genauen Reaktion auf Anforderungen und ein f\xfchrendes intelligentes Produkt in der Branche."},"AnimeSharp":{"description":"AnimeSharp (auch bekannt als „4x‑AnimeSharp“) ist ein von Kim2091 auf Basis der ESRGAN-Architektur entwickeltes Open-Source-Superaufl\xf6sungsmodell, das sich auf die Vergr\xf6\xdferung und Sch\xe4rfung von Anime-Stil-Bildern spezialisiert hat. Es wurde im Februar 2022 von „4x-TextSharpV1“ umbenannt und war urspr\xfcnglich auch f\xfcr Textbilder geeignet, wurde jedoch f\xfcr Anime-Inhalte erheblich optimiert."},"Baichuan2-Turbo":{"description":"Verwendet Suchverbesserungstechnologie, um eine umfassende Verkn\xfcpfung zwischen gro\xdfen Modellen und Fachwissen sowie Wissen aus dem gesamten Internet zu erm\xf6glichen. Unterst\xfctzt das Hochladen von Dokumenten wie PDF, Word und die Eingabe von URLs, um Informationen zeitnah und umfassend zu erhalten, mit genauen und professionellen Ergebnissen."},"Baichuan3-Turbo":{"description":"F\xfcr h\xe4ufige Unternehmensszenarien optimiert, mit erheblichen Leistungssteigerungen und einem hohen Preis-Leistungs-Verh\xe4ltnis. Im Vergleich zum Baichuan2-Modell wurde die Inhaltserstellung um 20 %, die Wissensabfrage um 17 % und die Rollenspiel-F\xe4higkeit um 40 % verbessert. Die Gesamtleistung \xfcbertrifft die von GPT-3.5."},"Baichuan3-Turbo-128k":{"description":"Verf\xfcgt \xfcber ein 128K Ultra-Langkontextfenster, optimiert f\xfcr h\xe4ufige Unternehmensszenarien, mit erheblichen Leistungssteigerungen und einem hohen Preis-Leistungs-Verh\xe4ltnis. Im Vergleich zum Baichuan2-Modell wurde die Inhaltserstellung um 20 %, die Wissensabfrage um 17 % und die Rollenspiel-F\xe4higkeit um 40 % verbessert. Die Gesamtleistung \xfcbertrifft die von GPT-3.5."},"Baichuan4":{"description":"Das Modell hat die h\xf6chste F\xe4higkeit im Inland und \xfcbertrifft ausl\xe4ndische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativer Generierung. Es verf\xfcgt auch \xfcber branchenf\xfchrende multimodale F\xe4higkeiten und zeigt in mehreren autoritativen Bewertungsbenchmarks hervorragende Leistungen."},"Baichuan4-Air":{"description":"Das Modell hat die h\xf6chste Leistungsf\xe4higkeit im Inland und \xfcbertrifft ausl\xe4ndische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativen Generierungen auf Chinesisch. Es verf\xfcgt auch \xfcber branchenf\xfchrende multimodale F\xe4higkeiten und zeigt in mehreren anerkannten Bewertungsbenchmarks hervorragende Leistungen."},"Baichuan4-Turbo":{"description":"Das Modell hat die h\xf6chste Leistungsf\xe4higkeit im Inland und \xfcbertrifft ausl\xe4ndische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativen Generierungen auf Chinesisch. Es verf\xfcgt auch \xfcber branchenf\xfchrende multimodale F\xe4higkeiten und zeigt in mehreren anerkannten Bewertungsbenchmarks hervorragende Leistungen."},"ByteDance-Seed/Seed-OSS-36B-Instruct":{"description":"Seed-OSS ist eine von ByteDance Seed entwickelten Reihe von Open-Source-Gro\xdfsprachmodellen, die speziell f\xfcr leistungsstarke Langkontextverarbeitung, Schlussfolgerungen, Agenten und allgemeine F\xe4higkeiten konzipiert sind. Das Modell Seed-OSS-36B-Instruct aus dieser Reihe ist ein feinabgestimmtes Instruktionsmodell mit 36 Milliarden Parametern, das nativ extrem lange Kontextl\xe4ngen unterst\xfctzt, wodurch es in der Lage ist, umfangreiche Dokumente oder komplexe Codebasen auf einmal zu verarbeiten. Dieses Modell ist besonders f\xfcr Schlussfolgerungen, Codegenerierung und Agentenaufgaben (wie Werkzeugnutzung) optimiert und bewahrt dabei eine ausgewogene und hervorragende allgemeine Leistungsf\xe4higkeit. Ein herausragendes Merkmal dieses Modells ist die Funktion \\"Thinking Budget\\", die es Nutzern erm\xf6glicht, die Schlussfolgerungsl\xe4nge flexibel anzupassen, um die Effizienz in praktischen Anwendungen effektiv zu steigern."},"DeepSeek-R1":{"description":"Ein hochmodernes, effizientes LLM, das sich auf Schlussfolgerungen, Mathematik und Programmierung spezialisiert hat."},"DeepSeek-R1-Distill-Llama-70B":{"description":"DeepSeek R1 – das gr\xf6\xdfere und intelligentere Modell im DeepSeek-Paket – wurde in die Llama 70B-Architektur destilliert. Basierend auf Benchmark-Tests und menschlicher Bewertung ist dieses Modell intelligenter als das urspr\xfcngliche Llama 70B, insbesondere bei Aufgaben, die mathematische und faktische Genauigkeit erfordern."},"DeepSeek-R1-Distill-Qwen-1.5B":{"description":"Das DeepSeek-R1-Distill-Modell basiert auf Qwen2.5-Math-1.5B und optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking."},"DeepSeek-R1-Distill-Qwen-14B":{"description":"Das DeepSeek-R1-Distill-Modell basiert auf Qwen2.5-14B und optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking."},"DeepSeek-R1-Distill-Qwen-32B":{"description":"Die DeepSeek-R1-Serie optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten, das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking und \xfcbertrifft das Niveau von OpenAI-o1-mini."},"DeepSeek-R1-Distill-Qwen-7B":{"description":"Das DeepSeek-R1-Distill-Modell basiert auf Qwen2.5-Math-7B und optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking."},"DeepSeek-V3":{"description":"DeepSeek-V3 ist ein von der DeepSeek Company entwickeltes MoE-Modell. Die Ergebnisse von DeepSeek-V3 \xfcbertreffen die anderer Open-Source-Modelle wie Qwen2.5-72B und Llama-3.1-405B und stehen in der Leistung auf Augenh\xf6he mit den weltweit f\xfchrenden Closed-Source-Modellen GPT-4o und Claude-3.5-Sonnet."},"DeepSeek-V3-1":{"description":"DeepSeek V3.1: Das n\xe4chste Generation Inferenzmodell, das komplexe Schlussfolgerungen und vernetztes Denken verbessert und sich f\xfcr Aufgaben eignet, die eine tiefgehende Analyse erfordern."},"DeepSeek-V3-Fast":{"description":"Modellanbieter: sophnet-Plattform. DeepSeek V3 Fast ist die Hochgeschwindigkeitsversion mit hohem TPS des DeepSeek V3 0324 Modells, voll funktionsf\xe4hig ohne Quantisierung, mit st\xe4rkerer Code- und mathematischer Leistungsf\xe4higkeit und schnellerer Reaktionszeit!"},"DeepSeek-V3.1":{"description":"DeepSeek-V3.1 - Nicht-Denkmodus; DeepSeek-V3.1 ist ein neu eingef\xfchrtes hybrides Inferenzmodell von DeepSeek, das zwei Inferenzmodi unterst\xfctzt: Denk- und Nicht-Denkmodus, mit h\xf6herer Denkeffizienz im Vergleich zu DeepSeek-R1-0528. Durch Post-Training-Optimierung wurde die Leistung bei Agenten-Werkzeugnutzung und Agentenaufgaben deutlich verbessert."},"DeepSeek-V3.1-Fast":{"description":"DeepSeek V3.1 Fast ist die Hochgeschwindigkeitsversion von DeepSeek V3.1 mit hoher TPS. Hybrid-Denkmodus: Durch \xc4nderung der Chat-Vorlage kann ein Modell sowohl Denk- als auch Nicht-Denkmodus gleichzeitig unterst\xfctzen. Intelligenterer Werkzeugaufruf: Durch Post-Training-Optimierung wurde die Leistung des Modells bei Werkzeugnutzung und Agentenaufgaben signifikant verbessert."},"DeepSeek-V3.1-Think":{"description":"DeepSeek-V3.1 - Denkmodus; DeepSeek-V3.1 ist ein neu eingef\xfchrtes hybrides Inferenzmodell von DeepSeek, das zwei Inferenzmodi unterst\xfctzt: Denk- und Nicht-Denkmodus, mit h\xf6herer Denkeffizienz im Vergleich zu DeepSeek-R1-0528. Durch Post-Training-Optimierung wurde die Leistung bei Agenten-Werkzeugnutzung und Agentenaufgaben deutlich verbessert."},"DeepSeek-V3.2-Exp":{"description":"DeepSeek V3.2 ist das neueste universelle Gro\xdfmodell von DeepSeek, das eine hybride Inferenzarchitektur unterst\xfctzt und \xfcber st\xe4rkere Agentenf\xe4higkeiten verf\xfcgt."},"DeepSeek-V3.2-Exp-Think":{"description":"DeepSeek V3.2 Denkmodus. Bevor die endg\xfcltige Antwort ausgegeben wird, gibt das Modell zun\xe4chst eine Gedankenkette aus, um die Genauigkeit der finalen Antwort zu verbessern."},"Doubao-lite-128k":{"description":"Doubao-lite bietet extrem schnelle Reaktionszeiten und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis, um Kunden in verschiedenen Szenarien flexiblere Optionen zu bieten. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 128k."},"Doubao-lite-32k":{"description":"Doubao-lite bietet extrem schnelle Reaktionszeiten und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis, um Kunden in verschiedenen Szenarien flexiblere Optionen zu bieten. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 32k."},"Doubao-lite-4k":{"description":"Doubao-lite bietet extrem schnelle Reaktionszeiten und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis, um Kunden in verschiedenen Szenarien flexiblere Optionen zu bieten. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 4k."},"Doubao-pro-128k":{"description":"Das leistungsst\xe4rkste Hauptmodell, geeignet f\xfcr komplexe Aufgaben. Es erzielt hervorragende Ergebnisse in Szenarien wie Referenzfragen, Zusammenfassungen, kreatives Schreiben, Textklassifikation und Rollenspielen. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 128k."},"Doubao-pro-32k":{"description":"Das leistungsst\xe4rkste Hauptmodell, geeignet f\xfcr komplexe Aufgaben. Es erzielt hervorragende Ergebnisse in Szenarien wie Referenzfragen, Zusammenfassungen, kreatives Schreiben, Textklassifikation und Rollenspielen. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 32k."},"Doubao-pro-4k":{"description":"Das leistungsst\xe4rkste Hauptmodell, geeignet f\xfcr komplexe Aufgaben. Es erzielt hervorragende Ergebnisse in Szenarien wie Referenzfragen, Zusammenfassungen, kreatives Schreiben, Textklassifikation und Rollenspielen. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 4k."},"DreamO":{"description":"DreamO ist ein von ByteDance und der Peking-Universit\xe4t gemeinsam entwickeltes Open-Source-Bildgenerierungsmodell zur individuellen Anpassung, das durch eine einheitliche Architektur Multitasking-Bildgenerierung unterst\xfctzt. Es verwendet eine effiziente kombinierte Modellierungsmethode, um basierend auf vom Nutzer angegebenen Identit\xe4t, Motiv, Stil, Hintergrund und weiteren Bedingungen hochgradig konsistente und ma\xdfgeschneiderte Bilder zu erzeugen."},"ERNIE-3.5-128K":{"description":"Das von Baidu entwickelte Flaggschiff-Modell f\xfcr gro\xdfangelegte Sprachverarbeitung, das eine riesige Menge an chinesischen und englischen Texten abdeckt. Es verf\xfcgt \xfcber starke allgemeine F\xe4higkeiten und kann die meisten Anforderungen an Dialogfragen, kreative Generierung und Anwendungsf\xe4lle von Plugins erf\xfcllen. Es unterst\xfctzt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualit\xe4t der Antwortinformationen zu gew\xe4hrleisten."},"ERNIE-3.5-8K":{"description":"Das von Baidu entwickelte Flaggschiff-Modell f\xfcr gro\xdfangelegte Sprachverarbeitung, das eine riesige Menge an chinesischen und englischen Texten abdeckt. Es verf\xfcgt \xfcber starke allgemeine F\xe4higkeiten und kann die meisten Anforderungen an Dialogfragen, kreative Generierung und Anwendungsf\xe4lle von Plugins erf\xfcllen. Es unterst\xfctzt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualit\xe4t der Antwortinformationen zu gew\xe4hrleisten."},"ERNIE-3.5-8K-Preview":{"description":"Das von Baidu entwickelte Flaggschiff-Modell f\xfcr gro\xdfangelegte Sprachverarbeitung, das eine riesige Menge an chinesischen und englischen Texten abdeckt. Es verf\xfcgt \xfcber starke allgemeine F\xe4higkeiten und kann die meisten Anforderungen an Dialogfragen, kreative Generierung und Anwendungsf\xe4lle von Plugins erf\xfcllen. Es unterst\xfctzt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualit\xe4t der Antwortinformationen zu gew\xe4hrleisten."},"ERNIE-4.0-8K-Latest":{"description":"Das von Baidu entwickelte Flaggschiff-Modell f\xfcr ultra-gro\xdfe Sprachverarbeitung, das im Vergleich zu ERNIE 3.5 eine umfassende Verbesserung der Modellf\xe4higkeiten erreicht hat und sich breit f\xfcr komplexe Aufgaben in verschiedenen Bereichen eignet; unterst\xfctzt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualit\xe4t der Antwortinformationen zu gew\xe4hrleisten."},"ERNIE-4.0-8K-Preview":{"description":"Das von Baidu entwickelte Flaggschiff-Modell f\xfcr ultra-gro\xdfe Sprachverarbeitung, das im Vergleich zu ERNIE 3.5 eine umfassende Verbesserung der Modellf\xe4higkeiten erreicht hat und sich breit f\xfcr komplexe Aufgaben in verschiedenen Bereichen eignet; unterst\xfctzt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualit\xe4t der Antwortinformationen zu gew\xe4hrleisten."},"ERNIE-4.0-Turbo-8K-Latest":{"description":"Baidus selbstentwickeltes Flaggschiff-Modell f\xfcr gro\xdffl\xe4chige Sprachverarbeitung, das in vielen komplexen Aufgaben hervorragende Ergebnisse zeigt und umfassend in verschiedenen Bereichen eingesetzt werden kann; unterst\xfctzt die automatische Anbindung an Baidu-Suchplugins, um die Aktualit\xe4t von Antwortinformationen zu gew\xe4hrleisten. Im Vergleich zu ERNIE 4.0 hat es eine bessere Leistung."},"ERNIE-4.0-Turbo-8K-Preview":{"description":"Das von Baidu entwickelte Flaggschiff-Modell f\xfcr ultra-gro\xdfe Sprachverarbeitung, das in der Gesamtleistung herausragend ist und sich breit f\xfcr komplexe Aufgaben in verschiedenen Bereichen eignet; unterst\xfctzt die automatische Anbindung an das Baidu-Such-Plugin, um die Aktualit\xe4t der Antwortinformationen zu gew\xe4hrleisten. Im Vergleich zu ERNIE 4.0 bietet es eine bessere Leistungsf\xe4higkeit."},"ERNIE-Character-8K":{"description":"Das von Baidu entwickelte Sprachmodell f\xfcr vertikale Szenarien, das sich f\xfcr Anwendungen wie Spiel-NPCs, Kundenservice-Dialoge und Rollenspiele eignet. Es hat einen klareren und konsistenteren Charakterstil, eine st\xe4rkere Befolgung von Anweisungen und eine bessere Inferenzleistung."},"ERNIE-Lite-Pro-128K":{"description":"Das von Baidu entwickelte leichte Sprachmodell, das hervorragende Modellleistung und Inferenzleistung kombiniert. Es bietet bessere Ergebnisse als ERNIE Lite und eignet sich f\xfcr die Inferenznutzung auf AI-Beschleunigungskarten mit geringer Rechenleistung."},"ERNIE-Speed-128K":{"description":"Das neueste von Baidu im Jahr 2024 ver\xf6ffentlichte hochleistungsf\xe4hige Sprachmodell, das \xfcberragende allgemeine F\xe4higkeiten bietet und sich als Basis-Modell f\xfcr Feinabstimmungen eignet, um spezifische Szenarien besser zu bearbeiten, und bietet gleichzeitig hervorragende Inferenzleistung."},"ERNIE-Speed-Pro-128K":{"description":"Das neueste von Baidu im Jahr 2024 ver\xf6ffentlichte hochleistungsf\xe4hige Sprachmodell, das \xfcberragende allgemeine F\xe4higkeiten bietet und bessere Ergebnisse als ERNIE Speed erzielt. Es eignet sich als Basis-Modell f\xfcr Feinabstimmungen, um spezifische Szenarien besser zu bearbeiten, und bietet gleichzeitig hervorragende Inferenzleistung."},"FLUX-1.1-pro":{"description":"FLUX.1.1 Pro"},"FLUX.1-Kontext-dev":{"description":"FLUX.1-Kontext-dev ist ein von Black Forest Labs entwickeltes multimodales Bildgenerierungs- und Bearbeitungsmodell auf Basis der Rectified Flow Transformer-Architektur mit 12 Milliarden Parametern. Es konzentriert sich auf die Generierung, Rekonstruktion, Verbesserung oder Bearbeitung von Bildern unter gegebenen Kontextbedingungen. Das Modell kombiniert die kontrollierbare Generierung von Diffusionsmodellen mit der Kontextmodellierung von Transformern, unterst\xfctzt hochwertige Bildausgaben und ist vielseitig einsetzbar f\xfcr Bildrestaurierung, Bildvervollst\xe4ndigung und visuelle Szenenrekonstruktion."},"FLUX.1-Kontext-pro":{"description":"FLUX.1 Kontext [pro]"},"FLUX.1-dev":{"description":"FLUX.1-dev ist ein von Black Forest Labs entwickeltes Open-Source-multimodales Sprachmodell (Multimodal Language Model, MLLM), das f\xfcr Bild-Text-Aufgaben optimiert ist und Verst\xe4ndnis sowie Generierung von Bildern und Texten vereint. Es basiert auf fortschrittlichen gro\xdfen Sprachmodellen wie Mistral-7B und erreicht durch sorgf\xe4ltig gestaltete visuelle Encoder und mehrstufige Instruktions-Feinabstimmung eine kooperative Verarbeitung von Bild und Text sowie komplexe Aufgabenlogik."},"Gryphe/MythoMax-L2-13b":{"description":"MythoMax-L2 (13B) ist ein innovatives Modell, das sich f\xfcr Anwendungen in mehreren Bereichen und komplexe Aufgaben eignet."},"HelloMeme":{"description":"HelloMeme ist ein KI-Tool, das automatisch Memes, animierte GIFs oder Kurzvideos basierend auf von dir bereitgestellten Bildern oder Aktionen erstellt. Es erfordert keine Zeichen- oder Programmierkenntnisse – du brauchst nur Referenzbilder, und es hilft dir, ansprechende, unterhaltsame und stilistisch einheitliche Inhalte zu erstellen."},"HiDream-I1-Full":{"description":"HiDream-E1-Full ist ein von HiDream.ai entwickeltes Open-Source-multimodales Bildbearbeitungsmodell, das auf der fortschrittlichen Diffusion Transformer-Architektur basiert und mit leistungsstarker Sprachverst\xe4ndnisf\xe4higkeit (integriert LLaMA 3.1-8B-Instruct) ausgestattet ist. Es unterst\xfctzt die Bildgenerierung, Stil\xfcbertragung, lokale Bearbeitung und Neugestaltung durch nat\xfcrliche Sprachbefehle und bietet exzellentes Verst\xe4ndnis und Ausf\xfchrung von Bild-Text-Anweisungen."},"HunyuanDiT-v1.2-Diffusers-Distilled":{"description":"hunyuandit-v1.2-distilled ist ein leichtgewichtiges Text-zu-Bild-Modell, das durch Destillation optimiert wurde, um schnell hochwertige Bilder zu erzeugen. Es eignet sich besonders f\xfcr ressourcenarme Umgebungen und Echtzeit-Generierungsaufgaben."},"InstantCharacter":{"description":"InstantCharacter ist ein 2025 vom Tencent AI-Team ver\xf6ffentlichtes tuning-freies personalisiertes Charaktergenerierungsmodell, das eine hochpr\xe4zise und konsistente Charaktererstellung \xfcber verschiedene Szenarien hinweg erm\xf6glicht. Das Modell kann einen Charakter allein anhand eines Referenzbildes modellieren und diesen flexibel in verschiedene Stile, Bewegungen und Hintergr\xfcnde \xfcbertragen."},"InternVL2-8B":{"description":"InternVL2-8B ist ein leistungsstarkes visuelles Sprachmodell, das multimodale Verarbeitung von Bildern und Text unterst\xfctzt und in der Lage ist, Bildinhalte pr\xe4zise zu erkennen und relevante Beschreibungen oder Antworten zu generieren."},"InternVL2.5-26B":{"description":"InternVL2.5-26B ist ein leistungsstarkes visuelles Sprachmodell, das multimodale Verarbeitung von Bildern und Text unterst\xfctzt und in der Lage ist, Bildinhalte pr\xe4zise zu erkennen und relevante Beschreibungen oder Antworten zu generieren."},"Kolors":{"description":"Kolors ist ein von Kuaishou Kolors Team entwickeltes Text-zu-Bild-Modell, das mit Milliarden von Parametern trainiert wurde und in visueller Qualit\xe4t, chinesischem semantischem Verst\xe4ndnis sowie Textdarstellung herausragende Vorteile bietet."},"Kwai-Kolors/Kolors":{"description":"Kolors ist ein von Kuaishou Kolors Team entwickeltes gro\xdf angelegtes latentes Diffusionsmodell zur Text-zu-Bild-Generierung. Es wurde mit Milliarden von Text-Bild-Paaren trainiert und zeigt herausragende Leistungen in visueller Qualit\xe4t, komplexer semantischer Genauigkeit sowie der Darstellung chinesischer und englischer Schriftzeichen. Es unterst\xfctzt sowohl chinesische als auch englische Eingaben und ist besonders leistungsf\xe4hig bei der Verarbeitung und Erzeugung chinesischsprachiger Inhalte."},"Kwaipilot/KAT-Dev":{"description":"KAT-Dev (32B) ist ein Open-Source-Modell mit 32 Milliarden Parametern, das speziell f\xfcr Aufgaben in der Softwareentwicklung konzipiert wurde. Beim SWE-Bench Verified Benchmark erreichte es eine L\xf6sungsrate von 62,4 % und belegte damit den f\xfcnften Platz unter allen Open-Source-Modellen unterschiedlicher Gr\xf6\xdfen. Das Modell wurde in mehreren Phasen optimiert, darunter Zwischen-Training, \xfcberwachtes Feintuning (SFT) und Reinforcement Learning (RL), um anspruchsvolle Programmieraufgaben wie Codevervollst\xe4ndigung, Fehlerbehebung und Code-Reviews effektiv zu unterst\xfctzen."},"Llama-3.2-11B-Vision-Instruct":{"description":"Hervorragende Bildschlussfolgerungsf\xe4higkeiten auf hochaufl\xf6senden Bildern, geeignet f\xfcr Anwendungen im Bereich der visuellen Verst\xe4ndigung."},"Llama-3.2-90B-Vision-Instruct\\t":{"description":"Fortgeschrittene Bildschlussfolgerungsf\xe4higkeiten f\xfcr Anwendungen im Bereich der visuellen Verst\xe4ndigung."},"Meta-Llama-3-3-70B-Instruct":{"description":"Llama 3.3 70B: Ein vielseitiges Transformer-Modell, geeignet f\xfcr Dialog- und Generierungsaufgaben."},"Meta-Llama-3.1-405B-Instruct":{"description":"Das auf Anweisungen optimierte Textmodell Llama 3.1 wurde f\xfcr mehrsprachige Dialoganwendungen optimiert und zeigt in vielen verf\xfcgbaren Open-Source- und geschlossenen Chat-Modellen in g\xe4ngigen Branchenbenchmarks hervorragende Leistungen."},"Meta-Llama-3.1-70B-Instruct":{"description":"Das auf Anweisungen optimierte Textmodell Llama 3.1 wurde f\xfcr mehrsprachige Dialoganwendungen optimiert und zeigt in vielen verf\xfcgbaren Open-Source- und geschlossenen Chat-Modellen in g\xe4ngigen Branchenbenchmarks hervorragende Leistungen."},"Meta-Llama-3.1-8B-Instruct":{"description":"Das auf Anweisungen optimierte Textmodell Llama 3.1 wurde f\xfcr mehrsprachige Dialoganwendungen optimiert und zeigt in vielen verf\xfcgbaren Open-Source- und geschlossenen Chat-Modellen in g\xe4ngigen Branchenbenchmarks hervorragende Leistungen."},"Meta-Llama-3.2-1B-Instruct":{"description":"Ein fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverst\xe4ndnis, hervorragenden Schlussfolgerungsf\xe4higkeiten und Textgenerierungsf\xe4higkeiten."},"Meta-Llama-3.2-3B-Instruct":{"description":"Ein fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverst\xe4ndnis, hervorragenden Schlussfolgerungsf\xe4higkeiten und Textgenerierungsf\xe4higkeiten."},"Meta-Llama-3.3-70B-Instruct":{"description":"Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und wurde durch \xfcberwachte Feinabstimmung (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF) in Bezug auf N\xfctzlichkeit und Sicherheit verbessert. Die auf Anweisungen optimierte Version ist speziell f\xfcr mehrsprachige Dialoge optimiert und \xfcbertrifft in mehreren Branchenbenchmarks viele verf\xfcgbare Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."},"Meta-Llama-4-Maverick-17B-128E-Instruct-FP8":{"description":"Llama 4 Maverick: Ein gro\xdf angelegtes Modell basierend auf Mixture-of-Experts, das eine effiziente Expertenaktivierungsstrategie bietet, um bei der Inferenz herausragende Leistungen zu erzielen."},"MiniMax-M1":{"description":"Ein neu entwickeltes Inferenzmodell. Weltweit f\xfchrend: 80K Denkketten x 1M Eingaben, vergleichbare Leistung mit den besten internationalen Modellen."},"MiniMax-M2":{"description":"Speziell entwickelt f\xfcr effizientes Programmieren und Agent-Workflows."},"MiniMax-Text-01":{"description":"In der MiniMax-01-Serie haben wir mutige Innovationen vorgenommen: Erstmals wurde die lineare Aufmerksamkeitsmechanismus in gro\xdfem Ma\xdfstab implementiert, sodass die traditionelle Transformer-Architektur nicht mehr die einzige Wahl ist. Dieses Modell hat eine Parameteranzahl von bis zu 456 Milliarden, wobei eine Aktivierung 45,9 Milliarden betr\xe4gt. Die Gesamtleistung des Modells kann mit den besten Modellen im Ausland mithalten und kann gleichzeitig effizient den weltweit l\xe4ngsten Kontext von 4 Millionen Tokens verarbeiten, was 32-mal so viel wie GPT-4o und 20-mal so viel wie Claude-3.5-Sonnet ist."},"MiniMaxAI/MiniMax-M1-80k":{"description":"MiniMax-M1 ist ein gro\xdf angelegtes hybrides Aufmerksamkeits-Inferenzmodell mit offenen Gewichten, das 456 Milliarden Parameter umfasst und etwa 45,9 Milliarden Parameter pro Token aktiviert. Das Modell unterst\xfctzt nativ einen ultralangen Kontext von 1 Million Tokens und spart durch den Blitz-Attention-Mechanismus bei Aufgaben mit 100.000 Tokens im Vergleich zu DeepSeek R1 75 % der Flie\xdfkommaoperationen ein. Gleichzeitig verwendet MiniMax-M1 eine MoE-Architektur (Mixture of Experts) und kombiniert den CISPO-Algorithmus mit einem hybriden Aufmerksamkeitsdesign f\xfcr effizientes verst\xe4rkendes Lernen, was in der Langzeiteingabe-Inferenz und realen Software-Engineering-Szenarien branchenf\xfchrende Leistung erzielt."},"MiniMaxAI/MiniMax-M2":{"description":"MiniMax-M2 definiert Effizienz f\xfcr Agenten neu. Es handelt sich um ein kompaktes, schnelles und kosteneffizientes MoE-Modell mit insgesamt 230 Milliarden Parametern und 10 Milliarden aktiven Parametern. Es wurde f\xfcr Spitzenleistungen bei Codierungs- und Agentenaufgaben entwickelt und bewahrt gleichzeitig eine starke allgemeine Intelligenz. Mit nur 10 Milliarden aktiven Parametern bietet MiniMax-M2 eine Leistung, die mit gro\xdf angelegten Modellen vergleichbar ist, und ist damit die ideale Wahl f\xfcr Anwendungen mit hohen Effizienzanforderungen."},"Moonshot-Kimi-K2-Instruct":{"description":"Mit insgesamt 1 Billion Parametern und 32 Milliarden aktivierten Parametern erreicht dieses nicht-denkende Modell Spitzenleistungen in den Bereichen aktuelles Wissen, Mathematik und Programmierung und ist besonders f\xfcr allgemeine Agentenaufgaben optimiert. Es wurde speziell f\xfcr Agentenaufgaben verfeinert, kann nicht nur Fragen beantworten, sondern auch Aktionen ausf\xfchren. Ideal f\xfcr spontane, allgemeine Gespr\xe4che und Agentenerfahrungen, ist es ein reflexartiges Modell ohne lange Denkzeiten."},"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO":{"description":"Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) ist ein hochpr\xe4zises Anweisungsmodell, das f\xfcr komplexe Berechnungen geeignet ist."},"OmniConsistency":{"description":"OmniConsistency verbessert durch den Einsatz gro\xdfskaliger Diffusion Transformers (DiTs) und gepaarter stilisierter Daten die Stil-Konsistenz und Generalisierungsf\xe4higkeit bei Bild-zu-Bild-Aufgaben und verhindert Stilverschlechterung."},"Phi-3-medium-128k-instruct":{"description":"Das gleiche Phi-3-medium-Modell, jedoch mit einer gr\xf6\xdferen Kontextgr\xf6\xdfe f\xfcr RAG oder Few-Shot-Prompting."},"Phi-3-medium-4k-instruct":{"description":"Ein Modell mit 14 Milliarden Parametern, das eine bessere Qualit\xe4t als Phi-3-mini bietet und sich auf qualitativ hochwertige, reasoning-dense Daten konzentriert."},"Phi-3-mini-128k-instruct":{"description":"Das gleiche Phi-3-mini-Modell, jedoch mit einer gr\xf6\xdferen Kontextgr\xf6\xdfe f\xfcr RAG oder Few-Shot-Prompting."},"Phi-3-mini-4k-instruct":{"description":"Das kleinste Mitglied der Phi-3-Familie. Optimiert f\xfcr Qualit\xe4t und geringe Latenz."},"Phi-3-small-128k-instruct":{"description":"Das gleiche Phi-3-small-Modell, jedoch mit einer gr\xf6\xdferen Kontextgr\xf6\xdfe f\xfcr RAG oder Few-Shot-Prompting."},"Phi-3-small-8k-instruct":{"description":"Ein Modell mit 7 Milliarden Parametern, das eine bessere Qualit\xe4t als Phi-3-mini bietet und sich auf qualitativ hochwertige, reasoning-dense Daten konzentriert."},"Phi-3.5-mini-instruct":{"description":"Aktualisierte Version des Phi-3-mini-Modells."},"Phi-3.5-vision-instrust":{"description":"Aktualisierte Version des Phi-3-vision-Modells."},"Pro/Qwen/Qwen2-7B-Instruct":{"description":"Qwen2-7B-Instruct ist das anweisungsfeinabgestimmte gro\xdfe Sprachmodell der Qwen2-Serie mit einer Parametergr\xf6\xdfe von 7B. Dieses Modell basiert auf der Transformer-Architektur und verwendet Technologien wie die SwiGLU-Aktivierungsfunktion, QKV-Offsets und gruppierte Abfrageaufmerksamkeit. Es kann gro\xdfe Eingaben verarbeiten. Das Modell zeigt hervorragende Leistungen in der Sprachverst\xe4ndnis, -generierung, Mehrsprachigkeit, Codierung, Mathematik und Inferenz in mehreren Benchmark-Tests und \xfcbertrifft die meisten Open-Source-Modelle und zeigt in bestimmten Aufgaben eine vergleichbare Wettbewerbsf\xe4higkeit mit propriet\xe4ren Modellen. Qwen2-7B-Instruct \xfcbertrifft Qwen1.5-7B-Chat in mehreren Bewertungen und zeigt signifikante Leistungsverbesserungen."},"Pro/Qwen/Qwen2.5-7B-Instruct":{"description":"Qwen2.5-7B-Instruct ist eines der neuesten gro\xdfen Sprachmodelle, die von Alibaba Cloud ver\xf6ffentlicht wurden. Dieses 7B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterst\xfctzung und deckt \xfcber 29 Sprachen ab, einschlie\xdflich Chinesisch und Englisch. Es zeigt signifikante Verbesserungen in der Befolgung von Anweisungen, im Verst\xe4ndnis strukturierter Daten und in der Generierung strukturierter Ausgaben (insbesondere JSON)."},"Pro/Qwen/Qwen2.5-Coder-7B-Instruct":{"description":"Qwen2.5-Coder-7B-Instruct ist die neueste Version der von Alibaba Cloud ver\xf6ffentlichten Reihe von code-spezifischen gro\xdfen Sprachmodellen. Dieses Modell basiert auf Qwen2.5 und wurde mit 55 Billionen Tokens trainiert, um die F\xe4higkeiten zur Codegenerierung, Inferenz und Fehlerbehebung erheblich zu verbessern. Es verbessert nicht nur die Codierungsf\xe4higkeiten, sondern bewahrt auch die Vorteile in Mathematik und allgemeinen F\xe4higkeiten. Das Modell bietet eine umfassendere Grundlage f\xfcr praktische Anwendungen wie Code-Agenten."},"Pro/Qwen/Qwen2.5-VL-7B-Instruct":{"description":"Qwen2.5-VL ist ein neues Mitglied der Qwen-Serie und verf\xfcgt \xfcber leistungsstarke visuelle Wahrnehmungsf\xe4higkeiten. Es kann Text, Diagramme und Layouts in Bildern analysieren, l\xe4ngere Videos verstehen und Ereignisse erfassen. Zudem kann es Schlussfolgerungen ziehen, Werkzeuge bedienen, mehrere Formate f\xfcr Objektlokalisation unterst\xfctzen und strukturierte Ausgaben generieren. Die Videoverarbeitung wurde durch dynamische Aufl\xf6sungs- und Frameratetraining optimiert, und die Effizienz des visuellen Encoders wurde verbessert."},"Pro/THUDM/GLM-4.1V-9B-Thinking":{"description":"GLM-4.1V-9B-Thinking ist ein von Zhipu AI und dem KEG-Labor der Tsinghua-Universit\xe4t gemeinsam ver\xf6ffentlichtes Open-Source-Visuell-Sprachmodell (VLM), das speziell f\xfcr die Bew\xe4ltigung komplexer multimodaler kognitiver Aufgaben entwickelt wurde. Das Modell basiert auf dem GLM-4-9B-0414-Grundmodell und verbessert durch die Einf\xfchrung des „Chain-of-Thought“-Schlussmechanismus und den Einsatz von Verst\xe4rkungslernstrategien seine multimodale Schlussfolgerungsf\xe4higkeit und Stabilit\xe4t erheblich."},"Pro/THUDM/glm-4-9b-chat":{"description":"GLM-4-9B-Chat ist die Open-Source-Version des GLM-4-Modells, das von Zhizhu AI eingef\xfchrt wurde. Dieses Modell zeigt hervorragende Leistungen in den Bereichen Semantik, Mathematik, Inferenz, Code und Wissen. Neben der Unterst\xfctzung f\xfcr mehrstufige Dialoge bietet GLM-4-9B-Chat auch fortgeschrittene Funktionen wie Web-Browsing, Code-Ausf\xfchrung, benutzerdefinierte Tool-Aufrufe (Function Call) und langes Textverst\xe4ndnis. Das Modell unterst\xfctzt 26 Sprachen, darunter Chinesisch, Englisch, Japanisch, Koreanisch und Deutsch. In mehreren Benchmark-Tests zeigt GLM-4-9B-Chat hervorragende Leistungen, wie AlignBench-v2, MT-Bench, MMLU und C-Eval. Das Modell unterst\xfctzt eine maximale Kontextl\xe4nge von 128K und ist f\xfcr akademische Forschung und kommerzielle Anwendungen geeignet."},"Pro/deepseek-ai/DeepSeek-R1":{"description":"DeepSeek-R1 ist ein durch verst\xe4rkendes Lernen (RL) gesteuertes Inferenzmodell, das Probleme mit Wiederholungen und Lesbarkeit im Modell l\xf6st. Vor dem RL f\xfchrte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmierbezogenen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert die Gesamtleistung durch sorgf\xe4ltig gestaltete Trainingsmethoden."},"Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B":{"description":"DeepSeek-R1-Distill-Qwen-7B ist ein Modell, das durch Wissensdistillierung auf Basis von Qwen2.5-Math-7B erstellt wurde. Dieses Modell wurde mit 800.000 sorgf\xe4ltig ausgew\xe4hlten Beispielen, die von DeepSeek-R1 generiert wurden, feinjustiert und zeigt ausgezeichnete Inferenzf\xe4higkeiten. Es erzielte in mehreren Benchmarks hervorragende Ergebnisse, darunter eine Genauigkeit von 92,8 % im MATH-500, einen Durchgangsrate von 55,5 % im AIME 2024 und eine Bewertung von 1189 auf CodeForces, was seine starken mathematischen und programmierischen F\xe4higkeiten als Modell mit 7B Parametern unterstreicht."},"Pro/deepseek-ai/DeepSeek-V3":{"description":"DeepSeek-V3 ist ein hybrides Experten (MoE) Sprachmodell mit 6710 Milliarden Parametern, das eine Multi-Head-Latente-Attention (MLA) und DeepSeekMoE-Architektur verwendet, kombiniert mit einer Lastenausgleichsstrategie ohne Hilfskosten, um die Inferenz- und Trainingseffizienz zu optimieren. Durch das Pre-Training auf 14,8 Billionen hochwertigen Tokens und anschlie\xdfende \xfcberwachte Feinabstimmung und verst\xe4rktes Lernen \xfcbertrifft DeepSeek-V3 in der Leistung andere Open-Source-Modelle und n\xe4hert sich f\xfchrenden geschlossenen Modellen."},"Pro/deepseek-ai/DeepSeek-V3.1-Terminus":{"description":"DeepSeek-V3.1-Terminus ist eine aktualisierte Version des V3.1-Modells von DeepSeek, positioniert als hybrides Agenten-Gro\xdfsprachmodell. Dieses Update konzentriert sich darauf, auf Nutzerfeedback basierende Probleme zu beheben und die Stabilit\xe4t zu verbessern, w\xe4hrend die urspr\xfcnglichen Modellf\xe4higkeiten erhalten bleiben. Es verbessert deutlich die Sprachkonsistenz und reduziert das Vermischen von Chinesisch und Englisch sowie das Auftreten ungew\xf6hnlicher Zeichen. Das Modell integriert den „Denkmodus“ (Thinking Mode) und den „Nicht-Denkmodus“ (Non-thinking Mode), zwischen denen Nutzer flexibel \xfcber Chatvorlagen wechseln k\xf6nnen, um unterschiedlichen Aufgaben gerecht zu werden. Als wichtige Optimierung verbessert V3.1-Terminus die Leistung des Code-Agenten und des Such-Agenten, wodurch diese bei Werkzeugaufrufen und der Ausf\xfchrung mehrstufiger komplexer Aufgaben zuverl\xe4ssiger sind."},"Pro/deepseek-ai/DeepSeek-V3.2-Exp":{"description":"DeepSeek-V3.2-Exp ist eine experimentelle Version 3.2 von DeepSeek und stellt einen Zwischenschritt auf dem Weg zur n\xe4chsten Generation der Architektur dar. Aufbauend auf V3.1-Terminus f\xfchrt sie den DeepSeek Sparse Attention (DSA)-Mechanismus ein, um die Effizienz beim Training und bei der Inferenz mit langen Kontexten zu verbessern. Sie wurde speziell f\xfcr Werkzeugaufrufe, das Verst\xe4ndnis langer Dokumente und mehrstufiges Schlussfolgern optimiert. V3.2-Exp dient als Br\xfccke zwischen Forschung und Produktreife und eignet sich f\xfcr Nutzer, die in Szenarien mit hohem Kontextbudget eine h\xf6here Inferenzleistung erkunden m\xf6chten."},"Pro/moonshotai/Kimi-K2-Instruct-0905":{"description":"Kimi K2-Instruct-0905 ist die neueste und leistungsst\xe4rkste Version von Kimi K2. Es handelt sich um ein erstklassiges Mixture-of-Experts (MoE) Sprachmodell mit insgesamt 1 Billion Parametern und 32 Milliarden aktivierten Parametern. Die Hauptmerkmale dieses Modells umfassen: verbesserte Agenten-Codierungsintelligenz, die in \xf6ffentlichen Benchmark-Tests und realen Agenten-Codierungsaufgaben eine signifikante Leistungssteigerung zeigt; verbesserte Frontend-Codierungserfahrung mit Fortschritten in \xc4sthetik und Praktikabilit\xe4t der Frontend-Programmierung."},"QwQ-32B-Preview":{"description":"QwQ-32B-Preview ist ein innovatives Modell f\xfcr die Verarbeitung nat\xfcrlicher Sprache, das komplexe Aufgaben der Dialoggenerierung und des Kontextverst\xe4ndnisses effizient bew\xe4ltigen kann."},"Qwen/QVQ-72B-Preview":{"description":"QVQ-72B-Preview ist ein forschungsorientiertes Modell, das vom Qwen-Team entwickelt wurde und sich auf visuelle Inferenzf\xe4higkeiten konzentriert. Es hat einzigartige Vorteile beim Verst\xe4ndnis komplexer Szenen und der L\xf6sung visuell verwandter mathematischer Probleme."},"Qwen/QwQ-32B":{"description":"QwQ ist das Inferenzmodell der Qwen-Serie. Im Vergleich zu traditionellen, anweisungsoptimierten Modellen verf\xfcgt QwQ \xfcber Denk- und Schlussfolgerungsf\xe4higkeiten, die eine signifikante Leistungssteigerung bei nachgelagerten Aufgaben erm\xf6glichen, insbesondere bei der L\xf6sung schwieriger Probleme. QwQ-32B ist ein mittelgro\xdfes Inferenzmodell, das im Vergleich zu den fortschrittlichsten Inferenzmodellen (wie DeepSeek-R1, o1-mini) wettbewerbsf\xe4hige Leistungen erzielt. Dieses Modell verwendet Technologien wie RoPE, SwiGLU, RMSNorm und Attention QKV Bias und hat eine Netzwerkstruktur mit 64 Schichten und 40 Q-Attention-K\xf6pfen (im GQA-Architektur sind es 8 KV)."},"Qwen/QwQ-32B-Preview":{"description":"QwQ-32B-Preview ist das neueste experimentelle Forschungsmodell von Qwen, das sich auf die Verbesserung der KI-Inferenzf\xe4higkeiten konzentriert. Durch die Erforschung komplexer Mechanismen wie Sprachmischung und rekursive Inferenz bietet es Hauptvorteile wie starke Analysef\xe4higkeiten, mathematische und Programmierf\xe4higkeiten. Gleichzeitig gibt es Herausforderungen wie Sprachwechsel, Inferenzzyklen, Sicherheits\xfcberlegungen und Unterschiede in anderen F\xe4higkeiten."},"Qwen/Qwen-Image":{"description":"Qwen-Image ist ein von Alibabas Tongyi Qianwen-Team entwickeltes Basismodell zur Bildgenerierung mit 20 Milliarden Parametern. Das Modell erzielt bemerkenswerte Fortschritte bei der komplexen Textrendering und pr\xe4zisen Bildbearbeitung und ist besonders gut darin, Bilder mit hochaufl\xf6senden chinesischen und englischen Texten zu erzeugen. Qwen-Image kann nicht nur mehrzeilige Layouts und absatzweise Texte verarbeiten, sondern bewahrt auch die Koh\xe4renz des Layouts und die Kontextharmonie bei der Bildgenerierung. Neben seiner herausragenden Textrendering-F\xe4higkeit unterst\xfctzt das Modell eine breite Palette k\xfcnstlerischer Stile – von realistischen Fotografien bis hin zu Anime-\xc4sthetik – und passt sich flexibel an verschiedene kreative Anforderungen an. Dar\xfcber hinaus verf\xfcgt es \xfcber leistungsstarke Bildbearbeitungs- und Bildverst\xe4ndnisf\xe4higkeiten, einschlie\xdflich Stiltransfer, Objektentfernung und -hinzuf\xfcgung, Detailverbesserung, Textbearbeitung und sogar Steuerung menschlicher Posen. Ziel ist es, ein umfassendes intelligentes Basismodell f\xfcr visuelle Kreation und Verarbeitung zu sein, das Sprache, Layout und Bild vereint."},"Qwen/Qwen-Image-Edit-2509":{"description":"Qwen-Image-Edit-2509 ist die neueste Version des Bildbearbeitungsmodells Qwen-Image, entwickelt vom Tongyi Qianwen-Team bei Alibaba. Das Modell basiert auf dem 20B-Parameter-Modell Qwen-Image und wurde gezielt weitertrainiert, um dessen einzigartige Textrendering-F\xe4higkeiten erfolgreich auf den Bereich der Bildbearbeitung zu \xfcbertragen – insbesondere f\xfcr die pr\xe4zise Bearbeitung von Texten in Bildern. Qwen-Image-Edit verwendet eine innovative Architektur, bei der das Eingabebild gleichzeitig in Qwen2.5-VL (f\xfcr semantische Steuerung) und den VAE-Encoder (f\xfcr visuelle Erscheinungskontrolle) eingespeist wird. Dadurch erm\xf6glicht es sowohl semantische als auch visuelle Bearbeitungen. Das bedeutet, dass es nicht nur lokale visuelle \xc4nderungen wie das Hinzuf\xfcgen, Entfernen oder Modifizieren von Elementen unterst\xfctzt, sondern auch komplexe semantische Bearbeitungen wie IP-Kreationen oder Stil\xfcbertragungen, bei denen die inhaltliche Konsistenz gewahrt bleibt. In mehreren \xf6ffentlichen Benchmark-Tests erzielte das Modell Spitzenleistungen (SOTA) und etabliert sich damit als leistungsstarkes Basismodell f\xfcr die Bildbearbeitung."},"Qwen/Qwen2-72B-Instruct":{"description":"Qwen2 ist ein fortschrittliches allgemeines Sprachmodell, das eine Vielzahl von Anweisungsarten unterst\xfctzt."},"Qwen/Qwen2-7B-Instruct":{"description":"Qwen2-72B-Instruct ist das anweisungsfeinabgestimmte gro\xdfe Sprachmodell der Qwen2-Serie mit einer Parametergr\xf6\xdfe von 72B. Dieses Modell basiert auf der Transformer-Architektur und verwendet Technologien wie die SwiGLU-Aktivierungsfunktion, QKV-Offsets und gruppierte Abfrageaufmerksamkeit. Es kann gro\xdfe Eingaben verarbeiten. Das Modell zeigt hervorragende Leistungen in der Sprachverst\xe4ndnis, -generierung, Mehrsprachigkeit, Codierung, Mathematik und Inferenz in mehreren Benchmark-Tests und \xfcbertrifft die meisten Open-Source-Modelle und zeigt in bestimmten Aufgaben eine vergleichbare Wettbewerbsf\xe4higkeit mit propriet\xe4ren Modellen."},"Qwen/Qwen2-VL-72B-Instruct":{"description":"Qwen2-VL ist die neueste Iteration des Qwen-VL-Modells, das in visuellen Verst\xe4ndnis-Benchmarks erstklassige Leistungen erzielt."},"Qwen/Qwen2.5-14B-Instruct":{"description":"Qwen2.5 ist eine brandneue Serie von gro\xdfen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."},"Qwen/Qwen2.5-32B-Instruct":{"description":"Qwen2.5 ist eine brandneue Serie von gro\xdfen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."},"Qwen/Qwen2.5-72B-Instruct":{"description":"Ein gro\xdfes Sprachmodell, das vom Alibaba Cloud Tongyi Qianwen-Team entwickelt wurde."},"Qwen/Qwen2.5-72B-Instruct-128K":{"description":"Qwen2.5 ist eine neue Serie gro\xdfer Sprachmodelle mit st\xe4rkeren Verst\xe4ndnis- und Generierungsf\xe4higkeiten."},"Qwen/Qwen2.5-72B-Instruct-Turbo":{"description":"Qwen2.5 ist eine neue Serie gro\xdfer Sprachmodelle, die darauf abzielt, die Verarbeitung von instructiven Aufgaben zu optimieren."},"Qwen/Qwen2.5-7B-Instruct":{"description":"Qwen2.5 ist eine brandneue Serie von gro\xdfen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."},"Qwen/Qwen2.5-7B-Instruct-Turbo":{"description":"Qwen2.5 ist eine neue Serie gro\xdfer Sprachmodelle, die darauf abzielt, die Verarbeitung von instructiven Aufgaben zu optimieren."},"Qwen/Qwen2.5-Coder-32B-Instruct":{"description":"Qwen2.5-Coder konzentriert sich auf das Programmieren."},"Qwen/Qwen2.5-Coder-7B-Instruct":{"description":"Qwen2.5-Coder-7B-Instruct ist die neueste Version der von Alibaba Cloud ver\xf6ffentlichten Reihe von code-spezifischen gro\xdfen Sprachmodellen. Dieses Modell basiert auf Qwen2.5 und wurde mit 55 Billionen Tokens trainiert, um die F\xe4higkeiten zur Codegenerierung, Inferenz und Fehlerbehebung erheblich zu verbessern. Es verbessert nicht nur die Codierungsf\xe4higkeiten, sondern bewahrt auch die Vorteile in Mathematik und allgemeinen F\xe4higkeiten. Das Modell bietet eine umfassendere Grundlage f\xfcr praktische Anwendungen wie Code-Agenten."},"Qwen/Qwen2.5-VL-32B-Instruct":{"description":"Qwen2.5-VL-32B-Instruct ist ein multimodales Gro\xdfmodell, das vom Qwen-Team entwickelt wurde und Teil der Qwen2.5-VL-Reihe ist. Dieses Modell ist nicht nur in der Lage, \xfcbliche Objekte zu erkennen, sondern kann auch Text, Diagramme, Symbole, Grafiken und Layouts in Bildern analysieren. Es kann als visueller Agent dienen, der in der Lage ist, zu schlie\xdfen und Werkzeuge dynamisch zu steuern, wobei es F\xe4higkeiten im Umgang mit Computern und Smartphones besitzt. Dar\xfcber hinaus kann dieses Modell Objekte in Bildern pr\xe4zise lokalisieren und strukturierte Ausgaben f\xfcr Rechnungen, Tabellen usw. generieren. Im Vergleich zum Vorg\xe4ngermodell Qwen2-VL wurde diese Version durch verst\xe4rktes Lernen in Mathematik und Probleml\xf6sungsf\xe4higkeiten weiter verbessert, und ihr Antwortstil entspricht st\xe4rker den menschlichen Vorlieben."},"Qwen/Qwen2.5-VL-72B-Instruct":{"description":"Qwen2.5-VL ist ein visueller Sprachmodell der Qwen2.5-Serie. Dieses Modell zeichnet sich durch erhebliche Verbesserungen aus: Es verf\xfcgt \xfcber eine st\xe4rkere visuelle Wahrnehmungsf\xe4higkeit, kann \xfcbliche Objekte erkennen, Texte, Diagramme und Layouts analysieren; als visueller Agent kann es Schlussfolgerungen ziehen und die dynamische Nutzung von Werkzeugen leiten; es unterst\xfctzt das Verstehen von Videos mit einer L\xe4nge von \xfcber einer Stunde und kann wichtige Ereignisse erfassen; es kann durch die Generierung von Begrenzungsrahmen oder Punkten Objekte in Bildern pr\xe4zise lokalisieren; es unterst\xfctzt die Erstellung strukturierter Ausgaben, insbesondere f\xfcr gescannte Daten wie Rechnungen und Tabellen."},"Qwen/Qwen3-14B":{"description":"Qwen3 ist ein neues, leistungsstark verbessertes Modell von Tongyi Qianwen, das in den Bereichen Denken, Allgemeinwissen, Agenten und Mehrsprachigkeit in mehreren Kernf\xe4higkeiten branchenf\xfchrende Standards erreicht und den Wechsel zwischen Denkmodi unterst\xfctzt."},"Qwen/Qwen3-235B-A22B":{"description":"Qwen3 ist ein neues, leistungsstark verbessertes Modell von Tongyi Qianwen, das in den Bereichen Denken, Allgemeinwissen, Agenten und Mehrsprachigkeit in mehreren Kernf\xe4higkeiten branchenf\xfchrende Standards erreicht und den Wechsel zwischen Denkmodi unterst\xfctzt."},"Qwen/Qwen3-235B-A22B-Instruct-2507":{"description":"Qwen3-235B-A22B-Instruct-2507 ist ein Flaggschiff-Misch-Experten-(MoE)-Gro\xdfsprachmodell aus der Qwen3-Serie, entwickelt vom Alibaba Cloud Tongyi Qianwen Team. Es verf\xfcgt \xfcber 235 Milliarden Gesamtparameter und aktiviert bei jeder Inferenz 22 Milliarden Parameter. Als aktualisierte Version des nicht-denkenden Qwen3-235B-A22B fokussiert es sich auf signifikante Verbesserungen in Instruktionsbefolgung, logischem Denken, Textverst\xe4ndnis, Mathematik, Wissenschaft, Programmierung und Werkzeugnutzung. Zudem wurde die Abdeckung mehrsprachigen Langschwanzwissens erweitert und die Ausrichtung auf Nutzerpr\xe4ferenzen bei subjektiven und offenen Aufgaben verbessert, um hilfreichere und qualitativ hochwertigere Texte zu generieren."},"Qwen/Qwen3-235B-A22B-Thinking-2507":{"description":"Qwen3-235B-A22B-Thinking-2507 ist ein Mitglied der Qwen3-Serie gro\xdfer Sprachmodelle von Alibaba Tongyi Qianwen, spezialisiert auf komplexe anspruchsvolle Schlussfolgerungsaufgaben. Das Modell basiert auf der Misch-Experten-(MoE)-Architektur mit 235 Milliarden Gesamtparametern, aktiviert jedoch nur etwa 22 Milliarden Parameter pro Token, was eine hohe Rechenleistung bei Effizienz erm\xf6glicht. Als dediziertes „Denk“-Modell zeigt es herausragende Leistungen in logischem Denken, Mathematik, Wissenschaft, Programmierung und akademischen Benchmarks und erreicht Spitzenwerte unter Open-Source-Denkmodellen. Zus\xe4tzlich verbessert es allgemeine F\xe4higkeiten wie Instruktionsbefolgung, Werkzeugnutzung und Textgenerierung und unterst\xfctzt nativ eine Kontextl\xe4nge von 256K, ideal f\xfcr tiefgehende Schlussfolgerungen und lange Dokumente."},"Qwen/Qwen3-30B-A3B":{"description":"Qwen3 ist ein neues, leistungsstark verbessertes Modell von Tongyi Qianwen, das in den Bereichen Denken, Allgemeinwissen, Agenten und Mehrsprachigkeit in mehreren Kernf\xe4higkeiten branchenf\xfchrende Standards erreicht und den Wechsel zwischen Denkmodi unterst\xfctzt."},"Qwen/Qwen3-30B-A3B-Instruct-2507":{"description":"Qwen3-30B-A3B-Instruct-2507 ist eine aktualisierte Version des Qwen3-30B-A3B im Nicht-Denkmodus. Es handelt sich um ein Mixture-of-Experts (MoE)-Modell mit insgesamt 30,5 Milliarden Parametern und 3,3 Milliarden Aktivierungsparametern. Das Modell wurde in mehreren Bereichen entscheidend verbessert, darunter eine signifikante Steigerung der Befolgung von Anweisungen, logisches Denken, Textverst\xe4ndnis, Mathematik, Wissenschaft, Programmierung und Werkzeugnutzung. Gleichzeitig wurden substanzielle Fortschritte bei der Abdeckung von Langschwanzwissen in mehreren Sprachen erzielt, und es kann besser auf die Pr\xe4ferenzen der Nutzer bei subjektiven und offenen Aufgaben abgestimmt werden, um hilfreichere Antworten und qualitativ hochwertigere Texte zu generieren. Dar\xfcber hinaus wurde die F\xe4higkeit zum Verst\xe4ndnis langer Texte auf 256K erweitert. Dieses Modell unterst\xfctzt ausschlie\xdflich den Nicht-Denkmodus und generiert keine `<think></think>`-Tags in der Ausgabe."},"Qwen/Qwen3-30B-A3B-Thinking-2507":{"description":"Qwen3-30B-A3B-Thinking-2507 ist das neueste Denkmodell der Qwen3‑Serie, ver\xf6ffentlicht vom Alibaba Tongyi Qianwen‑Team. Als ein Mixture-of-Experts-(MoE)-Modell mit 30,5 Milliarden Gesamtparametern und 3,3 Milliarden aktiven Parametern konzentriert es sich auf die Verbesserung der Bew\xe4ltigung komplexer Aufgaben. Das Modell zeigt deutliche Leistungssteigerungen in akademischen Benchmarks f\xfcr logisches Schlie\xdfen, Mathematik, Naturwissenschaften, Programmierung sowie Aufgaben, die menschliche Fachkenntnisse erfordern. Gleichzeitig wurden seine allgemeinen F\xe4higkeiten bei der Befolgung von Anweisungen, der Nutzung von Werkzeugen, der Textgenerierung und der Anpassung an menschliche Pr\xe4ferenzen erheblich gest\xe4rkt. Das Modell unterst\xfctzt nativ ein langes Kontextverst\xe4ndnis von 256K und ist auf bis zu 1 Million Tokens skalierbar. Diese Version ist speziell f\xfcr den \'Denkmodus\' konzipiert und zielt darauf ab, hochkomplexe Aufgaben durch ausf\xfchrliches, schrittweises Denken zu l\xf6sen; auch seine Agent‑F\xe4higkeiten sind hervorragend."},"Qwen/Qwen3-32B":{"description":"Qwen3 ist ein neues, leistungsstark verbessertes Modell von Tongyi Qianwen, das in den Bereichen Denken, Allgemeinwissen, Agenten und Mehrsprachigkeit in mehreren Kernf\xe4higkeiten branchenf\xfchrende Standards erreicht und den Wechsel zwischen Denkmodi unterst\xfctzt."},"Qwen/Qwen3-8B":{"description":"Qwen3 ist ein neues, leistungsstark verbessertes Modell von Tongyi Qianwen, das in den Bereichen Denken, Allgemeinwissen, Agenten und Mehrsprachigkeit in mehreren Kernf\xe4higkeiten branchenf\xfchrende Standards erreicht und den Wechsel zwischen Denkmodi unterst\xfctzt."},"Qwen/Qwen3-Coder-30B-A3B-Instruct":{"description":"Qwen3-Coder-30B-A3B-Instruct ist ein Code-Modell der Qwen3-Serie, das vom Alibaba-Team Tongyi Qianwen entwickelt wurde. Als schlank optimiertes Modell konzentriert es sich darauf, die Code-Verarbeitungsf\xe4higkeiten zu verbessern, w\xe4hrend es hohe Leistung und Effizienz beibeh\xe4lt. Das Modell zeigt unter Open-Source-Modellen deutliche Leistungsvorteile bei komplexen Aufgaben wie agentischem Programmieren (Agentic Coding), automatisierten Browseroperationen und Werkzeugaufrufen. Es unterst\xfctzt nativ lange Kontexte mit 256K Tokens und l\xe4sst sich auf bis zu 1M Tokens erweitern, sodass es Verst\xe4ndnis- und Verarbeitungsaufgaben auf Ebene ganzer Codebasen besser bew\xe4ltigen kann. Dar\xfcber hinaus bietet das Modell starke Agenten-Codierungsunterst\xfctzung f\xfcr Plattformen wie Qwen Code und CLINE und verf\xfcgt \xfcber ein speziell entwickeltes Format f\xfcr Funktionsaufrufe."},"Qwen/Qwen3-Coder-480B-A35B-Instruct":{"description":"Qwen3-Coder-480B-A35B-Instruct wurde von Alibaba ver\xf6ffentlicht und ist bislang das agentischste Code-Modell. Es ist ein Mixture-of-Experts-(MoE)-Modell mit 480 Milliarden Gesamtparametern und 35 Milliarden aktivierten Parametern, das ein ausgewogenes Verh\xe4ltnis von Effizienz und Leistung bietet. Das Modell unterst\xfctzt nativ eine Kontextl\xe4nge von 256K (≈260.000) Token und l\xe4sst sich mittels Extrapolationsverfahren wie YaRN auf bis zu 1.000.000 Token erweitern, sodass es gro\xdfe Codebasen und komplexe Programmieraufgaben verarbeiten kann. Qwen3-Coder wurde f\xfcr agentenbasierte Coding-Workflows entwickelt: Es generiert nicht nur Code, sondern kann auch eigenst\xe4ndig mit Entwicklungswerkzeugen und -umgebungen interagieren, um komplexe Programmierprobleme zu l\xf6sen. In mehreren Benchmarks zu Coding- und Agentenaufgaben geh\xf6rt das Modell zu den Spitzenreitern unter Open-Source-Modellen und erreicht eine Leistungsf\xe4higkeit, die mit f\xfchrenden Modellen wie Claude Sonnet 4 vergleichbar ist."},"Qwen/Qwen3-Next-80B-A3B-Instruct":{"description":"Qwen3-Next-80B-A3B-Instruct ist ein von Alibaba Tongyi Qianwen Team ver\xf6ffentlichtes n\xe4chstes Generation Basis-Modell. Es basiert auf der neuen Qwen3-Next-Architektur und zielt darauf ab, h\xf6chste Trainings- und Inferenz-Effizienz zu erreichen. Das Modell verwendet einen innovativen hybriden Aufmerksamkeitsmechanismus (Gated DeltaNet und Gated Attention), eine hochgradig sp\xe4rliche Mixture-of-Experts (MoE)-Struktur sowie mehrere Optimierungen zur Trainingsstabilit\xe4t. Als ein sp\xe4rliches Modell mit insgesamt 80 Milliarden Parametern werden bei der Inferenz nur etwa 3 Milliarden Parameter aktiviert, was die Rechenkosten erheblich senkt. Bei der Verarbeitung von Langkontextaufgaben mit \xfcber 32K Tokens \xfcbertrifft der Durchsatz das Qwen3-32B-Modell um das Zehnfache. Dieses Modell ist eine instruktionsfeinabgestimmte Version, die f\xfcr allgemeine Aufgaben konzipiert ist und den Thinking-Modus nicht unterst\xfctzt. In puncto Leistung ist es in einigen Benchmarks vergleichbar mit dem Flaggschiff-Modell Qwen3-235B von Tongyi Qianwen, insbesondere zeigt es bei sehr langen Kontexten deutliche Vorteile."},"Qwen/Qwen3-Next-80B-A3B-Thinking":{"description":"Qwen3-Next-80B-A3B-Thinking ist ein von Alibaba Tongyi Qianwen Team ver\xf6ffentlichtes n\xe4chstes Generation Basis-Modell, das speziell f\xfcr komplexe Inferenzaufgaben entwickelt wurde. Es basiert auf der innovativen Qwen3-Next-Architektur, die hybride Aufmerksamkeitsmechanismen (Gated DeltaNet und Gated Attention) mit einer hochgradig sp\xe4rlichen Mixture-of-Experts (MoE)-Struktur kombiniert, um h\xf6chste Trainings- und Inferenz-Effizienz zu gew\xe4hrleisten. Als sp\xe4rliches Modell mit insgesamt 80 Milliarden Parametern werden bei der Inferenz nur etwa 3 Milliarden Parameter aktiviert, was die Rechenkosten stark reduziert. Bei der Verarbeitung von Langkontextaufgaben mit \xfcber 32K Tokens \xfcbertrifft der Durchsatz das Qwen3-32B-Modell um das Zehnfache. Diese „Thinking“-Version ist f\xfcr anspruchsvolle mehrstufige Aufgaben wie mathematische Beweise, Code-Synthese, logische Analyse und Planung optimiert und gibt den Inferenzprozess standardm\xe4\xdfig in strukturierter „Denkketten“-Form aus. In der Leistung \xfcbertrifft es nicht nur kostenintensivere Modelle wie Qwen3-32B-Thinking, sondern auch in mehreren Benchmarks das Gemini-2.5-Flash-Thinking."},"Qwen/Qwen3-Omni-30B-A3B-Captioner":{"description":"Qwen3-Omni-30B-A3B-Captioner ist ein visuelles Sprachmodell (VLM) aus der Qwen3-Serie des Alibaba Tongyi Qianwen-Teams. Es ist speziell darauf ausgelegt, hochwertige, detaillierte und pr\xe4zise Bildbeschreibungen zu generieren. Das Modell basiert auf einer Mixture-of-Experts (MoE)-Architektur mit insgesamt 30 Milliarden Parametern und ist in der Lage, Bildinhalte tiefgreifend zu verstehen und in nat\xfcrlich flie\xdfende Textbeschreibungen umzuwandeln. Es \xfcberzeugt durch exzellente Leistungen in Bereichen wie Detailerkennung, Szenenverst\xe4ndnis, Objekterkennung und Beziehungslogik und eignet sich besonders f\xfcr Anwendungen, die pr\xe4zises Bildverst\xe4ndnis und Beschreibungsgenerierung erfordern."},"Qwen/Qwen3-Omni-30B-A3B-Instruct":{"description":"Qwen3-Omni-30B-A3B-Instruct ist ein Modell aus der neuesten Qwen3-Serie des Alibaba Tongyi Qianwen-Teams. Es handelt sich um ein Mixture-of-Experts (MoE)-Modell mit insgesamt 30 Milliarden Parametern und 3 Milliarden aktiven Parametern, das starke Leistung bei gleichzeitig reduzierten Inferenzkosten bietet. Das Modell wurde mit hochwertigen, vielf\xe4ltigen und mehrsprachigen Daten trainiert und verf\xfcgt \xfcber umfassende F\xe4higkeiten zur Verarbeitung multimodaler Eingaben, darunter Text, Bild, Audio und Video. Es kann Inhalte \xfcber verschiedene Modalit\xe4ten hinweg verstehen und generieren."},"Qwen/Qwen3-Omni-30B-A3B-Thinking":{"description":"Qwen3-Omni-30B-A3B-Thinking ist die zentrale \\"Denkkomponente\\" (Thinker) innerhalb des multimodalen Qwen3-Omni-Modells. Sie ist speziell daf\xfcr konzipiert, komplexe Denkketten und Schlussfolgerungen \xfcber multimodale Eingaben wie Text, Audio, Bilder und Videos hinweg zu verarbeiten. Als das \\"Gehirn\\" der Inferenz vereinheitlicht dieses Modell alle Eingaben in einem gemeinsamen Repr\xe4sentationsraum und erm\xf6glicht so tiefes Verst\xe4ndnis und komplexe Schlussfolgerungen \xfcber Modalit\xe4ten hinweg. Es basiert auf einer Mixture-of-Experts (MoE)-Architektur mit 30 Milliarden Gesamtparametern und 3 Milliarden aktiven Parametern und bietet starke Inferenzf\xe4higkeiten bei optimierter Rechenleistung."},"Qwen/Qwen3-VL-235B-A22B-Instruct":{"description":"Qwen3-VL-235B-A22B-Instruct ist ein gro\xdf angelegtes, instruktional feinabgestimmtes Modell der Qwen3-VL-Serie. Es basiert auf einer Mixture-of-Experts (MoE)-Architektur und bietet herausragende multimodale Verst\xe4ndnis- und Generierungsf\xe4higkeiten. Mit nativer Unterst\xfctzung f\xfcr 256K Kontextl\xe4nge eignet es sich ideal f\xfcr hochgradig parallele, produktionsreife multimodale Dienste."},"Qwen/Qwen3-VL-235B-A22B-Thinking":{"description":"Qwen3-VL-235B-A22B-Thinking ist die Flaggschiff-Version der Qwen3-VL-Serie mit Fokus auf Denkprozesse. Es wurde speziell f\xfcr komplexe multimodale Schlussfolgerungen, langkontextuelles Denken und Interaktionen mit Agenten optimiert und eignet sich f\xfcr unternehmensweite Szenarien, die tiefgreifendes Denken und visuelle Inferenz erfordern."},"Qwen/Qwen3-VL-30B-A3B-Instruct":{"description":"Qwen3-VL-30B-A3B-Instruct ist eine instruktional feinabgestimmte Version der Qwen3-VL-Serie mit leistungsstarken F\xe4higkeiten im Bereich visuell-sprachliches Verst\xe4ndnis und Generierung. Es unterst\xfctzt nativ eine Kontextl\xe4nge von 256K und eignet sich f\xfcr multimodale Dialoge und bildkonditionierte Generierungsaufgaben."},"Qwen/Qwen3-VL-30B-A3B-Thinking":{"description":"Qwen3-VL-30B-A3B-Thinking ist die reasoning-optimierte Version (Thinking) der Qwen3-VL-Serie. Sie wurde f\xfcr multimodale Schlussfolgerungen, Bild-zu-Code-Generierung und komplexe visuelle Verst\xe4ndnisaufgaben optimiert. Mit Unterst\xfctzung f\xfcr 256K Kontext bietet sie eine verbesserte F\xe4higkeit zum kettenbasierten Denken."},"Qwen/Qwen3-VL-32B-Instruct":{"description":"Qwen3-VL-32B-Instruct ist ein visuelles Sprachmodell des Alibaba Tongyi Qianwen-Teams, das in mehreren Benchmarks f\xfcr visuelle Sprachverarbeitung f\xfchrende SOTA-Ergebnisse erzielt hat. Das Modell unterst\xfctzt hochaufl\xf6sende Bildeingaben im Megapixelbereich und bietet starke F\xe4higkeiten in allgemeinem visuellen Verst\xe4ndnis, mehrsprachiger Texterkennung (OCR), feink\xf6rniger visueller Lokalisierung und visueller Dialogf\xfchrung. Als Teil der Qwen3-Serie ist es in der Lage, komplexe multimodale Aufgaben zu bew\xe4ltigen und unterst\xfctzt fortgeschrittene Funktionen wie Tool-Aufrufe und Pr\xe4fix-Fortsetzungen."},"Qwen/Qwen3-VL-32B-Thinking":{"description":"Qwen3-VL-32B-Thinking ist eine speziell f\xfcr komplexe visuelle Schlussfolgerungsaufgaben optimierte Version des visuellen Sprachmodells vom Alibaba Tongyi Qianwen-Team. Das Modell verf\xfcgt \xfcber einen integrierten \\"Denkmodus\\", der es ihm erm\xf6glicht, vor der Beantwortung von Fragen detaillierte Zwischenschritte der Argumentation zu generieren. Dadurch wird seine Leistung bei Aufgaben mit mehrstufiger Logik, Planung und komplexem Denken erheblich verbessert. Es unterst\xfctzt hochaufl\xf6sende Bildeingaben im Megapixelbereich und bietet starke F\xe4higkeiten in allgemeinem visuellen Verst\xe4ndnis, mehrsprachiger OCR, feink\xf6rniger visueller Lokalisierung und visueller Dialogf\xfchrung sowie Funktionen wie Tool-Aufrufe und Pr\xe4fix-Fortsetzungen."},"Qwen/Qwen3-VL-8B-Instruct":{"description":"Qwen3-VL-8B-Instruct ist ein visuelles Sprachmodell der Qwen3-Serie, basierend auf Qwen3-8B-Instruct und auf umfangreichen Bild-Text-Daten trainiert. Es ist spezialisiert auf allgemeines visuelles Verst\xe4ndnis, visuell zentrierte Dialoge und mehrsprachige Texterkennung in Bildern. Es eignet sich f\xfcr Szenarien wie visuelle Frage-Antwort-Systeme, Bildbeschreibungen, multimodale Befehlsausf\xfchrung und Tool-Integration."},"Qwen/Qwen3-VL-8B-Thinking":{"description":"Qwen3-VL-8B-Thinking ist die Version der Qwen3-Serie mit Fokus auf visuelles Denken. Sie wurde f\xfcr komplexe, mehrstufige Schlussfolgerungsaufgaben optimiert und generiert standardm\xe4\xdfig eine schrittweise Denkweise (Thinking Chain), um die Genauigkeit der Schlussfolgerungen zu verbessern. Ideal f\xfcr Szenarien, die tiefgreifende visuelle Analysen erfordern, wie visuelle Frage-Antwort-Systeme oder die detaillierte Auswertung von Bildinhalten."},"Qwen2-72B-Instruct":{"description":"Qwen2 ist die neueste Reihe des Qwen-Modells, das 128k Kontext unterst\xfctzt. Im Vergleich zu den derzeit besten Open-Source-Modellen \xfcbertrifft Qwen2-72B in den Bereichen nat\xfcrliche Sprachverst\xe4ndnis, Wissen, Code, Mathematik und Mehrsprachigkeit deutlich die f\xfchrenden Modelle."},"Qwen2-7B-Instruct":{"description":"Qwen2 ist die neueste Reihe des Qwen-Modells, das in der Lage ist, die besten Open-Source-Modelle \xe4hnlicher Gr\xf6\xdfe oder sogar gr\xf6\xdferer Modelle zu \xfcbertreffen. Qwen2 7B hat in mehreren Bewertungen signifikante Vorteile erzielt, insbesondere im Bereich Code und Verst\xe4ndnis der chinesischen Sprache."},"Qwen2-VL-72B":{"description":"Qwen2-VL-72B ist ein leistungsstarkes visuelles Sprachmodell, das multimodale Verarbeitung von Bildern und Text unterst\xfctzt und in der Lage ist, Bildinhalte pr\xe4zise zu erkennen und relevante Beschreibungen oder Antworten zu generieren."},"Qwen2.5-14B-Instruct":{"description":"Qwen2.5-14B-Instruct ist ein gro\xdfes Sprachmodell mit 14 Milliarden Parametern, das hervorragende Leistungen bietet, f\xfcr chinesische und mehrsprachige Szenarien optimiert ist und Anwendungen wie intelligente Fragen und Antworten sowie Inhaltserstellung unterst\xfctzt."},"Qwen2.5-32B-Instruct":{"description":"Qwen2.5-32B-Instruct ist ein gro\xdfes Sprachmodell mit 32 Milliarden Parametern, das eine ausgewogene Leistung bietet, f\xfcr chinesische und mehrsprachige Szenarien optimiert ist und Anwendungen wie intelligente Fragen und Antworten sowie Inhaltserstellung unterst\xfctzt."},"Qwen2.5-72B-Instruct":{"description":"Qwen2.5-72B-Instruct unterst\xfctzt 16k Kontext und generiert lange Texte \xfcber 8K. Es unterst\xfctzt Funktionsaufrufe und nahtlose Interaktionen mit externen Systemen, was die Flexibilit\xe4t und Skalierbarkeit erheblich verbessert. Das Wissen des Modells hat deutlich zugenommen, und die Codierungs- und mathematischen F\xe4higkeiten wurden erheblich verbessert, mit Unterst\xfctzung f\xfcr \xfcber 29 Sprachen."},"Qwen2.5-7B-Instruct":{"description":"Qwen2.5-7B-Instruct ist ein gro\xdfes Sprachmodell mit 7 Milliarden Parametern, das Funktionsaufrufe unterst\xfctzt und nahtlos mit externen Systemen interagiert, was die Flexibilit\xe4t und Skalierbarkeit erheblich erh\xf6ht. Es ist f\xfcr chinesische und mehrsprachige Szenarien optimiert und unterst\xfctzt Anwendungen wie intelligente Fragen und Antworten sowie Inhaltserstellung."},"Qwen2.5-Coder-14B-Instruct":{"description":"Qwen2.5-Coder-14B-Instruct ist ein auf gro\xdffl\xe4chigem Pre-Training basierendes Programmiermodell, das \xfcber starke F\xe4higkeiten zur Codeverstehung und -generierung verf\xfcgt und effizient verschiedene Programmieraufgaben bearbeiten kann. Es eignet sich besonders gut f\xfcr intelligente Codeerstellung, automatisierte Skripterstellung und die Beantwortung von Programmierfragen."},"Qwen2.5-Coder-32B-Instruct":{"description":"Qwen2.5-Coder-32B-Instruct ist ein gro\xdfes Sprachmodell, das speziell f\xfcr die Codegenerierung, das Verst\xe4ndnis von Code und effiziente Entwicklungsszenarien entwickelt wurde. Es verwendet eine branchenf\xfchrende Parametergr\xf6\xdfe von 32B und kann vielf\xe4ltige Programmieranforderungen erf\xfcllen."},"Qwen3-235B":{"description":"Qwen3-235B-A22B ist ein MoE (Mixture-of-Experts)-Modell, das den „Hybrid-Reasoning-Modus“ einf\xfchrt und Nutzern nahtloses Umschalten zwischen „Denkmodus“ und „Nicht-Denkmodus“ erm\xf6glicht. Es unterst\xfctzt das Verst\xe4ndnis und die Argumentation in 119 Sprachen und Dialekten und verf\xfcgt \xfcber leistungsstarke Werkzeugaufruff\xe4higkeiten. In umfassenden Benchmark-Tests zu allgemeinen F\xe4higkeiten, Programmierung und Mathematik, Mehrsprachigkeit, Wissen und Argumentation konkurriert es mit f\xfchrenden aktuellen Gro\xdfmodellen auf dem Markt wie DeepSeek R1, OpenAI o1, o3-mini, Grok 3 und Google Gemini 2.5 Pro."},"Qwen3-235B-A22B-Instruct-2507-FP8":{"description":"Qwen3 235B A22B Instruct 2507: Ein Modell, optimiert f\xfcr fortgeschrittene Schlussfolgerungen und Dialoganweisungen, mit einer hybriden Expertenarchitektur, die bei gro\xdfem Parameterumfang die Inferenz-Effizienz bewahrt."},"Qwen3-32B":{"description":"Qwen3-32B ist ein dichtes Modell (Dense Model), das den „Hybrid-Reasoning-Modus“ einf\xfchrt und Nutzern nahtloses Umschalten zwischen „Denkmodus“ und „Nicht-Denkmodus“ erm\xf6glicht. Aufgrund von Verbesserungen in der Modellarchitektur, einer Erweiterung der Trainingsdaten und effizienteren Trainingsmethoden entspricht die Gesamtleistung der von Qwen2.5-72B."},"SenseChat":{"description":"Basisversion des Modells (V4) mit 4K Kontextl\xe4nge, die \xfcber starke allgemeine F\xe4higkeiten verf\xfcgt."},"SenseChat-128K":{"description":"Basisversion des Modells (V4) mit 128K Kontextl\xe4nge, das in Aufgaben des Verst\xe4ndnisses und der Generierung langer Texte hervorragende Leistungen zeigt."},"SenseChat-32K":{"description":"Basisversion des Modells (V4) mit 32K Kontextl\xe4nge, flexibel einsetzbar in verschiedenen Szenarien."},"SenseChat-5":{"description":"Die neueste Modellversion (V5.5) mit 128K Kontextl\xe4nge hat signifikante Verbesserungen in den Bereichen mathematische Schlussfolgerungen, englische Konversation, Befolgen von Anweisungen und Verst\xe4ndnis langer Texte, vergleichbar mit GPT-4o."},"SenseChat-5-1202":{"description":"Basierend auf der neuesten Version V5.5 zeigt es im Vergleich zur Vorg\xe4ngerversion deutliche Verbesserungen in den Bereichen Grundf\xe4higkeiten in Chinesisch und Englisch, Chat, naturwissenschaftliches Wissen, geisteswissenschaftliches Wissen, Schreiben, mathematische Logik und Wortzahlkontrolle."},"SenseChat-5-Cantonese":{"description":"Mit 32K Kontextl\xe4nge \xfcbertrifft es GPT-4 im Verst\xe4ndnis von Konversationen auf Kantonesisch und kann in mehreren Bereichen wie Wissen, Schlussfolgerungen, Mathematik und Programmierung mit GPT-4 Turbo konkurrieren."},"SenseChat-5-beta":{"description":"Teilweise bessere Leistung als SenseCat-5-1202"},"SenseChat-Character":{"description":"Standardmodell mit 8K Kontextl\xe4nge und hoher Reaktionsgeschwindigkeit."},"SenseChat-Character-Pro":{"description":"Premium-Modell mit 32K Kontextl\xe4nge, das umfassende Verbesserungen in den F\xe4higkeiten bietet und sowohl chinesische als auch englische Konversationen unterst\xfctzt."},"SenseChat-Turbo":{"description":"Geeignet f\xfcr schnelle Fragen und Antworten sowie Szenarien zur Feinabstimmung des Modells."},"SenseChat-Turbo-1202":{"description":"Dies ist das neueste leichte Modell, das \xfcber 90 % der F\xe4higkeiten des Vollmodells erreicht und die Kosten f\xfcr die Inferenz erheblich senkt."},"SenseChat-Vision":{"description":"Das neueste Modell (V5.5) unterst\xfctzt die Eingabe mehrerer Bilder und optimiert umfassend die grundlegenden F\xe4higkeiten des Modells. Es hat signifikante Verbesserungen in der Erkennung von Objektattributen, r\xe4umlichen Beziehungen, Aktionsereignissen, Szenenverst\xe4ndnis, Emotionserkennung, logischem Wissen und Textverst\xe4ndnis und -generierung erreicht."},"SenseNova-V6-5-Pro":{"description":"Durch umfassende Aktualisierungen multimodaler, sprachlicher und argumentativer Daten sowie Optimierungen der Trainingsstrategie erzielt das neue Modell erhebliche Verbesserungen bei multimodalem Schlie\xdfen und generalisierter Befolgung von Anweisungen. Es unterst\xfctzt Kontextfenster von bis zu 128k und zeigt herausragende Leistungen bei spezialisierten Aufgaben wie OCR und der Erkennung von Tourismus-IP."},"SenseNova-V6-5-Turbo":{"description":"Durch umfassende Aktualisierungen multimodaler, sprachlicher und argumentativer Daten sowie Optimierungen der Trainingsstrategie erzielt das neue Modell erhebliche Verbesserungen bei multimodalem Schlie\xdfen und generalisierter Befolgung von Anweisungen. Es unterst\xfctzt Kontextfenster von bis zu 128k und zeigt herausragende Leistungen bei spezialisierten Aufgaben wie OCR und der Erkennung von Tourismus-IP."},"SenseNova-V6-Pro":{"description":"Erreicht eine native Einheit von Bild-, Text- und Video-F\xe4higkeiten, \xfcberwindet die traditionellen Grenzen der multimodalen Trennung und hat in den Bewertungen von OpenCompass und SuperCLUE zwei Meistertitel gewonnen."},"SenseNova-V6-Reasoner":{"description":"Vereint visuelle und sprachliche Tiefenlogik, erm\xf6glicht langsames Denken und tiefgreifende Schlussfolgerungen und pr\xe4sentiert den vollst\xe4ndigen Denkprozess."},"SenseNova-V6-Turbo":{"description":"Erreicht eine native Einheit von Bild-, Text- und Video-F\xe4higkeiten, \xfcberwindet die traditionellen Grenzen der multimodalen Trennung und f\xfchrt in den Kernbereichen wie multimodalen Grundf\xe4higkeiten und sprachlichen Grundf\xe4higkeiten umfassend. Es kombiniert literarische und wissenschaftliche Bildung und hat in mehreren Bewertungen mehrfach die Spitzenposition im In- und Ausland erreicht."},"Skylark2-lite-8k":{"description":"Das zweite Modell der Skylark-Reihe, das Skylark2-lite-Modell bietet eine hohe Reaktionsgeschwindigkeit und eignet sich f\xfcr Szenarien mit hohen Echtzeitanforderungen, kostensensitiven Anforderungen und geringeren Genauigkeitsanforderungen, mit einer Kontextfensterl\xe4nge von 8k."},"Skylark2-pro-32k":{"description":"Das zweite Modell der Skylark-Reihe, die Skylark2-pro-Version hat eine hohe Modellgenauigkeit und eignet sich f\xfcr komplexere Textgenerierungsszenarien, wie z. B. professionelle Texterstellung, Romankreation und hochwertige \xdcbersetzungen, mit einer Kontextfensterl\xe4nge von 32k."},"Skylark2-pro-4k":{"description":"Das zweite Modell der Skylark-Reihe, die Skylark2-pro-Version hat eine hohe Modellgenauigkeit und eignet sich f\xfcr komplexere Textgenerierungsszenarien, wie z. B. professionelle Texterstellung, Romankreation und hochwertige \xdcbersetzungen, mit einer Kontextfensterl\xe4nge von 4k."},"Skylark2-pro-character-4k":{"description":"Das zweite Modell der Skylark-Reihe, das Skylark2-pro-character-Modell hat hervorragende F\xe4higkeiten im Rollenspiel und Chat, kann sich entsprechend den Anforderungen des Benutzers verkleiden und bietet nat\xfcrliche und fl\xfcssige Dialoginhalte. Es eignet sich f\xfcr den Aufbau von Chatbots, virtuellen Assistenten und Online-Kundensupport und bietet eine hohe Reaktionsgeschwindigkeit."},"Skylark2-pro-turbo-8k":{"description":"Das zweite Modell der Skylark-Reihe, das Skylark2-pro-turbo-8k bietet schnellere Schlussfolgerungen und niedrigere Kosten, mit einer Kontextfensterl\xe4nge von 8k."},"THUDM/GLM-4-32B-0414":{"description":"GLM-4-32B-0414 ist das neue Open-Source-Modell der GLM-Serie mit 32 Milliarden Parametern. Die Leistung dieses Modells kann mit der GPT-Serie von OpenAI und der V3/R1-Serie von DeepSeek verglichen werden."},"THUDM/GLM-4-9B-0414":{"description":"GLM-4-9B-0414 ist ein kleines Modell der GLM-Serie mit 9 Milliarden Parametern. Dieses Modell \xfcbernimmt die technischen Merkmale der GLM-4-32B-Serie, bietet jedoch eine leichtere Bereitstellungsoption. Trotz seiner kleineren Gr\xf6\xdfe zeigt GLM-4-9B-0414 hervorragende F\xe4higkeiten in Aufgaben wie Codegenerierung, Webdesign, SVG-Grafikgenerierung und suchbasiertem Schreiben."},"THUDM/GLM-4.1V-9B-Thinking":{"description":"GLM-4.1V-9B-Thinking ist ein von Zhipu AI und dem KEG-Labor der Tsinghua-Universit\xe4t gemeinsam ver\xf6ffentlichtes Open-Source-Visuell-Sprachmodell (VLM), das speziell f\xfcr die Bew\xe4ltigung komplexer multimodaler kognitiver Aufgaben entwickelt wurde. Das Modell basiert auf dem GLM-4-9B-0414-Grundmodell und verbessert durch die Einf\xfchrung des „Chain-of-Thought“-Schlussmechanismus und den Einsatz von Verst\xe4rkungslernstrategien seine multimodale Schlussfolgerungsf\xe4higkeit und Stabilit\xe4t erheblich."},"THUDM/GLM-Z1-32B-0414":{"description":"GLM-Z1-32B-0414 ist ein Schlussfolgerungsmodell mit tiefen Denkf\xe4higkeiten. Dieses Modell wurde auf der Grundlage von GLM-4-32B-0414 durch Kaltstart und verst\xe4rktes Lernen entwickelt und wurde weiter in Mathematik, Programmierung und logischen Aufgaben trainiert. Im Vergleich zum Basismodell hat GLM-Z1-32B-0414 die mathematischen F\xe4higkeiten und die F\xe4higkeit zur L\xf6sung komplexer Aufgaben erheblich verbessert."},"THUDM/GLM-Z1-9B-0414":{"description":"GLM-Z1-9B-0414 ist ein kleines Modell der GLM-Serie mit nur 9 Milliarden Parametern, das jedoch erstaunliche F\xe4higkeiten zeigt, w\xe4hrend es die Open-Source-Tradition beibeh\xe4lt. Trotz seiner kleineren Gr\xf6\xdfe zeigt dieses Modell hervorragende Leistungen in mathematischen Schlussfolgerungen und allgemeinen Aufgaben und hat in seiner Gr\xf6\xdfenklasse eine f\xfchrende Gesamtleistung unter Open-Source-Modellen."},"THUDM/GLM-Z1-Rumination-32B-0414":{"description":"GLM-Z1-Rumination-32B-0414 ist ein tiefes Schlussfolgerungsmodell mit nachdenklichen F\xe4higkeiten (vergleichbar mit OpenAI\'s Deep Research). Im Gegensatz zu typischen tiefen Denkmodellen verwendet das nachdenkliche Modell l\xe4ngere Zeitr\xe4ume des tiefen Denkens, um offenere und komplexere Probleme zu l\xf6sen."},"THUDM/glm-4-9b-chat":{"description":"GLM-4 9B ist die Open-Source-Version, die ein optimiertes Dialogerlebnis f\xfcr Konversationsanwendungen bietet."},"Tongyi-Zhiwen/QwenLong-L1-32B":{"description":"QwenLong-L1-32B ist das erste gro\xdfe Langkontext-Inferenzmodell (LRM), das mit verst\xe4rkendem Lernen trainiert wurde und speziell f\xfcr Langtext-Inferenzaufgaben optimiert ist. Das Modell erreicht durch ein progressives Kontext-Erweiterungs-Framework eine stabile \xdcbertragung von kurzen zu langen Kontexten. In sieben Langkontext-Dokumenten-Q&A-Benchmarks \xfcbertrifft QwenLong-L1-32B Flaggschiffmodelle wie OpenAI-o3-mini und Qwen3-235B-A22B und erreicht eine Leistung vergleichbar mit Claude-3.7-Sonnet-Thinking. Es ist besonders stark in komplexen Aufgaben wie mathematischer, logischer und mehrstufiger Inferenz."},"Yi-34B-Chat":{"description":"Yi-1.5-34B hat die hervorragenden allgemeinen Sprachf\xe4higkeiten des urspr\xfcnglichen Modells beibehalten und durch inkrementelles Training von 500 Milliarden hochwertigen Tokens die mathematische Logik und Codierungsf\xe4higkeiten erheblich verbessert."},"abab5.5-chat":{"description":"F\xfcr produktivit\xe4tsorientierte Szenarien konzipiert, unterst\xfctzt es die Verarbeitung komplexer Aufgaben und die effiziente Textgenerierung, geeignet f\xfcr professionelle Anwendungen."},"abab5.5s-chat":{"description":"Speziell f\xfcr chinesische Charakterdialoge konzipiert, bietet es hochwertige chinesische Dialoggenerierung und ist f\xfcr verschiedene Anwendungsszenarien geeignet."},"abab6.5g-chat":{"description":"Speziell f\xfcr mehrsprachige Charakterdialoge konzipiert, unterst\xfctzt die hochwertige Dialoggenerierung in Englisch und anderen Sprachen."},"abab6.5s-chat":{"description":"Geeignet f\xfcr eine Vielzahl von Aufgaben der nat\xfcrlichen Sprachverarbeitung, einschlie\xdflich Textgenerierung und Dialogsystemen."},"abab6.5t-chat":{"description":"F\xfcr chinesische Charakterdialoge optimiert, bietet es fl\xfcssige und den chinesischen Ausdrucksgewohnheiten entsprechende Dialoggenerierung."},"accounts/fireworks/models/deepseek-r1":{"description":"DeepSeek-R1 ist ein hochmodernes gro\xdfes Sprachmodell, das durch verst\xe4rktes Lernen und Optimierung mit Kaltstartdaten hervorragende Leistungen in Inferenz, Mathematik und Programmierung bietet."},"accounts/fireworks/models/deepseek-v3":{"description":"Ein leistungsstarkes Mixture-of-Experts (MoE) Sprachmodell von Deepseek mit insgesamt 671B Parametern, wobei 37B Parameter pro Token aktiviert werden."},"accounts/fireworks/models/llama-v3-70b-instruct":{"description":"Das Llama 3 70B Instruct-Modell ist speziell f\xfcr mehrsprachige Dialoge und nat\xfcrliche Sprachverst\xe4ndnis optimiert und \xfcbertrifft die meisten Wettbewerbsmodelle."},"accounts/fireworks/models/llama-v3-8b-instruct":{"description":"Das Llama 3 8B Instruct-Modell ist f\xfcr Dialoge und mehrsprachige Aufgaben optimiert und bietet hervorragende und effiziente Leistungen."},"accounts/fireworks/models/llama-v3-8b-instruct-hf":{"description":"Das Llama 3 8B Instruct-Modell (HF-Version) stimmt mit den offiziellen Ergebnissen \xfcberein und bietet hohe Konsistenz und plattform\xfcbergreifende Kompatibilit\xe4t."},"accounts/fireworks/models/llama-v3p1-405b-instruct":{"description":"Das Llama 3.1 405B Instruct-Modell verf\xfcgt \xfcber eine extrem gro\xdfe Anzahl von Parametern und eignet sich f\xfcr komplexe Aufgaben und Anweisungsverfolgung in hochbelasteten Szenarien."},"accounts/fireworks/models/llama-v3p1-70b-instruct":{"description":"Das Llama 3.1 70B Instruct-Modell bietet hervorragende nat\xfcrliche Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und ist die ideale Wahl f\xfcr Dialog- und Analyseaufgaben."},"accounts/fireworks/models/llama-v3p1-8b-instruct":{"description":"Das Llama 3.1 8B Instruct-Modell ist speziell f\xfcr mehrsprachige Dialoge optimiert und kann die meisten Open-Source- und Closed-Source-Modelle in g\xe4ngigen Branchenbenchmarks \xfcbertreffen."},"accounts/fireworks/models/llama-v3p2-11b-vision-instruct":{"description":"Meta\'s 11B Parameter instruct-Modell f\xfcr Bildverarbeitung. Dieses Modell ist optimiert f\xfcr visuelle Erkennung, Bildverarbeitung, Bildbeschreibung und die Beantwortung allgemeiner Fragen zu Bildern. Es kann visuelle Daten wie Diagramme und Grafiken verstehen und schlie\xdft die L\xfccke zwischen visuellen und sprachlichen Informationen, indem es textuelle Beschreibungen der Bilddetails generiert."},"accounts/fireworks/models/llama-v3p2-3b-instruct":{"description":"Llama 3.2 3B instruct-Modell ist ein leichtgewichtiges mehrsprachiges Modell, das von Meta ver\xf6ffentlicht wurde. Dieses Modell zielt darauf ab, die Effizienz zu steigern und bietet im Vergleich zu gr\xf6\xdferen Modellen signifikante Verbesserungen in Bezug auf Latenz und Kosten. Anwendungsbeispiele f\xfcr dieses Modell sind Abfragen und Aufforderungsneuschreibungen sowie Schreibassistenz."},"accounts/fireworks/models/llama-v3p2-90b-vision-instruct":{"description":"Meta\'s 90B Parameter instruct-Modell f\xfcr Bildverarbeitung. Dieses Modell ist optimiert f\xfcr visuelle Erkennung, Bildverarbeitung, Bildbeschreibung und die Beantwortung allgemeiner Fragen zu Bildern. Es kann visuelle Daten wie Diagramme und Grafiken verstehen und schlie\xdft die L\xfccke zwischen visuellen und sprachlichen Informationen, indem es textuelle Beschreibungen der Bilddetails generiert."},"accounts/fireworks/models/llama-v3p3-70b-instruct":{"description":"Llama 3.3 70B Instruct ist die aktualisierte Version von Llama 3.1 70B aus dem Dezember. Dieses Modell wurde auf der Grundlage von Llama 3.1 70B (ver\xf6ffentlicht im Juli 2024) verbessert und bietet erweiterte Funktionen f\xfcr Toolaufrufe, mehrsprachige Textunterst\xfctzung sowie mathematische und Programmierf\xe4higkeiten. Das Modell erreicht branchenf\xfchrende Leistungen in den Bereichen Inferenz, Mathematik und Befehlsbefolgung und bietet eine \xe4hnliche Leistung wie 3.1 405B, w\xe4hrend es gleichzeitig signifikante Vorteile in Bezug auf Geschwindigkeit und Kosten bietet."},"accounts/fireworks/models/mistral-small-24b-instruct-2501":{"description":"Ein 24B-Parameter-Modell mit fortschrittlichen F\xe4higkeiten, die mit gr\xf6\xdferen Modellen vergleichbar sind."},"accounts/fireworks/models/mixtral-8x22b-instruct":{"description":"Das Mixtral MoE 8x22B Instruct-Modell unterst\xfctzt durch seine gro\xdfe Anzahl an Parametern und Multi-Expert-Architektur die effiziente Verarbeitung komplexer Aufgaben."},"accounts/fireworks/models/mixtral-8x7b-instruct":{"description":"Das Mixtral MoE 8x7B Instruct-Modell bietet durch seine Multi-Expert-Architektur effiziente Anweisungsverfolgung und -ausf\xfchrung."},"accounts/fireworks/models/mythomax-l2-13b":{"description":"Das MythoMax L2 13B-Modell kombiniert neuartige Kombinations-Technologien und ist besonders gut in Erz\xe4hlungen und Rollenspielen."},"accounts/fireworks/models/phi-3-vision-128k-instruct":{"description":"Das Phi 3 Vision Instruct-Modell ist ein leichtgewichtiges multimodales Modell, das komplexe visuelle und textuelle Informationen verarbeiten kann und \xfcber starke Schlussfolgerungsf\xe4higkeiten verf\xfcgt."},"accounts/fireworks/models/qwen-qwq-32b-preview":{"description":"Das QwQ-Modell ist ein experimentelles Forschungsmodell, das vom Qwen-Team entwickelt wurde und sich auf die Verbesserung der KI-Inferenzf\xe4higkeiten konzentriert."},"accounts/fireworks/models/qwen2-vl-72b-instruct":{"description":"Die 72B-Version des Qwen-VL-Modells ist das neueste Ergebnis von Alibabas Iteration und repr\xe4sentiert fast ein Jahr an Innovation."},"accounts/fireworks/models/qwen2p5-72b-instruct":{"description":"Qwen2.5 ist eine Reihe von Sprachmodellen mit ausschlie\xdflich Decodern, die vom Alibaba Cloud Qwen-Team entwickelt wurde. Diese Modelle sind in verschiedenen Gr\xf6\xdfen erh\xe4ltlich, darunter 0.5B, 1.5B, 3B, 7B, 14B, 32B und 72B, mit Basis- und instruct-Varianten."},"accounts/fireworks/models/qwen2p5-coder-32b-instruct":{"description":"Qwen2.5 Coder 32B Instruct ist die neueste Version der von Alibaba Cloud ver\xf6ffentlichten Reihe von code-spezifischen gro\xdfen Sprachmodellen. Dieses Modell basiert auf Qwen2.5 und wurde mit 55 Billionen Tokens trainiert, um die F\xe4higkeiten zur Codegenerierung, Inferenz und Fehlerbehebung erheblich zu verbessern. Es verbessert nicht nur die Codierungsf\xe4higkeiten, sondern bewahrt auch die Vorteile in Mathematik und allgemeinen F\xe4higkeiten. Das Modell bietet eine umfassendere Grundlage f\xfcr praktische Anwendungen wie Code-Agenten."},"accounts/yi-01-ai/models/yi-large":{"description":"Das Yi-Large-Modell bietet hervorragende mehrsprachige Verarbeitungsf\xe4higkeiten und kann f\xfcr verschiedene Sprachgenerierungs- und Verst\xe4ndnisaufgaben eingesetzt werden."},"ai21-jamba-1.5-large":{"description":"Ein mehrsprachiges Modell mit 398 Milliarden Parametern (94 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und fundierte Generierung bietet."},"ai21-jamba-1.5-mini":{"description":"Ein mehrsprachiges Modell mit 52 Milliarden Parametern (12 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und fundierte Generierung bietet."},"ai21-labs/AI21-Jamba-1.5-Large":{"description":"Ein mehrsprachiges Modell mit 398 Milliarden Parametern (davon 94 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und faktengest\xfctzte Generierung bietet."},"ai21-labs/AI21-Jamba-1.5-Mini":{"description":"Ein mehrsprachiges Modell mit 52 Milliarden Parametern (davon 12 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und faktengest\xfctzte Generierung bietet."},"alibaba/qwen-3-14b":{"description":"Qwen3 ist das neueste gro\xdfe Sprachmodell der Qwen-Serie und bietet eine umfassende Palette an dichten und gemischten Experten (MoE) Modellen. Basierend auf umfangreichem Training erzielt Qwen3 bahnbrechende Fortschritte in den Bereichen Inferenz, Befolgung von Anweisungen, Agentenf\xe4higkeiten und mehrsprachige Unterst\xfctzung."},"alibaba/qwen-3-235b":{"description":"Qwen3 ist das neueste gro\xdfe Sprachmodell der Qwen-Serie und bietet eine umfassende Palette an dichten und gemischten Experten (MoE) Modellen. Basierend auf umfangreichem Training erzielt Qwen3 bahnbrechende Fortschritte in den Bereichen Inferenz, Befolgung von Anweisungen, Agentenf\xe4higkeiten und mehrsprachige Unterst\xfctzung."},"alibaba/qwen-3-30b":{"description":"Qwen3 ist das neueste gro\xdfe Sprachmodell der Qwen-Serie und bietet eine umfassende Palette an dichten und gemischten Experten (MoE) Modellen. Basierend auf umfangreichem Training erzielt Qwen3 bahnbrechende Fortschritte in den Bereichen Inferenz, Befolgung von Anweisungen, Agentenf\xe4higkeiten und mehrsprachige Unterst\xfctzung."},"alibaba/qwen-3-32b":{"description":"Qwen3 ist das neueste gro\xdfe Sprachmodell der Qwen-Serie und bietet eine umfassende Palette an dichten und gemischten Experten (MoE) Modellen. Basierend auf umfangreichem Training erzielt Qwen3 bahnbrechende Fortschritte in den Bereichen Inferenz, Befolgung von Anweisungen, Agentenf\xe4higkeiten und mehrsprachige Unterst\xfctzung."},"alibaba/qwen3-coder":{"description":"Qwen3-Coder-480B-A35B-Instruct ist das agentenf\xe4higste Codierungsmodell von Qwen mit herausragender Leistung bei Agenten-Codierung, Agenten-Browsernutzung und anderen grundlegenden Codierungsaufgaben, vergleichbar mit Claude Sonnet."},"amazon/nova-lite":{"description":"Ein \xe4u\xdferst kosteng\xfcnstiges multimodales Modell, das Bilder, Videos und Texteingaben extrem schnell verarbeitet."},"amazon/nova-micro":{"description":"Ein reines Textmodell, das bei sehr niedrigen Kosten die geringste Latenz f\xfcr Antworten bietet."},"amazon/nova-pro":{"description":"Ein hochkompetentes multimodales Modell mit optimaler Kombination aus Genauigkeit, Geschwindigkeit und Kosten, geeignet f\xfcr eine breite Palette von Aufgaben."},"amazon/titan-embed-text-v2":{"description":"Amazon Titan Text Embeddings V2 ist ein leichtgewichtiges, effizientes mehrsprachiges Einbettungsmodell mit Unterst\xfctzung f\xfcr 1024, 512 und 256 Dimensionen."},"anthropic.claude-3-5-sonnet-20240620-v1:0":{"description":"Claude 3.5 Sonnet hebt den Branchenstandard an, \xfcbertrifft die Konkurrenzmodelle und Claude 3 Opus und zeigt in umfassenden Bewertungen hervorragende Leistungen, w\xe4hrend es die Geschwindigkeit und Kosten unserer mittleren Modelle beibeh\xe4lt."},"anthropic.claude-3-5-sonnet-20241022-v2:0":{"description":"Claude 3.5 Sonnet setzt neue Ma\xdfst\xe4be in der Branche, \xfcbertrifft die Modelle der Konkurrenz und Claude 3 Opus, und zeigt in umfassenden Bewertungen hervorragende Leistungen, w\xe4hrend es die Geschwindigkeit und Kosten unserer mittelgro\xdfen Modelle beibeh\xe4lt."},"anthropic.claude-3-haiku-20240307-v1:0":{"description":"Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic und bietet nahezu sofortige Reaktionsgeschwindigkeiten. Es kann schnell einfache Anfragen und Anforderungen beantworten. Kunden werden in der Lage sein, nahtlose AI-Erlebnisse zu schaffen, die menschliche Interaktionen nachahmen. Claude 3 Haiku kann Bilder verarbeiten und Textausgaben zur\xfcckgeben, mit einem Kontextfenster von 200K."},"anthropic.claude-3-opus-20240229-v1:0":{"description":"Claude 3 Opus ist das leistungsst\xe4rkste AI-Modell von Anthropic mit fortschrittlicher Leistung bei hochkomplexen Aufgaben. Es kann offene Eingaben und unbekannte Szenarien verarbeiten und zeigt hervorragende Fl\xfcssigkeit und menschen\xe4hnliches Verst\xe4ndnis. Claude 3 Opus demonstriert die Grenzen der M\xf6glichkeiten generativer AI. Claude 3 Opus kann Bilder verarbeiten und Textausgaben zur\xfcckgeben, mit einem Kontextfenster von 200K."},"anthropic.claude-3-sonnet-20240229-v1:0":{"description":"Anthropic\'s Claude 3 Sonnet erreicht ein ideales Gleichgewicht zwischen Intelligenz und Geschwindigkeit – besonders geeignet f\xfcr Unternehmensarbeitslasten. Es bietet maximalen Nutzen zu einem Preis, der unter dem der Konkurrenz liegt, und wurde als zuverl\xe4ssiges, langlebiges Hauptmodell f\xfcr skalierbare AI-Implementierungen konzipiert. Claude 3 Sonnet kann Bilder verarbeiten und Textausgaben zur\xfcckgeben, mit einem Kontextfenster von 200K."},"anthropic.claude-instant-v1":{"description":"Ein schnelles, kosteng\xfcnstiges und dennoch sehr leistungsf\xe4higes Modell, das eine Reihe von Aufgaben bew\xe4ltigen kann, darunter allt\xe4gliche Gespr\xe4che, Textanalysen, Zusammenfassungen und Dokumentenfragen."},"anthropic.claude-v2":{"description":"Anthropic zeigt in einer Vielzahl von Aufgaben, von komplexen Dialogen und kreativer Inhaltserstellung bis hin zu detaillierten Anweisungen, ein hohes Ma\xdf an F\xe4higkeiten."},"anthropic.claude-v2:1":{"description":"Die aktualisierte Version von Claude 2 bietet ein doppelt so gro\xdfes Kontextfenster sowie Verbesserungen in der Zuverl\xe4ssigkeit, der Halluzinationsrate und der evidenzbasierten Genauigkeit in langen Dokumenten und RAG-Kontexten."},"anthropic/claude-3-haiku":{"description":"Claude 3 Haiku ist das bisher schnellste Modell von Anthropic, speziell f\xfcr Unternehmens-Workloads mit meist l\xe4ngeren Eingabeaufforderungen entwickelt. Haiku kann gro\xdfe Dokumentenmengen wie Quartalsberichte, Vertr\xe4ge oder Rechtsf\xe4lle schnell analysieren und kostet dabei nur die H\xe4lfte anderer Modelle seiner Leistungsklasse."},"anthropic/claude-3-opus":{"description":"Claude 3 Opus ist das intelligenteste Modell von Anthropic mit marktf\xfchrender Leistung bei hochkomplexen Aufgaben. Es meistert offene Eingabeaufforderungen und unbekannte Szenarien mit herausragender Fl\xfcssigkeit und menschen\xe4hnlichem Verst\xe4ndnis."},"anthropic/claude-3.5-haiku":{"description":"Claude 3.5 Haiku ist die n\xe4chste Generation unseres schnellsten Modells. Mit \xe4hnlicher Geschwindigkeit wie Claude 3 Haiku wurde Claude 3.5 Haiku in allen Kompetenzbereichen verbessert und \xfcbertrifft in vielen Intelligenz-Benchmarks unser bisher gr\xf6\xdftes Modell Claude 3 Opus."},"anthropic/claude-3.5-sonnet":{"description":"Claude 3.5 Sonnet erreicht eine ideale Balance zwischen Intelligenz und Geschwindigkeit – besonders f\xfcr Unternehmens-Workloads. Im Vergleich zu \xe4hnlichen Produkten bietet es starke Leistung zu geringeren Kosten und ist f\xfcr hohe Belastbarkeit bei gro\xdffl\xe4chigen KI-Eins\xe4tzen konzipiert."},"anthropic/claude-3.7-sonnet":{"description":"Claude 3.7 Sonnet ist das erste hybride Inferenzmodell und das intelligenteste Modell von Anthropic bisher. Es bietet modernste Leistung bei Codierung, Inhaltserstellung, Datenanalyse und Planungsaufgaben und baut auf den Software-Engineering- und Computerf\xe4higkeiten seines Vorg\xe4ngers Claude 3.5 Sonnet auf."},"anthropic/claude-opus-4":{"description":"Claude Opus 4 ist das leistungsst\xe4rkste Modell von Anthropic und das weltweit beste Codierungsmodell mit Spitzenwerten bei SWE-bench (72,5 %) und Terminal-bench (43,2 %). Es bietet anhaltende Leistung f\xfcr langfristige Aufgaben mit tausenden Schritten und kann stundenlang ununterbrochen arbeiten – was die F\xe4higkeiten von KI-Agenten erheblich erweitert."},"anthropic/claude-opus-4.1":{"description":"Claude Opus 4.1 ist ein Plug-and-Play-Ersatz f\xfcr Opus 4 und bietet herausragende Leistung und Pr\xe4zision f\xfcr praktische Codierungs- und Agentenaufgaben. Opus 4.1 hebt die modernste Codierungsleistung auf 74,5 % bei SWE-bench Verified und behandelt komplexe mehrstufige Probleme mit h\xf6herer Genauigkeit und Detailgenauigkeit."},"anthropic/claude-sonnet-4":{"description":"Claude Sonnet 4 baut auf den branchenf\xfchrenden F\xe4higkeiten von Sonnet 3.7 auf und zeigt herausragende Codierungsleistung mit einem Spitzenwert von 72,7 % bei SWE-bench. Das Modell bietet eine ausgewogene Kombination aus Leistung und Effizienz, geeignet f\xfcr interne und externe Anwendungsf\xe4lle, und erm\xf6glicht durch verbesserte Steuerbarkeit eine gr\xf6\xdfere Kontrolle \xfcber die Ergebnisse."},"anthropic/claude-sonnet-4.5":{"description":"Claude Sonnet 4.5 ist das bisher intelligenteste Modell von Anthropic."},"ascend-tribe/pangu-pro-moe":{"description":"Pangu-Pro-MoE 72B-A16B ist ein sp\xe4rlich besetztes gro\xdfes Sprachmodell mit 72 Milliarden Parametern und 16 Milliarden aktivierten Parametern. Es basiert auf der gruppierten Mixture-of-Experts-Architektur (MoGE), bei der Experten in Gruppen eingeteilt werden und Tokens innerhalb jeder Gruppe eine gleiche Anzahl von Experten aktivieren, um eine ausgewogene Expertenauslastung zu gew\xe4hrleisten. Dies verbessert die Effizienz der Modellausf\xfchrung auf der Ascend-Plattform erheblich."},"aya":{"description":"Aya 23 ist ein mehrsprachiges Modell von Cohere, das 23 Sprachen unterst\xfctzt und die Anwendung in einer Vielzahl von Sprachen erleichtert."},"aya:35b":{"description":"Aya 23 ist ein mehrsprachiges Modell von Cohere, das 23 Sprachen unterst\xfctzt und die Anwendung in einer Vielzahl von Sprachen erleichtert."},"azure-DeepSeek-R1-0528":{"description":"Bereitgestellt von Microsoft; Das DeepSeek R1 Modell wurde in einer kleinen Versionsaktualisierung verbessert, die aktuelle Version ist DeepSeek-R1-0528. Im neuesten Update wurde die Rechentiefe und Inferenzf\xe4higkeit von DeepSeek R1 durch Erh\xf6hung der Rechenressourcen und Einf\xfchrung eines Algorithmus-Optimierungsmechanismus in der Nachtrainingsphase erheblich gesteigert. Dieses Modell zeigt hervorragende Leistungen in mehreren Benchmark-Tests wie Mathematik, Programmierung und allgemeiner Logik und n\xe4hert sich in der Gesamtleistung f\xfchrenden Modellen wie O3 und Gemini 2.5 Pro an."},"baichuan-m2-32b":{"description":"Baichuan M2 32B ist ein hybrides Expertenmodell von Baichuan Intelligence mit leistungsstarken F\xe4higkeiten im logischen Schlussfolgern."},"baichuan/baichuan2-13b-chat":{"description":"Baichuan-13B ist ein Open-Source-Sprachmodell mit 13 Milliarden Parametern, das von Baichuan Intelligence entwickelt wurde und in autorisierten chinesischen und englischen Benchmarks die besten Ergebnisse in seiner Gr\xf6\xdfenordnung erzielt hat."},"baidu/ERNIE-4.5-300B-A47B":{"description":"ERNIE-4.5-300B-A47B ist ein von Baidu entwickeltes gro\xdfes Sprachmodell, das auf einer Mixture-of-Experts (MoE)-Architektur basiert. Das Modell verf\xfcgt \xfcber insgesamt 300 Milliarden Parameter, aktiviert jedoch bei der Inferenz nur 47 Milliarden Parameter pro Token, was eine starke Leistung bei gleichzeitig hoher Rechen-effizienz gew\xe4hrleistet. Als eines der Kernmodelle der ERNIE 4.5-Serie zeigt es herausragende F\xe4higkeiten in Textverst\xe4ndnis, -generierung, Schlussfolgerung und Programmierung. Das Modell verwendet eine innovative multimodale heterogene MoE-Vortrainingsmethode, die durch gemeinsames Training von Text- und visuellen Modalit\xe4ten die Gesamtleistung verbessert, insbesondere bei der Befolgung von Anweisungen und dem Erinnern von Weltwissen."},"c4ai-aya-expanse-32b":{"description":"Aya Expanse ist ein leistungsstarkes 32B mehrsprachiges Modell, das darauf abzielt, die Leistung von einsprachigen Modellen durch innovative Ans\xe4tze wie Anweisungsoptimierung, Datenarbitrage, Pr\xe4ferenztraining und Modellfusion herauszufordern. Es unterst\xfctzt 23 Sprachen."},"c4ai-aya-expanse-8b":{"description":"Aya Expanse ist ein leistungsstarkes 8B mehrsprachiges Modell, das darauf abzielt, die Leistung von einsprachigen Modellen durch innovative Ans\xe4tze wie Anweisungsoptimierung, Datenarbitrage, Pr\xe4ferenztraining und Modellfusion herauszufordern. Es unterst\xfctzt 23 Sprachen."},"c4ai-aya-vision-32b":{"description":"Aya Vision ist ein hochmodernes multimodales Modell, das in mehreren wichtigen Benchmarks f\xfcr Sprache, Text und Bild hervorragende Leistungen zeigt. Diese 32B-Version konzentriert sich auf die fortschrittlichste mehrsprachige Leistung und unterst\xfctzt 23 Sprachen."},"c4ai-aya-vision-8b":{"description":"Aya Vision ist ein hochmodernes multimodales Modell, das in mehreren wichtigen Benchmarks f\xfcr Sprache, Text und Bild hervorragende Leistungen zeigt. Diese 8B-Version konzentriert sich auf niedrige Latenz und optimale Leistung."},"charglm-3":{"description":"CharGLM-3 ist f\xfcr Rollenspiele und emotionale Begleitung konzipiert und unterst\xfctzt extrem lange Mehrfachged\xe4chtnisse und personalisierte Dialoge, mit breiter Anwendung."},"charglm-4":{"description":"CharGLM-4 wurde speziell f\xfcr Rollenspiele und emotionale Begleitung entwickelt, unterst\xfctzt extrem lange Mehrfachged\xe4chtnisse und personalisierte Dialoge und findet breite Anwendung."},"chatgpt-4o-latest":{"description":"ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gew\xe4hrleisten. Es kombiniert starke Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und eignet sich f\xfcr gro\xdfangelegte Anwendungsszenarien, einschlie\xdflich Kundenservice, Bildung und technische Unterst\xfctzung."},"claude-2.0":{"description":"Claude 2 bietet Unternehmen Fortschritte in kritischen F\xe4higkeiten, einschlie\xdflich branchenf\xfchrenden 200K Token Kontext, erheblich reduzierter H\xe4ufigkeit von Modellillusionen, Systemaufforderungen und einer neuen Testfunktion: Werkzeugaufrufe."},"claude-2.1":{"description":"Claude 2 bietet Unternehmen Fortschritte in kritischen F\xe4higkeiten, einschlie\xdflich branchenf\xfchrenden 200K Token Kontext, erheblich reduzierter H\xe4ufigkeit von Modellillusionen, Systemaufforderungen und einer neuen Testfunktion: Werkzeugaufrufe."},"claude-3-5-haiku-20241022":{"description":"Claude 3.5 Haiku ist das schnellste n\xe4chste Modell von Anthropic. Im Vergleich zu Claude 3 Haiku hat Claude 3.5 Haiku in allen F\xe4higkeiten Verbesserungen erzielt und \xfcbertrifft das vorherige gr\xf6\xdfte Modell, Claude 3 Opus, in vielen intellektuellen Benchmark-Tests."},"claude-3-5-haiku-latest":{"description":"Claude 3.5 Haiku bietet schnelle Reaktionen und eignet sich f\xfcr leichte Aufgaben."},"claude-3-7-sonnet-20250219":{"description":"Claude 3.7 Sonnet hebt den Branchenstandard an, \xfcbertrifft die Modelle der Konkurrenz und Claude 3 Opus, und zeigt in umfassenden Bewertungen hervorragende Leistungen, w\xe4hrend es die Geschwindigkeit und Kosten unserer mittelgro\xdfen Modelle beibeh\xe4lt."},"claude-3-7-sonnet-latest":{"description":"Claude 3.7 Sonnet ist das neueste und leistungsst\xe4rkste Modell von Anthropic f\xfcr hochkomplexe Aufgaben. Es \xfcberzeugt durch herausragende Leistung, Intelligenz, Fl\xfcssigkeit und Verst\xe4ndnis."},"claude-3-haiku-20240307":{"description":"Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic, das darauf abzielt, nahezu sofortige Antworten zu liefern. Es bietet schnelle und pr\xe4zise zielgerichtete Leistungen."},"claude-3-opus-20240229":{"description":"Claude 3 Opus ist das leistungsst\xe4rkste Modell von Anthropic f\xfcr die Verarbeitung hochkomplexer Aufgaben. Es bietet herausragende Leistungen in Bezug auf Leistung, Intelligenz, Fl\xfcssigkeit und Verst\xe4ndnis."},"claude-3-sonnet-20240229":{"description":"Claude 3 Sonnet bietet eine ideale Balance zwischen Intelligenz und Geschwindigkeit f\xfcr Unternehmensarbeitslasten. Es bietet maximalen Nutzen zu einem niedrigeren Preis, ist zuverl\xe4ssig und f\xfcr gro\xdffl\xe4chige Bereitstellungen geeignet."},"claude-haiku-4-5-20251001":{"description":"Claude Haiku 4.5 ist das schnellste und intelligenteste Haiku-Modell von Anthropic, mit blitzschneller Geschwindigkeit und erweiterter Denkf\xe4higkeit."},"claude-opus-4-1-20250805":{"description":"Claude Opus 4.1 ist das neueste und leistungsst\xe4rkste Modell von Anthropic zur Bew\xe4ltigung hochkomplexer Aufgaben. Es \xfcberzeugt durch herausragende Leistung, Intelligenz, Fl\xfcssigkeit und Verst\xe4ndnis."},"claude-opus-4-1-20250805-thinking":{"description":"Claude Opus 4.1 Denkmodell, eine fortgeschrittene Version, die ihren Denkprozess offenlegt."},"claude-opus-4-20250514":{"description":"Claude Opus 4 ist das leistungsst\xe4rkste Modell von Anthropic zur Bew\xe4ltigung hochkomplexer Aufgaben. Es zeichnet sich durch hervorragende Leistung, Intelligenz, Fl\xfcssigkeit und Verst\xe4ndnis aus."},"claude-sonnet-4-20250514":{"description":"Claude Sonnet 4 kann nahezu sofortige Antworten oder verl\xe4ngerte schrittweise \xdcberlegungen erzeugen, die f\xfcr den Nutzer klar nachvollziehbar sind."},"claude-sonnet-4-20250514-thinking":{"description":"Claude Sonnet 4 Denkmodell kann nahezu sofortige Antworten oder verl\xe4ngerte schrittweise \xdcberlegungen erzeugen, die f\xfcr den Nutzer klar nachvollziehbar sind."},"claude-sonnet-4-5-20250929":{"description":"Claude Sonnet 4.5 ist das bisher intelligenteste Modell von Anthropic."},"codegeex-4":{"description":"CodeGeeX-4 ist ein leistungsstarker AI-Programmierassistent, der intelligente Fragen und Codevervollst\xe4ndigung in verschiedenen Programmiersprachen unterst\xfctzt und die Entwicklungseffizienz steigert."},"codegeex4-all-9b":{"description":"CodeGeeX4-ALL-9B ist ein mehrsprachiges Code-Generierungsmodell, das umfassende Funktionen unterst\xfctzt, darunter Code-Vervollst\xe4ndigung und -Generierung, Code-Interpreter, Websuche, Funktionsaufrufe und repository-weite Codefragen und -antworten, und deckt verschiedene Szenarien der Softwareentwicklung ab. Es ist das f\xfchrende Code-Generierungsmodell mit weniger als 10B Parametern."},"codegemma":{"description":"CodeGemma ist ein leichtgewichtiges Sprachmodell, das speziell f\xfcr verschiedene Programmieraufgaben entwickelt wurde und schnelle Iterationen und Integrationen unterst\xfctzt."},"codegemma:2b":{"description":"CodeGemma ist ein leichtgewichtiges Sprachmodell, das speziell f\xfcr verschiedene Programmieraufgaben entwickelt wurde und schnelle Iterationen und Integrationen unterst\xfctzt."},"codellama":{"description":"Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterst\xfctzung f\xfcr Programmiersprachen bietet, die sich f\xfcr Entwicklerumgebungen eignet."},"codellama/CodeLlama-34b-Instruct-hf":{"description":"Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterst\xfctzung f\xfcr Programmiersprachen bietet, die f\xfcr Entwicklerumgebungen geeignet ist."},"codellama:13b":{"description":"Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterst\xfctzung f\xfcr Programmiersprachen bietet, die sich f\xfcr Entwicklerumgebungen eignet."},"codellama:34b":{"description":"Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterst\xfctzung f\xfcr Programmiersprachen bietet, die sich f\xfcr Entwicklerumgebungen eignet."},"codellama:70b":{"description":"Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterst\xfctzung f\xfcr Programmiersprachen bietet, die sich f\xfcr Entwicklerumgebungen eignet."},"codeqwen":{"description":"CodeQwen1.5 ist ein gro\xdfes Sprachmodell, das auf einer umfangreichen Code-Datenbasis trainiert wurde und speziell f\xfcr die L\xf6sung komplexer Programmieraufgaben entwickelt wurde."},"codestral":{"description":"Codestral ist das erste Code-Modell von Mistral AI und bietet hervorragende Unterst\xfctzung f\xfcr Aufgaben der Codegenerierung."},"codestral-latest":{"description":"Codestral ist ein hochmodernes Generierungsmodell, das sich auf die Codegenerierung konzentriert und f\xfcr Aufgaben wie das Ausf\xfcllen von Zwischenr\xe4umen und die Codevervollst\xe4ndigung optimiert wurde."},"codex-mini-latest":{"description":"codex-mini-latest ist eine feinabgestimmte Version von o4-mini, speziell f\xfcr Codex CLI entwickelt. F\xfcr die direkte Nutzung \xfcber die API empfehlen wir den Start mit gpt-4.1."},"cogview-4":{"description":"CogView-4 ist das erste von Zhipu entwickelte Open-Source-Text-zu-Bild-Modell, das die Generierung chinesischer Schriftzeichen unterst\xfctzt. Es bietet umfassende Verbesserungen in den Bereichen semantisches Verst\xe4ndnis, Bildgenerierungsqualit\xe4t und die F\xe4higkeit, chinesische und englische Schriftzeichen zu erzeugen. Es unterst\xfctzt mehrsprachige Eingaben beliebiger L\xe4nge in Chinesisch und Englisch und kann Bilder in beliebiger Aufl\xf6sung innerhalb eines vorgegebenen Bereichs erzeugen."},"cohere-command-r":{"description":"Command R ist ein skalierbares generatives Modell, das auf RAG und Tool-Nutzung abzielt, um KI in Produktionsgr\xf6\xdfe f\xfcr Unternehmen zu erm\xf6glichen."},"cohere-command-r-plus":{"description":"Command R+ ist ein hochmodernes, RAG-optimiertes Modell, das f\xfcr unternehmensgerechte Arbeitslasten konzipiert ist."},"cohere/Cohere-command-r":{"description":"Command R ist ein skalierbares Generierungsmodell, das f\xfcr RAG und Tool-Nutzung entwickelt wurde, um Unternehmen produktionsreife KI zu erm\xf6glichen."},"cohere/Cohere-command-r-plus":{"description":"Command R+ ist ein hochmodernes, f\xfcr RAG optimiertes Modell, das f\xfcr unternehmensweite Arbeitslasten ausgelegt ist."},"cohere/command-a":{"description":"Command A ist das leistungsst\xe4rkste Modell von Cohere mit hervorragender Leistung bei Werkzeugnutzung, Agenten, Retrieval-unterst\xfctzter Generierung (RAG) und mehrsprachigen Anwendungsf\xe4llen. Command A unterst\xfctzt eine Kontextl\xe4nge von 256K und l\xe4uft auf nur zwei GPUs, mit einer 150 % h\xf6heren Durchsatzrate im Vergleich zu Command R+ 08-2024."},"cohere/command-r":{"description":"Command R ist ein gro\xdfes Sprachmodell, optimiert f\xfcr dialogbasierte Interaktionen und Aufgaben mit langem Kontext. Es geh\xf6rt zur Kategorie der \\"skalierbaren\\" Modelle und bietet eine Balance zwischen hoher Leistung und starker Genauigkeit, sodass Unternehmen \xfcber Proof-of-Concept hinaus in die Produktion gehen k\xf6nnen."},"cohere/command-r-plus":{"description":"Command R+ ist das neueste gro\xdfe Sprachmodell von Cohere, optimiert f\xfcr dialogbasierte Interaktionen und Aufgaben mit langem Kontext. Es zielt darauf ab, au\xdfergew\xf6hnliche Leistung zu bieten, damit Unternehmen \xfcber Proof-of-Concept hinaus in die Produktion gehen k\xf6nnen."},"cohere/embed-v4.0":{"description":"Ein Modell, das es erm\xf6glicht, Text, Bilder oder gemischte Inhalte zu klassifizieren oder in Einbettungen umzuwandeln."},"comfyui/flux-dev":{"description":"FLUX.1 Dev – Hochwertiges Text-zu-Bild-Modell, erzeugt Bilder in 10–50 Schritten, ideal f\xfcr kreative Arbeiten und k\xfcnstlerische Bildgenerierung"},"comfyui/flux-kontext-dev":{"description":"FLUX.1 Kontext-dev – Bildbearbeitungsmodell, unterst\xfctzt textbasierte Anweisungen zur Modifikation bestehender Bilder, inklusive lokaler \xc4nderungen und Stil\xfcbertragungen"},"comfyui/flux-krea-dev":{"description":"FLUX.1 Krea-dev – Sicheres Text-zu-Bild-Modell, entwickelt in Zusammenarbeit mit Krea, mit integriertem Sicherheitsfilter"},"comfyui/flux-schnell":{"description":"FLUX.1 Schnell – Ultraflottes Text-zu-Bild-Modell, erzeugt hochwertige Bilder in nur 1–4 Schritten, ideal f\xfcr Echtzeitanwendungen und schnelles Prototyping"},"comfyui/stable-diffusion-15":{"description":"Stable Diffusion 1.5 Text-zu-Bild-Modell, klassisches 512x512-Aufl\xf6sungsmodell f\xfcr schnelle Prototypen und kreative Experimente"},"comfyui/stable-diffusion-35":{"description":"Stable Diffusion 3.5 – N\xe4chste Generation des Text-zu-Bild-Modells, verf\xfcgbar in Large- und Medium-Versionen, ben\xf6tigt externe CLIP-Encoder-Dateien, bietet herausragende Bildqualit\xe4t und pr\xe4zise Prompt-\xdcbereinstimmung"},"comfyui/stable-diffusion-35-inclclip":{"description":"Stable Diffusion 3.5 mit integriertem CLIP/T5-Encoder – keine externen Encoder-Dateien erforderlich, geeignet f\xfcr Modelle wie sd3.5_medium_incl_clips, ressourcenschonender"},"comfyui/stable-diffusion-custom":{"description":"Benutzerdefiniertes SD Text-zu-Bild-Modell – Modell-Dateiname sollte custom_sd_lobe.safetensors lauten, bei VAE bitte custom_sd_vae_lobe.safetensors verwenden; Modell-Dateien m\xfcssen gem\xe4\xdf Comfy-Vorgaben im entsprechenden Ordner abgelegt werden"},"comfyui/stable-diffusion-custom-refiner":{"description":"Benutzerdefiniertes SDXL Bild-zu-Bild-Modell – Modell-Dateiname sollte custom_sd_lobe.safetensors lauten, bei VAE bitte custom_sd_vae_lobe.safetensors verwenden; Modell-Dateien m\xfcssen gem\xe4\xdf Comfy-Vorgaben im entsprechenden Ordner abgelegt werden"},"comfyui/stable-diffusion-refiner":{"description":"SDXL Bild-zu-Bild-Modell – Hochwertige Bildtransformation basierend auf Eingabebildern, unterst\xfctzt Stil\xfcbertragungen, Bildrestaurierung und kreative Modifikationen"},"comfyui/stable-diffusion-xl":{"description":"SDXL Text-zu-Bild-Modell – Unterst\xfctzt hochaufl\xf6sende 1024x1024 Text-zu-Bild-Generierung, bietet verbesserte Bildqualit\xe4t und Detailgenauigkeit"},"command":{"description":"Ein dialogbasiertes Modell, das Anweisungen folgt und in sprachlichen Aufgaben hohe Qualit\xe4t und Zuverl\xe4ssigkeit bietet. Im Vergleich zu unserem grundlegenden Generierungsmodell hat es eine l\xe4ngere Kontextl\xe4nge."},"command-a-03-2025":{"description":"Command A ist unser bisher leistungsst\xe4rkstes Modell, das in der Nutzung von Werkzeugen, Agenten, Retrieval-Enhanced Generation (RAG) und mehrsprachigen Anwendungsszenarien hervorragende Leistungen zeigt. Command A hat eine Kontextl\xe4nge von 256K, ben\xf6tigt nur zwei GPUs zum Betrieb und bietet im Vergleich zu Command R+ 08-2024 eine Steigerung der Durchsatzrate um 150 %."},"command-light":{"description":"Eine kleinere, schnellere Version von Command, die fast ebenso leistungsstark ist, aber schneller arbeitet."},"command-light-nightly":{"description":"Um die Zeitspanne zwischen den Hauptversionsver\xf6ffentlichungen zu verk\xfcrzen, haben wir eine n\xe4chtliche Version des Command Modells eingef\xfchrt. F\xfcr die command-light-Serie wird diese Version als command-light-nightly bezeichnet. Bitte beachten Sie, dass command-light-nightly die neueste, experimentellste und (m\xf6glicherweise) instabilste Version ist. Die n\xe4chtlichen Versionen werden regelm\xe4\xdfig aktualisiert, ohne vorherige Ank\xfcndigung, daher wird die Verwendung in Produktionsumgebungen nicht empfohlen."},"command-nightly":{"description":"Um die Zeitspanne zwischen den Hauptversionsver\xf6ffentlichungen zu verk\xfcrzen, haben wir eine n\xe4chtliche Version des Command Modells eingef\xfchrt. F\xfcr die Command-Serie wird diese Version als command-cightly bezeichnet. Bitte beachten Sie, dass command-nightly die neueste, experimentellste und (m\xf6glicherweise) instabilste Version ist. Die n\xe4chtlichen Versionen werden regelm\xe4\xdfig aktualisiert, ohne vorherige Ank\xfcndigung, daher wird die Verwendung in Produktionsumgebungen nicht empfohlen."},"command-r":{"description":"Command R ist ein LLM, das f\xfcr Dialoge und Aufgaben mit langen Kontexten optimiert ist und sich besonders gut f\xfcr dynamische Interaktionen und Wissensmanagement eignet."},"command-r-03-2024":{"description":"Command R ist ein dialogbasiertes Modell, das Anweisungen folgt und in sprachlichen Aufgaben eine h\xf6here Qualit\xe4t und Zuverl\xe4ssigkeit bietet. Im Vergleich zu fr\xfcheren Modellen hat es eine l\xe4ngere Kontextl\xe4nge. Es kann f\xfcr komplexe Workflows wie Codegenerierung, Retrieval-Enhanced Generation (RAG), Werkzeugnutzung und Agenten verwendet werden."},"command-r-08-2024":{"description":"command-r-08-2024 ist die aktualisierte Version des Command R Modells, das im August 2024 ver\xf6ffentlicht wurde."},"command-r-plus":{"description":"Command R+ ist ein leistungsstarkes gro\xdfes Sprachmodell, das speziell f\xfcr reale Unternehmensszenarien und komplexe Anwendungen entwickelt wurde."},"command-r-plus-04-2024":{"description":"Command R+ ist ein dialogbasiertes Modell, das Anweisungen folgt und in sprachlichen Aufgaben eine h\xf6here Qualit\xe4t und Zuverl\xe4ssigkeit bietet. Im Vergleich zu fr\xfcheren Modellen hat es eine l\xe4ngere Kontextl\xe4nge. Es eignet sich am besten f\xfcr komplexe RAG-Workflows und mehrstufige Werkzeugnutzung."},"command-r-plus-08-2024":{"description":"Command R+ ist ein dialogbasiertes Modell, das Anweisungen befolgt und in sprachlichen Aufgaben eine h\xf6here Qualit\xe4t und Zuverl\xe4ssigkeit bietet, mit einer l\xe4ngeren Kontextl\xe4nge im Vergleich zu fr\xfcheren Modellen. Es eignet sich am besten f\xfcr komplexe RAG-Workflows und die Nutzung mehrerer Werkzeuge."},"command-r7b-12-2024":{"description":"command-r7b-12-2024 ist eine kompakte und effiziente aktualisierte Version, die im Dezember 2024 ver\xf6ffentlicht wurde. Es zeigt hervorragende Leistungen in Aufgaben, die komplexes Denken und mehrstufige Verarbeitung erfordern, wie RAG, Werkzeugnutzung und Agenten."},"computer-use-preview":{"description":"Das Modell computer-use-preview ist ein speziell f\xfcr „Computeranwendungstools“ entwickeltes Modell, das darauf trainiert wurde, computerbezogene Aufgaben zu verstehen und auszuf\xfchren."},"dall-e-2":{"description":"Zweite Generation des DALL\xb7E-Modells, unterst\xfctzt realistischere und genauere Bildgenerierung, mit einer Aufl\xf6sung, die viermal so hoch ist wie die der ersten Generation."},"dall-e-3":{"description":"Das neueste DALL\xb7E-Modell, ver\xf6ffentlicht im November 2023. Unterst\xfctzt realistischere und genauere Bildgenerierung mit verbesserter Detailgenauigkeit."},"databricks/dbrx-instruct":{"description":"DBRX Instruct bietet zuverl\xe4ssige Anweisungsverarbeitungsf\xe4higkeiten und unterst\xfctzt Anwendungen in verschiedenen Branchen."},"deepseek-ai/DeepSeek-OCR":{"description":"DeepSeek-OCR ist ein visuelles Sprachmodell von DeepSeek AI, das sich auf optische Zeichenerkennung (OCR) und \\"kontextuelle optische Kompression\\" spezialisiert hat. Das Modell zielt darauf ab, die Grenzen der Kontextkompression aus Bildern auszuloten und kann Dokumente effizient verarbeiten und in strukturierte Textformate wie Markdown umwandeln. Es erkennt Textinhalte in Bildern pr\xe4zise und eignet sich besonders f\xfcr Anwendungen wie Dokumentendigitalisierung, Textextraktion und strukturierte Verarbeitung."},"deepseek-ai/DeepSeek-R1":{"description":"DeepSeek-R1 ist ein durch verst\xe4rkendes Lernen (RL) gesteuertes Inferenzmodell, das die Probleme der Wiederholbarkeit und Lesbarkeit im Modell l\xf6st. Vor dem RL f\xfchrte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmierbezogenen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert durch sorgf\xe4ltig gestaltete Trainingsmethoden die Gesamteffizienz."},"deepseek-ai/DeepSeek-R1-0528":{"description":"DeepSeek R1 verbessert durch den Einsatz erh\xf6hter Rechenressourcen und die Einf\xfchrung algorithmischer Optimierungsmechanismen im Nachtraining signifikant die Tiefe seiner Schlussfolgerungs- und Deduktionsf\xe4higkeiten. Das Modell zeigt hervorragende Leistungen in verschiedenen Benchmark-Tests, einschlie\xdflich Mathematik, Programmierung und allgemeiner Logik. Die Gesamtleistung n\xe4hert sich f\xfchrenden Modellen wie O3 und Gemini 2.5 Pro an."},"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B":{"description":"DeepSeek-R1-0528-Qwen3-8B ist ein Modell, das durch Destillation der Denkprozesskette vom DeepSeek-R1-0528-Modell auf das Qwen3 8B Base Modell gewonnen wurde. Es erreicht in Open-Source-Modellen den Stand der Technik (SOTA), \xfcbertrifft im AIME 2024 Test Qwen3 8B um 10 % und erreicht die Leistungsstufe von Qwen3-235B-thinking. Das Modell zeigt hervorragende Leistungen in Mathematik, Programmierung und allgemeiner Logik in mehreren Benchmarks. Die Architektur entspricht Qwen3-8B, teilt jedoch die Tokenizer-Konfiguration von DeepSeek-R1-0528."},"deepseek-ai/DeepSeek-R1-Distill-Llama-70B":{"description":"Das DeepSeek-R1-Distill-Modell optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B":{"description":"Das DeepSeek-R1-Distill-Modell optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B":{"description":"Das DeepSeek-R1-Distill-Modell optimiert die Inferenzleistung durch verst\xe4rkendes Lernen und Kaltstartdaten. Das Open-Source-Modell setzt neue Ma\xdfst\xe4be f\xfcr Multitasking."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":{"description":"DeepSeek-R1-Distill-Qwen-32B ist ein Modell, das durch Wissensdestillation aus Qwen2.5-32B gewonnen wurde. Dieses Modell wurde mit 800.000 ausgew\xe4hlten Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt herausragende Leistungen in mehreren Bereichen wie Mathematik, Programmierung und Inferenz. Es hat in mehreren Benchmark-Tests, darunter AIME 2024, MATH-500 und GPQA Diamond, hervorragende Ergebnisse erzielt, wobei es in MATH-500 eine Genauigkeit von 94,3 % erreicht hat und damit starke mathematische Schlussfolgerungsf\xe4higkeiten demonstriert."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B":{"description":"DeepSeek-R1-Distill-Qwen-7B ist ein Modell, das durch Wissensdestillation aus Qwen2.5-Math-7B gewonnen wurde. Dieses Modell wurde mit 800.000 ausgew\xe4hlten Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt hervorragende Inferenzf\xe4higkeiten. Es hat in mehreren Benchmark-Tests, darunter eine Genauigkeit von 92,8 % in MATH-500, eine Bestehensquote von 55,5 % in AIME 2024 und eine Bewertung von 1189 in CodeForces, was starke mathematische und Programmierf\xe4higkeiten f\xfcr ein 7B-Modell demonstriert."},"deepseek-ai/DeepSeek-V2.5":{"description":"DeepSeek V2.5 vereint die hervorragenden Merkmale fr\xfcherer Versionen und verbessert die allgemeinen und kodierenden F\xe4higkeiten."},"deepseek-ai/DeepSeek-V3":{"description":"DeepSeek-V3 ist ein hybrides Expertenmodell (MoE) mit 6710 Milliarden Parametern, das eine Multi-Head-Latent-Attention (MLA) und die DeepSeekMoE-Architektur verwendet, kombiniert mit einer Lastenausgleichsstrategie ohne Hilfskosten, um die Inferenz- und Trainingseffizienz zu optimieren. Durch das Pre-Training auf 14,8 Billionen hochwertigen Tokens und anschlie\xdfendes \xfcberwachten Feintuning und verst\xe4rkendes Lernen \xfcbertrifft DeepSeek-V3 in der Leistung andere Open-Source-Modelle und n\xe4hert sich f\xfchrenden Closed-Source-Modellen."},"deepseek-ai/DeepSeek-V3.1":{"description":"Das DeepSeek V3.1 Modell basiert auf einer hybriden Inferenzarchitektur und unterst\xfctzt sowohl Denk- als auch Nicht-Denk-Modi."},"deepseek-ai/DeepSeek-V3.1-Terminus":{"description":"DeepSeek-V3.1-Terminus ist eine aktualisierte Version des V3.1-Modells von DeepSeek, positioniert als hybrides Agenten-Gro\xdfsprachmodell. Dieses Update konzentriert sich darauf, auf Nutzerfeedback basierende Probleme zu beheben und die Stabilit\xe4t zu verbessern, w\xe4hrend die urspr\xfcnglichen Modellf\xe4higkeiten erhalten bleiben. Es verbessert deutlich die Sprachkonsistenz und reduziert das Vermischen von Chinesisch und Englisch sowie das Auftreten ungew\xf6hnlicher Zeichen. Das Modell integriert den „Denkmodus“ (Thinking Mode) und den „Nicht-Denkmodus“ (Non-thinking Mode), zwischen denen Nutzer flexibel \xfcber Chatvorlagen wechseln k\xf6nnen, um unterschiedlichen Aufgaben gerecht zu werden. Als wichtige Optimierung verbessert V3.1-Terminus die Leistung des Code-Agenten und des Such-Agenten, wodurch diese bei Werkzeugaufrufen und der Ausf\xfchrung mehrstufiger komplexer Aufgaben zuverl\xe4ssiger sind."},"deepseek-ai/DeepSeek-V3.2-Exp":{"description":"DeepSeek-V3.2-Exp ist eine experimentelle Version 3.2 von DeepSeek und stellt einen Zwischenschritt auf dem Weg zur n\xe4chsten Generation der Architektur dar. Aufbauend auf V3.1-Terminus f\xfchrt sie den DeepSeek Sparse Attention (DSA)-Mechanismus ein, um die Effizienz beim Training und bei der Inferenz mit langen Kontexten zu verbessern. Sie wurde speziell f\xfcr Werkzeugaufrufe, das Verst\xe4ndnis langer Dokumente und mehrstufiges Schlussfolgern optimiert. V3.2-Exp dient als Br\xfccke zwischen Forschung und Produktreife und eignet sich f\xfcr Nutzer, die in Szenarien mit hohem Kontextbudget eine h\xf6here Inferenzleistung erkunden m\xf6chten."},"deepseek-ai/deepseek-llm-67b-chat":{"description":"DeepSeek 67B ist ein fortschrittliches Modell, das f\xfcr komplexe Dialoge trainiert wurde."},"deepseek-ai/deepseek-r1":{"description":"Hochmodernes, effizientes LLM, das auf Schlussfolgern, Mathematik und Programmierung spezialisiert ist."},"deepseek-ai/deepseek-v3.1":{"description":"DeepSeek V3.1: Ein Inferenzmodell der n\xe4chsten Generation, das komplexe Schlussfolgerungen und verkn\xfcpfte Denkf\xe4higkeiten verbessert und sich f\xfcr Aufgaben eignet, die tiefgehende Analysen erfordern."},"deepseek-ai/deepseek-v3.1-terminus":{"description":"DeepSeek V3.1: Das n\xe4chste Generation von Inferenzmodellen mit verbesserter F\xe4higkeit zum komplexen Schlussfolgern und vernetztem Denken – ideal f\xfcr Aufgaben, die tiefgehende Analysen erfordern."},"deepseek-ai/deepseek-vl2":{"description":"DeepSeek-VL2 ist ein hybrides Expertenmodell (MoE) f\xfcr visuelle Sprache, das auf DeepSeekMoE-27B basiert und eine sp\xe4rliche Aktivierung der MoE-Architektur verwendet, um au\xdfergew\xf6hnliche Leistungen bei der Aktivierung von nur 4,5 Milliarden Parametern zu erzielen. Dieses Modell zeigt hervorragende Leistungen in mehreren Aufgaben, darunter visuelle Fragenbeantwortung, optische Zeichenerkennung, Dokument-/Tabellen-/Diagrammverst\xe4ndnis und visuelle Lokalisierung."},"deepseek-chat":{"description":"Ein neues Open-Source-Modell, das allgemeine und Codef\xe4higkeiten kombiniert. Es bewahrt nicht nur die allgemeinen Dialogf\xe4higkeiten des urspr\xfcnglichen Chat-Modells und die leistungsstarken Codeverarbeitungsf\xe4higkeiten des Coder-Modells, sondern stimmt auch besser mit menschlichen Pr\xe4ferenzen \xfcberein. Dar\xfcber hinaus hat DeepSeek-V2.5 in mehreren Bereichen wie Schreibaufgaben und Befolgung von Anweisungen erhebliche Verbesserungen erzielt."},"deepseek-coder-33B-instruct":{"description":"DeepSeek Coder 33B ist ein Code-Sprachmodell, das auf 20 Billionen Daten trainiert wurde, von denen 87 % Code und 13 % in Chinesisch und Englisch sind. Das Modell f\xfchrt eine Fenstergr\xf6\xdfe von 16K und Aufgaben zur L\xfcckenerg\xe4nzung ein und bietet projektbezogene Code-Vervollst\xe4ndigung und Fragmentf\xfcllfunktionen."},"deepseek-coder-v2":{"description":"DeepSeek Coder V2 ist ein Open-Source-Mischexperten-Code-Modell, das in Codeaufgaben hervorragende Leistungen erbringt und mit GPT4-Turbo vergleichbar ist."},"deepseek-coder-v2:236b":{"description":"DeepSeek Coder V2 ist ein Open-Source-Mischexperten-Code-Modell, das in Codeaufgaben hervorragende Leistungen erbringt und mit GPT4-Turbo vergleichbar ist."},"deepseek-r1":{"description":"DeepSeek-R1 ist ein durch verst\xe4rkendes Lernen (RL) gesteuertes Inferenzmodell, das die Probleme der Wiederholbarkeit und Lesbarkeit im Modell l\xf6st. Vor dem RL f\xfchrte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in mathematischen, programmierbezogenen und Inferenzaufgaben eine vergleichbare Leistung zu OpenAI-o1 und verbessert durch sorgf\xe4ltig gestaltete Trainingsmethoden die Gesamteffizienz."},"deepseek-r1-0528":{"description":"Das voll ausgestattete 685B-Modell, ver\xf6ffentlicht am 28. Mai 2025. DeepSeek-R1 nutzt im Nachtrainingsprozess umfangreiche Verst\xe4rkungslernverfahren und verbessert die Modell-Inferenzf\xe4higkeit erheblich, selbst bei minimalen annotierten Daten. Es zeigt hohe Leistung und starke F\xe4higkeiten in Mathematik, Programmierung und nat\xfcrlicher Sprachlogik."},"deepseek-r1-250528":{"description":"DeepSeek R1 250528, die Vollversion des DeepSeek-R1-Inferenzmodells, geeignet f\xfcr anspruchsvolle Mathematik- und Logikaufgaben."},"deepseek-r1-70b-fast-online":{"description":"DeepSeek R1 70B Schnellversion, die Echtzeit-Online-Suche unterst\xfctzt und eine schnellere Reaktionszeit bei gleichbleibender Modellleistung bietet."},"deepseek-r1-70b-online":{"description":"DeepSeek R1 70B Standardversion, die Echtzeit-Online-Suche unterst\xfctzt und sich f\xfcr Dialoge und Textverarbeitungsaufgaben eignet, die aktuelle Informationen ben\xf6tigen."},"deepseek-r1-distill-llama":{"description":"deepseek-r1-distill-llama ist ein Modell, das auf der Grundlage von Llama aus DeepSeek-R1 destilliert wurde."},"deepseek-r1-distill-llama-70b":{"description":"DeepSeek R1 Distill Llama 70B, ein distilliertes Modell, das die allgemeine R1-Inferenzf\xe4higkeit mit dem Llama-\xd6kosystem kombiniert."},"deepseek-r1-distill-llama-8b":{"description":"DeepSeek-R1-Distill-Llama-8B ist ein distilliertes gro\xdfes Sprachmodell auf Basis von Llama-3.1-8B unter Verwendung der Ausgaben von DeepSeek R1."},"deepseek-r1-distill-qianfan-70b":{"description":"DeepSeek R1 Distill Qianfan 70B, ein kosteneffizientes R1-Distillationsmodell basierend auf Qianfan-70B."},"deepseek-r1-distill-qianfan-8b":{"description":"DeepSeek R1 Distill Qianfan 8B, ein R1-Distillationsmodell auf Basis von Qianfan-8B, geeignet f\xfcr mittelgro\xdfe und kleinere Anwendungen."},"deepseek-r1-distill-qianfan-llama-70b":{"description":"DeepSeek R1 Distill Qianfan Llama 70B, ein R1-Distillationsmodell basierend auf Llama-70B."},"deepseek-r1-distill-qwen":{"description":"deepseek-r1-distill-qwen ist ein Modell, das auf der Grundlage von Qwen durch Distillierung aus DeepSeek-R1 erstellt wurde."},"deepseek-r1-distill-qwen-1.5b":{"description":"DeepSeek R1 Distill Qwen 1.5B, ein ultraleichtes R1-Distillationsmodell, ideal f\xfcr Umgebungen mit sehr begrenzten Ressourcen."},"deepseek-r1-distill-qwen-14b":{"description":"DeepSeek R1 Distill Qwen 14B, ein mittelgro\xdfes R1-Distillationsmodell, geeignet f\xfcr den Einsatz in verschiedenen Szenarien."},"deepseek-r1-distill-qwen-32b":{"description":"DeepSeek R1 Distill Qwen 32B, ein R1-Distillationsmodell basierend auf Qwen-32B, das Leistung und Kosten ausbalanciert."},"deepseek-r1-distill-qwen-7b":{"description":"DeepSeek R1 Distill Qwen 7B, ein leichtgewichtiges R1-Distillationsmodell, geeignet f\xfcr Edge-Computing und unternehmensinterne Umgebungen."},"deepseek-r1-fast-online":{"description":"DeepSeek R1 Vollschnellversion, die Echtzeit-Online-Suche unterst\xfctzt und die leistungsstarken F\xe4higkeiten von 671B Parametern mit einer schnelleren Reaktionszeit kombiniert."},"deepseek-r1-online":{"description":"DeepSeek R1 Vollversion mit 671B Parametern, die Echtzeit-Online-Suche unterst\xfctzt und \xfcber verbesserte Verst\xe4ndnis- und Generierungsf\xe4higkeiten verf\xfcgt."},"deepseek-reasoner":{"description":"DeepSeek V3.2 Denkmodus. Bevor die endg\xfcltige Antwort ausgegeben wird, gibt das Modell zun\xe4chst eine Gedankenkette aus, um die Genauigkeit der finalen Antwort zu verbessern."},"deepseek-v2":{"description":"DeepSeek V2 ist ein effizientes Mixture-of-Experts-Sprachmodell, das f\xfcr wirtschaftliche Verarbeitungsanforderungen geeignet ist."},"deepseek-v2:236b":{"description":"DeepSeek V2 236B ist das Design-Code-Modell von DeepSeek und bietet starke F\xe4higkeiten zur Codegenerierung."},"deepseek-v3":{"description":"DeepSeek-V3 ist ein MoE-Modell, das von der Hangzhou DeepSeek Artificial Intelligence Technology Research Co., Ltd. entwickelt wurde. Es hat in mehreren Bewertungen herausragende Ergebnisse erzielt und belegt in den g\xe4ngigen Rankings den ersten Platz unter den Open-Source-Modellen. Im Vergleich zum V2.5-Modell hat sich die Generierungsgeschwindigkeit um das Dreifache erh\xf6ht, was den Nutzern ein schnelleres und fl\xfcssigeres Nutzungserlebnis bietet."},"deepseek-v3-0324":{"description":"DeepSeek-V3-0324 ist ein MoE-Modell mit 671 Milliarden Parametern, das in den Bereichen Programmierung und technische F\xe4higkeiten, Kontextverst\xe4ndnis und Verarbeitung langer Texte herausragende Vorteile bietet."},"deepseek-v3.1":{"description":"DeepSeek-V3.1 ist ein neu eingef\xfchrtes hybrides Inferenzmodell von DeepSeek, das zwei Inferenzmodi unterst\xfctzt: Denkmodus und Nicht-Denkmodus. Es ist effizienter im Denkprozess als DeepSeek-R1-0528. Durch Post-Training-Optimierung wurden die Nutzung von Agenten-Tools und die Leistung bei Agentenaufgaben erheblich verbessert. Unterst\xfctzt ein Kontextfenster von 128k und eine maximale Ausgabel\xe4nge von 64k Tokens."},"deepseek-v3.1-terminus":{"description":"DeepSeek-V3.1-Terminus ist eine optimierte Version des gro\xdfen Sprachmodells von DeepSeek, speziell f\xfcr Endger\xe4te entwickelt."},"deepseek-v3.1-think-250821":{"description":"DeepSeek V3.1 Think 250821, das Deep-Thinking-Modell der Terminus-Version, geeignet f\xfcr leistungsstarke Inferenzszenarien."},"deepseek-v3.1:671b":{"description":"DeepSeek V3.1: Ein Inferenzmodell der n\xe4chsten Generation, das komplexe Schlussfolgerungen und verkn\xfcpfte Denkf\xe4higkeiten verbessert und sich f\xfcr Aufgaben eignet, die tiefgehende Analysen erfordern."},"deepseek-v3.2-exp":{"description":"deepseek-v3.2-exp f\xfchrt einen sparsamen Aufmerksamkeitsmechanismus ein, um die Effizienz beim Training und der Inferenz bei der Verarbeitung langer Texte zu verbessern. Der Preis liegt unter dem von deepseek-v3.1."},"deepseek-v3.2-think":{"description":"DeepSeek V3.2 Think, die Vollversion des Deep-Thinking-Modells mit verbesserter F\xe4higkeit zur Langketteninferenz."},"deepseek-vl2":{"description":"DeepSeek VL2, ein multimodales Modell mit Unterst\xfctzung f\xfcr Bild-Text-Verst\xe4ndnis und fein abgestimmte visuelle Fragebeantwortung."},"deepseek-vl2-small":{"description":"DeepSeek VL2 Small, eine leichte multimodale Version, geeignet f\xfcr ressourcenbeschr\xe4nkte und hochparallele Szenarien."},"deepseek/deepseek-chat-v3-0324":{"description":"DeepSeek V3 ist ein Experten-Mischmodell mit 685B Parametern und die neueste Iteration der Flaggschiff-Chatmodellreihe des DeepSeek-Teams.\\n\\nEs erbt das [DeepSeek V3](/deepseek/deepseek-chat-v3) Modell und zeigt hervorragende Leistungen in verschiedenen Aufgaben."},"deepseek/deepseek-chat-v3-0324:free":{"description":"DeepSeek V3 ist ein Experten-Mischmodell mit 685B Parametern und die neueste Iteration der Flaggschiff-Chatmodellreihe des DeepSeek-Teams.\\n\\nEs erbt das [DeepSeek V3](/deepseek/deepseek-chat-v3) Modell und zeigt hervorragende Leistungen in verschiedenen Aufgaben."},"deepseek/deepseek-chat-v3.1":{"description":"DeepSeek-V3.1 ist ein gro\xdfes hybrides Inferenzmodell, das 128K langen Kontext und effizienten Moduswechsel unterst\xfctzt. Es erzielt herausragende Leistung und Geschwindigkeit bei Tool-Aufrufen, Codegenerierung und komplexen Inferenzaufgaben."},"deepseek/deepseek-r1":{"description":"Das DeepSeek R1 Modell wurde in einer kleinen Version aktualisiert, aktuell DeepSeek-R1-0528. Das neueste Update verbessert die Inferenztiefe und -f\xe4higkeit erheblich durch erh\xf6hte Rechenressourcen und nachtr\xe4gliche algorithmische Optimierungen. Das Modell zeigt hervorragende Leistungen in Mathematik, Programmierung und allgemeiner Logik und n\xe4hert sich f\xfchrenden Modellen wie O3 und Gemini 2.5 Pro an."},"deepseek/deepseek-r1-0528":{"description":"DeepSeek-R1 verbessert die Modellschlussfolgerungsf\xe4higkeit erheblich, selbst bei sehr begrenzten annotierten Daten. Vor der Ausgabe der endg\xfcltigen Antwort generiert das Modell eine Denkprozesskette, um die Genauigkeit der Antwort zu erh\xf6hen."},"deepseek/deepseek-r1-0528:free":{"description":"DeepSeek-R1 verbessert die Modellschlussfolgerungsf\xe4higkeit erheblich, selbst bei sehr begrenzten annotierten Daten. Vor der Ausgabe der endg\xfcltigen Antwort generiert das Modell eine Denkprozesskette, um die Genauigkeit der Antwort zu erh\xf6hen."},"deepseek/deepseek-r1-distill-llama-70b":{"description":"DeepSeek R1 Distill Llama 70B ist ein gro\xdfes Sprachmodell auf Basis von Llama3.3 70B. Durch Feintuning mit den Ausgaben von DeepSeek R1 erreicht es eine konkurrenzf\xe4hige Leistung, die mit f\xfchrenden Gro\xdfmodellen vergleichbar ist."},"deepseek/deepseek-r1-distill-llama-8b":{"description":"DeepSeek R1 Distill Llama 8B ist ein distilliertes gro\xdfes Sprachmodell, das auf Llama-3.1-8B-Instruct basiert und durch Training mit den Ausgaben von DeepSeek R1 erstellt wurde."},"deepseek/deepseek-r1-distill-qwen-14b":{"description":"DeepSeek R1 Distill Qwen 14B ist ein distilliertes gro\xdfes Sprachmodell, das auf Qwen 2.5 14B basiert und durch Training mit den Ausgaben von DeepSeek R1 erstellt wurde. Dieses Modell hat in mehreren Benchmark-Tests OpenAI\'s o1-mini \xfcbertroffen und die neuesten technologischen Fortschritte bei dichten Modellen (state-of-the-art) erzielt. Hier sind einige Ergebnisse der Benchmark-Tests:\\nAIME 2024 pass@1: 69.7\\nMATH-500 pass@1: 93.9\\nCodeForces Rating: 1481\\nDas Modell zeigt durch Feinabstimmung mit den Ausgaben von DeepSeek R1 eine wettbewerbsf\xe4hige Leistung, die mit gr\xf6\xdferen, fortschrittlichen Modellen vergleichbar ist."},"deepseek/deepseek-r1-distill-qwen-32b":{"description":"DeepSeek R1 Distill Qwen 32B ist ein distilliertes gro\xdfes Sprachmodell, das auf Qwen 2.5 32B basiert und durch Training mit den Ausgaben von DeepSeek R1 erstellt wurde. Dieses Modell hat in mehreren Benchmark-Tests OpenAI\'s o1-mini \xfcbertroffen und die neuesten technologischen Fortschritte bei dichten Modellen (state-of-the-art) erzielt. Hier sind einige Ergebnisse der Benchmark-Tests:\\nAIME 2024 pass@1: 72.6\\nMATH-500 pass@1: 94.3\\nCodeForces Rating: 1691\\nDas Modell zeigt durch Feinabstimmung mit den Ausgaben von DeepSeek R1 eine wettbewerbsf\xe4hige Leistung, die mit gr\xf6\xdferen, fortschrittlichen Modellen vergleichbar ist."},"deepseek/deepseek-r1/community":{"description":"DeepSeek R1 ist das neueste Open-Source-Modell, das vom DeepSeek-Team ver\xf6ffentlicht wurde und \xfcber eine \xe4u\xdferst leistungsstarke Inferenzleistung verf\xfcgt, insbesondere in den Bereichen Mathematik, Programmierung und logisches Denken, die mit dem OpenAI o1-Modell vergleichbar ist."},"deepseek/deepseek-r1:free":{"description":"DeepSeek-R1 hat die Schlussfolgerungsf\xe4higkeiten des Modells erheblich verbessert, selbst bei nur wenigen gekennzeichneten Daten. Bevor das Modell die endg\xfcltige Antwort ausgibt, gibt es zun\xe4chst eine Denkprozesskette aus, um die Genauigkeit der endg\xfcltigen Antwort zu erh\xf6hen."},"deepseek/deepseek-v3":{"description":"Schnelles, universelles gro\xdfes Sprachmodell mit verbesserter Inferenzf\xe4higkeit."},"deepseek/deepseek-v3.1-base":{"description":"DeepSeek V3.1 Base ist eine verbesserte Version des DeepSeek V3 Modells."},"deepseek/deepseek-v3/community":{"description":"DeepSeek-V3 hat einen bedeutenden Durchbruch in der Inferenzgeschwindigkeit im Vergleich zu fr\xfcheren Modellen erzielt. Es belegt den ersten Platz unter den Open-Source-Modellen und kann mit den weltweit fortschrittlichsten propriet\xe4ren Modellen konkurrieren. DeepSeek-V3 verwendet die Multi-Head-Latent-Attention (MLA) und die DeepSeekMoE-Architektur, die in DeepSeek-V2 umfassend validiert wurden. Dar\xfcber hinaus hat DeepSeek-V3 eine unterst\xfctzende verlustfreie Strategie f\xfcr die Lastenverteilung eingef\xfchrt und mehrere Zielvorgaben f\xfcr das Training von Mehrfachvorhersagen festgelegt, um eine st\xe4rkere Leistung zu erzielen."},"deepseek_r1":{"description":"DeepSeek-R1 ist ein durch verst\xe4rktes Lernen (RL) gesteuertes Schlussfolgerungsmodell, das Probleme der Wiederholung und Lesbarkeit im Modell l\xf6st. Vor dem RL f\xfchrte DeepSeek-R1 Kaltstartdaten ein, um die Inferenzleistung weiter zu optimieren. Es zeigt in Mathematik, Programmierung und Schlussfolgerungsaufgaben vergleichbare Leistungen zu OpenAI-o1 und hat durch sorgf\xe4ltig gestaltete Trainingsmethoden die Gesamtleistung verbessert."},"deepseek_r1_distill_llama_70b":{"description":"DeepSeek-R1-Distill-Llama-70B ist ein Modell, das durch Destillationstraining auf der Basis von Llama-3.3-70B-Instruct entwickelt wurde. Dieses Modell ist Teil der DeepSeek-R1-Serie und zeigt durch die Feinabstimmung mit Beispielen, die von DeepSeek-R1 generiert wurden, hervorragende Leistungen in Mathematik, Programmierung und Schlussfolgerung in mehreren Bereichen."},"deepseek_r1_distill_qwen_14b":{"description":"DeepSeek-R1-Distill-Qwen-14B ist ein Modell, das durch Wissensdistillation auf der Basis von Qwen2.5-14B entwickelt wurde. Dieses Modell wurde mit 800.000 ausgew\xe4hlten Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt hervorragende Schlussfolgerungsf\xe4higkeiten."},"deepseek_r1_distill_qwen_32b":{"description":"DeepSeek-R1-Distill-Qwen-32B ist ein Modell, das durch Wissensdistillation auf der Basis von Qwen2.5-32B entwickelt wurde. Dieses Modell wurde mit 800.000 ausgew\xe4hlten Beispielen, die von DeepSeek-R1 generiert wurden, feinabgestimmt und zeigt in Mathematik, Programmierung und Schlussfolgerung in mehreren Bereichen herausragende Leistungen."},"doubao-1.5-lite-32k":{"description":"Doubao-1.5-lite ist das neueste leichte Modell der n\xe4chsten Generation, das eine extrem schnelle Reaktionszeit bietet und sowohl in der Leistung als auch in der Latenz weltweit erstklassig ist."},"doubao-1.5-pro-256k":{"description":"Doubao-1.5-pro-256k ist die umfassend verbesserte Version von Doubao-1.5-Pro, die die Gesamtleistung um 10 % steigert. Es unterst\xfctzt Schlussfolgerungen mit einem Kontextfenster von 256k und eine maximale Ausgabel\xe4nge von 12k Tokens. H\xf6here Leistung, gr\xf6\xdferes Fenster und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis machen es f\xfcr eine breitere Palette von Anwendungsszenarien geeignet."},"doubao-1.5-pro-32k":{"description":"Doubao-1.5-pro ist das neueste Hauptmodell der n\xe4chsten Generation, dessen Leistung umfassend verbessert wurde und das in den Bereichen Wissen, Code, Schlussfolgerungen usw. herausragende Leistungen zeigt."},"doubao-1.5-thinking-pro":{"description":"Das Doubao-1.5 Modell f\xfcr tiefes Denken ist neu und zeichnet sich in Fachbereichen wie Mathematik, Programmierung und wissenschaftlichem Denken sowie in allgemeinen Aufgaben wie kreativem Schreiben aus. Es erreicht oder n\xe4hert sich in mehreren renommierten Benchmarks wie AIME 2024, Codeforces und GPQA dem Spitzenlevel der Branche. Es unterst\xfctzt ein Kontextfenster von 128k und eine Ausgabe von 16k."},"doubao-1.5-thinking-pro-m":{"description":"Doubao-1.5 ist ein neues tiefgr\xfcndiges Denkmodell (m-Version mit nativer multimodaler Tiefeninferenzf\xe4higkeit), das in Fachgebieten wie Mathematik, Programmierung, wissenschaftlichem Denken sowie bei allgemeinen Aufgaben wie kreativem Schreiben herausragende Leistungen zeigt. Erreicht oder n\xe4hert sich in renommierten Benchmarks wie AIME 2024, Codeforces und GPQA der Spitzenklasse der Branche. Unterst\xfctzt ein Kontextfenster von 128k und eine Ausgabe von 16k."},"doubao-1.5-thinking-vision-pro":{"description":"Neues visuelles Tiefendenkmodell mit st\xe4rkerer allgemeiner multimodaler Verst\xe4ndnis- und Inferenzf\xe4higkeit, das in 37 von 59 \xf6ffentlichen Benchmark-Tests SOTA-Leistungen erzielt."},"doubao-1.5-ui-tars":{"description":"Doubao-1.5-UI-TARS ist ein nativer Agentenmodell f\xfcr grafische Benutzeroberfl\xe4chen (GUI). Es interagiert nahtlos mit GUIs durch menschen\xe4hnliche F\xe4higkeiten wie Wahrnehmung, Inferenz und Handlung."},"doubao-1.5-vision-lite":{"description":"Doubao-1.5-vision-lite ist ein neu verbessertes multimodales gro\xdfes Modell, das beliebige Aufl\xf6sungen und extreme Seitenverh\xe4ltnisse bei der Bilderkennung unterst\xfctzt und die F\xe4higkeiten in visueller Schlussfolgerung, Dokumentenerkennung, Detailverst\xe4ndnis und Befolgung von Anweisungen verbessert. Es unterst\xfctzt ein Kontextfenster von 128k und eine maximale Ausgabel\xe4nge von 16k Tokens."},"doubao-1.5-vision-pro":{"description":"Doubao-1.5-vision-pro ist ein neu aufger\xfcstetes multimodales Gro\xdfmodell, das Bilderkennung in beliebiger Aufl\xf6sung und extremen Seitenverh\xe4ltnissen unterst\xfctzt und die F\xe4higkeiten in visueller Inferenz, Dokumentenerkennung, Detailverst\xe4ndnis und Befolgung von Anweisungen verbessert."},"doubao-1.5-vision-pro-32k":{"description":"Doubao-1.5-vision-pro ist ein neu aufger\xfcstetes multimodales Gro\xdfmodell, das Bilderkennung in beliebiger Aufl\xf6sung und extremen Seitenverh\xe4ltnissen unterst\xfctzt und die F\xe4higkeiten in visueller Inferenz, Dokumentenerkennung, Detailverst\xe4ndnis und Befolgung von Anweisungen verbessert."},"doubao-lite-128k":{"description":"Bietet extrem schnelle Reaktionszeiten und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis, um Kunden in verschiedenen Szenarien flexiblere Optionen zu bieten. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 128k."},"doubao-lite-32k":{"description":"Bietet extrem schnelle Reaktionszeiten und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis, um Kunden in verschiedenen Szenarien flexiblere Optionen zu bieten. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 32k."},"doubao-lite-4k":{"description":"Bietet extrem schnelle Reaktionszeiten und ein hervorragendes Preis-Leistungs-Verh\xe4ltnis, um Kunden in verschiedenen Szenarien flexiblere Optionen zu bieten. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 4k."},"doubao-pro-256k":{"description":"Das leistungsst\xe4rkste Hauptmodell, geeignet f\xfcr komplexe Aufgaben. Es erzielt hervorragende Ergebnisse in Szenarien wie Referenzfragen, Zusammenfassungen, kreatives Schreiben, Textklassifikation und Rollenspielen. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 256k."},"doubao-pro-32k":{"description":"Das leistungsst\xe4rkste Hauptmodell, geeignet f\xfcr komplexe Aufgaben. Es erzielt hervorragende Ergebnisse in Szenarien wie Referenzfragen, Zusammenfassungen, kreatives Schreiben, Textklassifikation und Rollenspielen. Unterst\xfctzt Inferenz und Feintuning mit einem Kontextfenster von 32k."},"doubao-seed-1.6":{"description":"Doubao-Seed-1.6 ist ein neues multimodales Modell f\xfcr tiefgehendes Denken, das drei Denkmodi unterst\xfctzt: auto, thinking und non-thinking. Im non-thinking-Modus ist die Modellleistung im Vergleich zu Doubao-1.5-pro/250115 deutlich verbessert. Unterst\xfctzt ein Kontextfenster von 256k und eine maximale Ausgabel\xe4nge von 16k Tokens."},"doubao-seed-1.6-flash":{"description":"Doubao-Seed-1.6-flash ist ein multimodales Modell f\xfcr tiefgehendes Denken mit extrem schneller Inferenzgeschwindigkeit, TPOT ben\xf6tigt nur 10 ms; unterst\xfctzt sowohl Text- als auch visuelles Verst\xe4ndnis, die Textverst\xe4ndnisf\xe4higkeit \xfcbertrifft die vorherige Lite-Generation, das visuelle Verst\xe4ndnis ist vergleichbar mit den Pro-Modellen der Konkurrenz. Unterst\xfctzt ein Kontextfenster von 256k und eine maximale Ausgabel\xe4nge von 16k Tokens."},"doubao-seed-1.6-lite":{"description":"Doubao-Seed-1.6-lite ist ein neues multimodales Modell f\xfcr tiefes Denken mit einstellbarem Denkaufwand (reasoning effort) in vier Stufen: Minimal, Low, Medium und High. Es bietet ein hervorragendes Preis-Leistungs-Verh\xe4ltnis und ist die beste Wahl f\xfcr g\xe4ngige Aufgaben, mit einem Kontextfenster von bis zu 256k."},"doubao-seed-1.6-thinking":{"description":"Das Doubao-Seed-1.6-thinking Modell verf\xfcgt \xfcber stark verbesserte Denkf\xe4higkeiten. Im Vergleich zu Doubao-1.5-thinking-pro wurden die Grundf\xe4higkeiten in Coding, Mathematik und logischem Denken weiter verbessert und unterst\xfctzt visuelles Verst\xe4ndnis. Unterst\xfctzt ein Kontextfenster von 256k und eine maximale Ausgabel\xe4nge von 16k Tokens."},"doubao-seed-1.6-vision":{"description":"Doubao-Seed-1.6-vision ist ein visuelles Tiefdenkmodell, das in Szenarien wie Bildung, Bildpr\xfcfung, Inspektion und Sicherheit sowie KI-Suchfragen eine st\xe4rkere allgemeine multimodale Verst\xe4ndnis- und Schlussfolgerungsf\xe4higkeit zeigt. Unterst\xfctzt ein Kontextfenster von 256k und eine maximale Ausgabel\xe4nge von 64k Tokens."},"doubao-seededit-3-0-i2i-250628":{"description":"Das Doubao-Bildgenerierungsmodell wurde vom Seed-Team von ByteDance entwickelt, unterst\xfctzt Texteingaben und Bilder und bietet eine hochgradig kontrollierbare, qualitativ hochwertige Bildgenerierung. Es erm\xf6glicht die Bildbearbeitung mittels Textanweisungen mit Bildseitenl\xe4ngen zwischen 512 und 1536 Pixel."},"doubao-seedream-3-0-t2i-250415":{"description":"Seedream 3.0 Bildgenerierungsmodell vom Seed-Team von ByteDance, unterst\xfctzt Texteingaben und Bilder und bietet eine hochgradig kontrollierbare, qualitativ hochwertige Bildgenerierung. Bilder werden basierend auf Textanweisungen erzeugt."},"doubao-seedream-4-0-250828":{"description":"Seedream 4.0 Bildgenerierungsmodell vom Seed-Team von ByteDance, unterst\xfctzt Texteingaben und Bilder und bietet eine hochgradig kontrollierbare, qualitativ hochwertige Bildgenerierung. Bilder werden basierend auf Textanweisungen erzeugt."},"doubao-vision-lite-32k":{"description":"Das Doubao-vision-Modell ist ein multimodales Gro\xdfmodell von Doubao mit starker Bildverst\xe4ndnis- und Inferenzf\xe4higkeit sowie pr\xe4ziser Befehlsinterpretation. Es zeigt starke Leistung bei der Extraktion von Bild- und Textinformationen sowie bei bildbasierten Inferenzaufgaben und eignet sich f\xfcr komplexere und umfassendere visuelle Frage-Antwort-Aufgaben."},"doubao-vision-pro-32k":{"description":"Das Doubao-vision-Modell ist ein multimodales Gro\xdfmodell von Doubao mit starker Bildverst\xe4ndnis- und Inferenzf\xe4higkeit sowie pr\xe4ziser Befehlsinterpretation. Es zeigt starke Leistung bei der Extraktion von Bild- und Textinformationen sowie bei bildbasierten Inferenzaufgaben und eignet sich f\xfcr komplexere und umfassendere visuelle Frage-Antwort-Aufgaben."},"emohaa":{"description":"Emohaa ist ein psychologisches Modell mit professionellen Beratungsf\xe4higkeiten, das den Nutzern hilft, emotionale Probleme zu verstehen."},"ernie-4.5-0.3b":{"description":"ERNIE 4.5 0.3B, ein leichtgewichtiges Open-Source-Modell, ideal f\xfcr lokale und ma\xdfgeschneiderte Bereitstellungen."},"ernie-4.5-21b-a3b":{"description":"ERNIE 4.5 21B A3B, ein Open-Source-Modell mit gro\xdfer Parameteranzahl, leistungsstark bei Verst\xe4ndnis- und Generierungsaufgaben."},"ernie-4.5-300b-a47b":{"description":"ERNIE 4.5 300B A47B ist ein gro\xdfskaliges hybrides Expertenmodell von Baidu Wenxin mit exzellenten F\xe4higkeiten im logischen Schlussfolgern."},"ernie-4.5-8k-preview":{"description":"ERNIE 4.5 8K Preview, ein Vorschau-Modell mit 8K-Kontext, zur Erprobung und zum Testen der F\xe4higkeiten von Wenxin 4.5."},"ernie-4.5-turbo-128k":{"description":"ERNIE 4.5 Turbo 128K, ein leistungsstarkes Allzweckmodell mit Unterst\xfctzung f\xfcr suchbasierte Erweiterung und Tool-Nutzung, geeignet f\xfcr QA, Code, Agenten und mehr."},"ernie-4.5-turbo-128k-preview":{"description":"ERNIE 4.5 Turbo 128K Preview, eine Vorschauversion mit denselben F\xe4higkeiten wie die finale Version, ideal f\xfcr Integrationstests und schrittweise Einf\xfchrung."},"ernie-4.5-turbo-32k":{"description":"ERNIE 4.5 Turbo 32K, eine Version mit mittellangem Kontext, geeignet f\xfcr QA, Wissensdatenbankabfragen und mehrstufige Dialoge."},"ernie-4.5-turbo-latest":{"description":"ERNIE 4.5 Turbo Latest, die neueste Version mit umfassender Leistungsoptimierung, ideal als Hauptmodell f\xfcr Produktionsumgebungen."},"ernie-4.5-turbo-vl":{"description":"ERNIE 4.5 Turbo VL, ein ausgereiftes multimodales Modell f\xfcr Bild-Text-Verst\xe4ndnis und Erkennung in Produktionsumgebungen."},"ernie-4.5-turbo-vl-32k":{"description":"ERNIE 4.5 Turbo VL 32K, eine multimodale Version mit mittellangem Textkontext, geeignet f\xfcr das kombinierte Verst\xe4ndnis von langen Dokumenten und Bildern."},"ernie-4.5-turbo-vl-32k-preview":{"description":"ERNIE 4.5 Turbo VL 32K Preview, eine Vorschauversion des multimodalen 32K-Modells zur Bewertung der Langkontext-Bildverarbeitungsf\xe4higkeiten."},"ernie-4.5-turbo-vl-latest":{"description":"ERNIE 4.5 Turbo VL Latest, die neueste multimodale Version mit verbesserter Bild-Text-Verst\xe4ndnis- und Inferenzleistung."},"ernie-4.5-turbo-vl-preview":{"description":"ERNIE 4.5 Turbo VL Preview, ein multimodales Vorschau-Modell mit Unterst\xfctzung f\xfcr Bild-Text-Verst\xe4ndnis und -Generierung, ideal f\xfcr visuelle QA und Inhaltsverst\xe4ndnis."},"ernie-4.5-vl-28b-a3b":{"description":"ERNIE 4.5 VL 28B A3B, ein Open-Source-Multimodalmodell f\xfcr Bild-Text-Verst\xe4ndnis und Inferenzaufgaben."},"ernie-5.0-thinking-preview":{"description":"Wenxin 5.0 Thinking Preview, ein natives, multimodales Flaggschiffmodell mit einheitlicher Modellierung von Text, Bild, Audio und Video. Umfassend verbesserte F\xe4higkeiten f\xfcr komplexe QA, kreative Aufgaben und Agentenszenarien."},"ernie-char-8k":{"description":"ERNIE Character 8K, ein dialogorientiertes Modell mit Charakterpers\xf6nlichkeit, ideal f\xfcr IP-Charakterentwicklung und langfristige Begleitdialoge."},"ernie-char-fiction-8k":{"description":"ERNIE Character Fiction 8K, ein Pers\xf6nlichkeitsmodell f\xfcr Roman- und Storytelling, geeignet f\xfcr die Generierung langer Geschichten."},"ernie-char-fiction-8k-preview":{"description":"ERNIE Character Fiction 8K Preview, eine Vorschauversion f\xfcr Charakter- und Storytelling-Modelle zur Funktionsbewertung und zum Testen."},"ernie-irag-edit":{"description":"ERNIE iRAG Edit, ein Bildbearbeitungsmodell mit Unterst\xfctzung f\xfcr Bildl\xf6schung, Neuzeichnung und Varianten-Generierung."},"ernie-lite-8k":{"description":"ERNIE Lite 8K, ein leichtgewichtiges Allzweckmodell, ideal f\xfcr kostensensitive Alltags-QA- und Content-Generierungsszenarien."},"ernie-lite-pro-128k":{"description":"ERNIE Lite Pro 128K, ein leichtes Hochleistungsmodell, geeignet f\xfcr latenz- und kostensensitive Gesch\xe4ftsanwendungen."},"ernie-novel-8k":{"description":"ERNIE Novel 8K, ein Modell f\xfcr Romane und IP-Storytelling, spezialisiert auf Mehrcharakter- und Multistrang-Erz\xe4hlungen."},"ernie-speed-128k":{"description":"ERNIE Speed 128K, ein gro\xdfes Modell ohne Ein-/Ausgabegeb\xfchren, ideal f\xfcr Langtextverst\xe4ndnis und gro\xdffl\xe4chige Testszenarien."},"ernie-speed-8k":{"description":"ERNIE Speed 8K, ein kostenloses Schnellmodell, geeignet f\xfcr allt\xe4gliche Dialoge und leichte Textaufgaben."},"ernie-speed-pro-128k":{"description":"ERNIE Speed Pro 128K, ein hochgradig paralleles und kosteneffizientes Modell, ideal f\xfcr gro\xdffl\xe4chige Online-Dienste und Unternehmensanwendungen."},"ernie-tiny-8k":{"description":"ERNIE Tiny 8K, ein extrem leichtes Modell, geeignet f\xfcr einfache QA-, Klassifizierungs- und kosteng\xfcnstige Inferenzszenarien."},"ernie-x1-turbo-32k":{"description":"ERNIE X1 Turbo 32K, ein Hochgeschwindigkeits-Denkmodell mit 32K-Kontext, geeignet f\xfcr komplexe Inferenz und mehrstufige Dialoge."},"ernie-x1.1-preview":{"description":"ERNIE X1.1 Preview, eine Vorschauversion des Denkmodells ERNIE X1.1, geeignet f\xfcr F\xe4higkeitsvalidierung und Tests."},"fal-ai/bytedance/seedream/v4":{"description":"Seedream 4.0 Bildgenerierungsmodell vom Seed-Team von ByteDance, unterst\xfctzt Texteingaben und Bilder und bietet eine hochgradig kontrollierbare, qualitativ hochwertige Bildgenerierung. Bilder werden basierend auf Textanweisungen erzeugt."},"fal-ai/flux-kontext/dev":{"description":"FLUX.1 Modell, spezialisiert auf Bildbearbeitungsaufgaben, unterst\xfctzt Texteingaben und Bilder."},"fal-ai/flux-pro/kontext":{"description":"FLUX.1 Kontext [pro] kann Texte und Referenzbilder als Eingabe verarbeiten und erm\xf6glicht nahtlose zielgerichtete lokale Bearbeitungen sowie komplexe umfassende Szenenver\xe4nderungen."},"fal-ai/flux/krea":{"description":"Flux Krea [dev] ist ein bildgenerierendes Modell mit \xe4sthetischer Pr\xe4ferenz, das darauf abzielt, realistischere und nat\xfcrlichere Bilder zu erzeugen."},"fal-ai/flux/schnell":{"description":"FLUX.1 [schnell] ist ein bildgenerierendes Modell mit 12 Milliarden Parametern, das sich auf die schnelle Erzeugung hochwertiger Bilder konzentriert."},"fal-ai/hunyuan-image/v3":{"description":"Ein leistungsstarkes natives multimodales Bildgenerierungsmodell"},"fal-ai/imagen4/preview":{"description":"Hochwertiges Bildgenerierungsmodell von Google."},"fal-ai/nano-banana":{"description":"Nano Banana ist Googles neuestes, schnellstes und effizientestes natives multimodales Modell, das es erm\xf6glicht, Bilder durch Dialog zu erzeugen und zu bearbeiten."},"fal-ai/qwen-image":{"description":"Das leistungsstarke Rohbildmodell des Qwen-Teams beeindruckt mit hervorragender chinesischer Textgenerierung und vielf\xe4ltigen visuellen Bildstilen."},"fal-ai/qwen-image-edit":{"description":"Das professionelle Bildbearbeitungsmodell des Qwen-Teams unterst\xfctzt semantische und optische Bearbeitungen, erm\xf6glicht pr\xe4zise Bearbeitung chinesischer und englischer Texte sowie Stilwechsel, Objektrotation und weitere hochwertige Bildbearbeitungen."},"flux-1-schnell":{"description":"Ein von Black Forest Labs entwickeltes Text-zu-Bild-Modell mit 12 Milliarden Parametern, das latente adversariale Diffusionsdestillation verwendet und in 1 bis 4 Schritten hochwertige Bilder erzeugen kann. Die Leistung ist vergleichbar mit propriet\xe4ren Alternativen und wird unter der Apache-2.0-Lizenz f\xfcr private, wissenschaftliche und kommerzielle Nutzung ver\xf6ffentlicht."},"flux-dev":{"description":"FLUX.1 [dev] ist ein Open-Source-Gewichtungs- und Feinschlichtungsmodell f\xfcr nicht-kommerzielle Anwendungen. Es bietet eine Bildqualit\xe4t und Instruktionsbefolgung \xe4hnlich der professionellen FLUX-Version, jedoch mit h\xf6herer Effizienz. Im Vergleich zu Standardmodellen gleicher Gr\xf6\xdfe ist es ressourcenschonender."},"flux-kontext-max":{"description":"Modernste kontextbezogene Bildgenerierung und -bearbeitung – verbindet Text und Bilder, um pr\xe4zise, koh\xe4rente Ergebnisse zu erzielen."},"flux-kontext-pro":{"description":"Modernste kontextbezogene Bildgenerierung und -bearbeitung – verbindet Text und Bild zu pr\xe4zisen, koh\xe4renten Ergebnissen."},"flux-merged":{"description":"Das FLUX.1-merged Modell kombiniert die tiefgehenden Eigenschaften, die in der Entwicklungsphase von „DEV“ erforscht wurden, mit der hohen Ausf\xfchrungsgeschwindigkeit von „Schnell“. Dadurch werden sowohl die Leistungsgrenzen des Modells erweitert als auch dessen Anwendungsbereich vergr\xf6\xdfert."},"flux-pro":{"description":"Premium-kommerzielles KI-Bildgenerierungsmodell — unvergleichliche Bildqualit\xe4t und vielf\xe4ltige Ausgabem\xf6glichkeiten."},"flux-pro-1.1":{"description":"Verbessertes professionelles KI-Modell zur Bildgenerierung — bietet herausragende Bildqualit\xe4t und eine pr\xe4zise Umsetzung von Eingabeaufforderungen."},"flux-pro-1.1-ultra":{"description":"Ultrahochaufl\xf6sende KI-Bildgenerierung — unterst\xfctzt Ausgaben mit 4 Megapixeln und erstellt hochaufl\xf6sende Bilder innerhalb von 10 Sekunden."},"flux-schnell":{"description":"FLUX.1 [schnell] ist das derzeit fortschrittlichste Open-Source-Modell mit wenigen Schritten, das nicht nur Konkurrenten \xfcbertrifft, sondern auch leistungsst\xe4rkere nicht-feinabgestimmte Modelle wie Midjourney v6.0 und DALL\xb7E 3 (HD) \xfcbertrifft. Das Modell wurde speziell feinabgestimmt, um die gesamte Vielfalt der Vortrainingsausgaben zu bewahren. Im Vergleich zu den aktuell besten Modellen auf dem Markt bietet FLUX.1 [schnell] erhebliche Verbesserungen in visueller Qualit\xe4t, Instruktionsbefolgung, Gr\xf6\xdfen- und Proportions\xe4nderungen, Schriftartenverarbeitung und Ausgabediversit\xe4t, was den Nutzern eine reichhaltigere und vielf\xe4ltigere kreative Bildgenerierung erm\xf6glicht."},"flux.1-schnell":{"description":"FLUX.1-schnell, ein leistungsstarkes Bildgenerierungsmodell, ideal f\xfcr die schnelle Erstellung von Bildern in verschiedenen Stilen."},"gemini-1.0-pro-001":{"description":"Gemini 1.0 Pro 001 (Tuning) bietet stabile und anpassbare Leistung und ist die ideale Wahl f\xfcr L\xf6sungen komplexer Aufgaben."},"gemini-1.0-pro-002":{"description":"Gemini 1.0 Pro 002 (Tuning) bietet hervorragende multimodale Unterst\xfctzung und konzentriert sich auf die effektive L\xf6sung komplexer Aufgaben."},"gemini-1.0-pro-latest":{"description":"Gemini 1.0 Pro ist Googles leistungsstarkes KI-Modell, das f\xfcr die Skalierung einer Vielzahl von Aufgaben konzipiert ist."},"gemini-1.5-flash-001":{"description":"Gemini 1.5 Flash 001 ist ein effizientes multimodales Modell, das eine breite Anwendbarkeit unterst\xfctzt."},"gemini-1.5-flash-002":{"description":"Gemini 1.5 Flash 002 ist ein effizientes multimodales Modell, das eine breite Palette von Anwendungen unterst\xfctzt."},"gemini-1.5-flash-8b":{"description":"Gemini 1.5 Flash 8B ist ein leistungsstarkes multimodales Modell, das eine breite Palette von Anwendungen unterst\xfctzt."},"gemini-1.5-flash-8b-exp-0924":{"description":"Gemini 1.5 Flash 8B 0924 ist das neueste experimentelle Modell, das in Text- und multimodalen Anwendungsf\xe4llen erhebliche Leistungsverbesserungen aufweist."},"gemini-1.5-flash-8b-latest":{"description":"Gemini 1.5 Flash 8B ist ein effizientes multimodales Modell, das eine breite Palette von Anwendungen unterst\xfctzt."},"gemini-1.5-flash-exp-0827":{"description":"Gemini 1.5 Flash 0827 bietet optimierte multimodale Verarbeitungskapazit\xe4ten, die f\xfcr verschiedene komplexe Aufgaben geeignet sind."},"gemini-1.5-flash-latest":{"description":"Gemini 1.5 Flash ist Googles neuestes multimodales KI-Modell, das \xfcber schnelle Verarbeitungsf\xe4higkeiten verf\xfcgt und Text-, Bild- und Videoeingaben unterst\xfctzt, um eine effiziente Skalierung f\xfcr verschiedene Aufgaben zu erm\xf6glichen."},"gemini-1.5-pro-001":{"description":"Gemini 1.5 Pro 001 ist eine skalierbare multimodale KI-L\xf6sung, die eine breite Palette komplexer Aufgaben unterst\xfctzt."},"gemini-1.5-pro-002":{"description":"Gemini 1.5 Pro 002 ist das neueste produktionsbereite Modell, das eine h\xf6here Ausgabequalit\xe4t bietet, insbesondere bei mathematischen, langen Kontexten und visuellen Aufgaben erhebliche Verbesserungen aufweist."},"gemini-1.5-pro-exp-0801":{"description":"Gemini 1.5 Pro 0801 bietet herausragende multimodale Verarbeitungskapazit\xe4ten und bringt gr\xf6\xdfere Flexibilit\xe4t in die Anwendungsentwicklung."},"gemini-1.5-pro-exp-0827":{"description":"Gemini 1.5 Pro 0827 kombiniert die neuesten Optimierungstechnologien, um eine effizientere multimodale Datenverarbeitung zu erm\xf6glichen."},"gemini-1.5-pro-latest":{"description":"Gemini 1.5 Pro unterst\xfctzt bis zu 2 Millionen Tokens und ist die ideale Wahl f\xfcr mittelgro\xdfe multimodale Modelle, die umfassende Unterst\xfctzung f\xfcr komplexe Aufgaben bieten."},"gemini-2.0-flash":{"description":"Gemini 2.0 Flash bietet n\xe4chste Generation Funktionen und Verbesserungen, einschlie\xdflich au\xdfergew\xf6hnlicher Geschwindigkeit, nativer Werkzeugnutzung, multimodaler Generierung und einem Kontextfenster von 1M Tokens."},"gemini-2.0-flash-001":{"description":"Gemini 2.0 Flash bietet n\xe4chste Generation Funktionen und Verbesserungen, einschlie\xdflich au\xdfergew\xf6hnlicher Geschwindigkeit, nativer Werkzeugnutzung, multimodaler Generierung und einem Kontextfenster von 1M Tokens."},"gemini-2.0-flash-exp":{"description":"Gemini 2.0 Flash-Modellvariante, die auf Kosteneffizienz und niedrige Latenz optimiert ist."},"gemini-2.0-flash-exp-image-generation":{"description":"Gemini 2.0 Flash Experimentmodell, das die Bildgenerierung unterst\xfctzt"},"gemini-2.0-flash-lite":{"description":"Gemini 2.0 Flash ist eine Modellvariante, die auf Kosteneffizienz und niedrige Latenz optimiert ist."},"gemini-2.0-flash-lite-001":{"description":"Gemini 2.0 Flash ist eine Modellvariante, die auf Kosteneffizienz und niedrige Latenz optimiert ist."},"gemini-2.5-flash":{"description":"Gemini 2.5 Flash ist Googles kosteneffizientestes Modell und bietet umfassende Funktionen."},"gemini-2.5-flash-image":{"description":"Nano Banana ist Googles neuestes, schnellstes und effizientestes natives multimodales Modell, das es Ihnen erm\xf6glicht, Bilder durch Dialog zu generieren und zu bearbeiten."},"gemini-2.5-flash-image-preview":{"description":"Nano Banana ist Googles neuestes, schnellstes und effizientestes natives multimodales Modell, das es Ihnen erm\xf6glicht, Bilder durch Dialog zu generieren und zu bearbeiten."},"gemini-2.5-flash-image-preview:image":{"description":"Nano Banana ist Googles neuestes, schnellstes und effizientestes natives multimodales Modell, das es Ihnen erm\xf6glicht, Bilder durch Dialog zu generieren und zu bearbeiten."},"gemini-2.5-flash-image:image":{"description":"Nano Banana ist Googles neuestes, schnellstes und effizientestes natives multimodales Modell, das es Ihnen erm\xf6glicht, Bilder durch Dialog zu generieren und zu bearbeiten."},"gemini-2.5-flash-lite":{"description":"Gemini 2.5 Flash-Lite ist Googles kleinstes und kosteneffizientestes Modell, das speziell f\xfcr den gro\xdffl\xe4chigen Einsatz entwickelt wurde."},"gemini-2.5-flash-lite-preview-06-17":{"description":"Gemini 2.5 Flash-Lite Preview ist Googles kleinstes und kosteneffizientestes Modell, speziell f\xfcr den gro\xdffl\xe4chigen Einsatz konzipiert."},"gemini-2.5-flash-lite-preview-09-2025":{"description":"Vorschauversion (25. September 2025) von Gemini 2.5 Flash-Lite"},"gemini-2.5-flash-preview-04-17":{"description":"Gemini 2.5 Flash Preview ist das kosteneffizienteste Modell von Google und bietet umfassende Funktionen."},"gemini-2.5-flash-preview-09-2025":{"description":"Vorschauversion (25. September 2025) von Gemini 2.5 Flash"},"gemini-2.5-pro":{"description":"Gemini 2.5 Pro ist Googles fortschrittlichstes Denkmodell, das komplexe Probleme in den Bereichen Code, Mathematik und MINT-F\xe4cher l\xf6sen kann und gro\xdfe Datens\xe4tze, Codebasen und Dokumente mit langem Kontext analysiert."},"gemini-2.5-pro-preview-03-25":{"description":"Gemini 2.5 Pro Preview ist Googles fortschrittlichstes Denkmodell, das in der Lage ist, komplexe Probleme in den Bereichen Code, Mathematik und STEM zu analysieren sowie gro\xdfe Datens\xe4tze, Codebasen und Dokumente mithilfe von langen Kontextanalysen zu verarbeiten."},"gemini-2.5-pro-preview-05-06":{"description":"Gemini 2.5 Pro Preview ist Googles fortschrittlichstes Denkmodell, das in der Lage ist, komplexe Probleme in den Bereichen Code, Mathematik und STEM zu analysieren und gro\xdfe Datens\xe4tze, Codebasen und Dokumente mithilfe von Langzeitkontext zu analysieren."},"gemini-2.5-pro-preview-06-05":{"description":"Gemini 2.5 Pro Preview ist Googles fortschrittlichstes Denkmodell, das komplexe Probleme in den Bereichen Code, Mathematik und MINT-F\xe4cher l\xf6sen kann und gro\xdfe Datens\xe4tze, Codebasen und Dokumente mit langem Kontext analysiert."},"gemini-3-pro-preview":{"description":"Gemini 3 Pro ist das intelligenteste Modell von Google mit modernster Schlussfolgerungsf\xe4higkeit, multimodaler Verarbeitung sowie leistungsstarken Agenten- und Kontextkodierungsfunktionen."},"gemini-flash-latest":{"description":"Neueste Version von Gemini Flash"},"gemini-flash-lite-latest":{"description":"Neueste Version von Gemini Flash-Lite"},"gemini-pro-latest":{"description":"Neueste Version von Gemini Pro"},"gemma-7b-it":{"description":"Gemma 7B eignet sich f\xfcr die Verarbeitung von mittelgro\xdfen Aufgaben und bietet ein gutes Kosten-Nutzen-Verh\xe4ltnis."},"gemma2":{"description":"Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."},"gemma2-9b-it":{"description":"Gemma 2 9B ist ein Modell, das f\xfcr spezifische Aufgaben und die Integration von Werkzeugen optimiert wurde."},"gemma2:27b":{"description":"Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."},"gemma2:2b":{"description":"Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."},"generalv3":{"description":"Spark Pro ist ein hochleistungsf\xe4higes gro\xdfes Sprachmodell, das f\xfcr professionelle Bereiche optimiert ist und sich auf Mathematik, Programmierung, Medizin, Bildung und andere Bereiche konzentriert, und unterst\xfctzt die Online-Suche sowie integrierte Plugins f\xfcr Wetter, Datum usw. Das optimierte Modell zeigt hervorragende Leistungen und hohe Effizienz in komplexen Wissensabfragen, Sprachverst\xe4ndnis und hochrangiger Textgenerierung und ist die ideale Wahl f\xfcr professionelle Anwendungsszenarien."},"generalv3.5":{"description":"Spark3.5 Max ist die umfassendste Version, die Online-Suche und zahlreiche integrierte Plugins unterst\xfctzt. Ihre umfassend optimierten Kernf\xe4higkeiten sowie die Systemrolleneinstellungen und Funktionsaufrufm\xf6glichkeiten erm\xf6glichen eine au\xdfergew\xf6hnliche Leistung in verschiedenen komplexen Anwendungsszenarien."},"glm-4":{"description":"GLM-4 ist die alte Flaggschiffversion, die im Januar 2024 ver\xf6ffentlicht wurde und mittlerweile durch das leistungsst\xe4rkere GLM-4-0520 ersetzt wurde."},"glm-4-0520":{"description":"GLM-4-0520 ist die neueste Modellversion, die f\xfcr hochkomplexe und vielf\xe4ltige Aufgaben konzipiert wurde und hervorragende Leistungen zeigt."},"glm-4-32b-0414":{"description":"GLM-4 32B 0414, eine Version des allgemeinen GLM-Gro\xdfmodells mit Unterst\xfctzung f\xfcr Textgenerierung und -verst\xe4ndnis in mehreren Aufgaben."},"glm-4-9b-chat":{"description":"GLM-4-9B-Chat bietet hohe Leistung in Bereichen wie Semantik, Mathematik, logisches Denken, Programmierung und Wissen. Es unterst\xfctzt Web-Browsing, Code-Ausf\xfchrung, benutzerdefinierte Tool-Nutzung und Langtext-Inferenz. Unterst\xfctzt 26 Sprachen, darunter Japanisch, Koreanisch und Deutsch."},"glm-4-air":{"description":"GLM-4-Air ist eine kosteneffiziente Version, die in der Leistung nahe am GLM-4 liegt und schnelle Geschwindigkeiten zu einem erschwinglichen Preis bietet."},"glm-4-air-250414":{"description":"GLM-4-Air ist die kosteneffiziente Version, deren Leistung nahe an der von GLM-4 liegt und schnelle Geschwindigkeiten zu einem erschwinglichen Preis bietet."},"glm-4-airx":{"description":"GLM-4-AirX bietet eine effiziente Version von GLM-4-Air mit einer Inferenzgeschwindigkeit von bis zu 2,6-fach."},"glm-4-alltools":{"description":"GLM-4-AllTools ist ein multifunktionales Agentenmodell, das optimiert wurde, um komplexe Anweisungsplanung und Werkzeugaufrufe zu unterst\xfctzen, wie z. B. Web-Browsing, Code-Interpretation und Textgenerierung, geeignet f\xfcr die Ausf\xfchrung mehrerer Aufgaben."},"glm-4-flash":{"description":"GLM-4-Flash ist die ideale Wahl f\xfcr die Verarbeitung einfacher Aufgaben, mit der schnellsten Geschwindigkeit und dem besten Preis."},"glm-4-flash-250414":{"description":"GLM-4-Flash ist die ideale Wahl f\xfcr die Bearbeitung einfacher Aufgaben, mit der schnellsten Geschwindigkeit und kostenlos."},"glm-4-flashx":{"description":"GLM-4-FlashX ist eine verbesserte Version von Flash mit extrem schneller Inferenzgeschwindigkeit."},"glm-4-long":{"description":"GLM-4-Long unterst\xfctzt extrem lange Texteingaben und eignet sich f\xfcr Ged\xe4chtnisaufgaben und die Verarbeitung gro\xdfer Dokumente."},"glm-4-plus":{"description":"GLM-4-Plus ist das hochintelligente Flaggschiffmodell mit starken F\xe4higkeiten zur Verarbeitung langer Texte und komplexer Aufgaben, mit umfassenden Leistungsverbesserungen."},"glm-4.1v-thinking-flash":{"description":"Die GLM-4.1V-Thinking-Serie ist das leistungsst\xe4rkste visuelle Modell unter den bekannten 10-Milliarden-Parameter-VLMs und integriert SOTA-Leistungen auf diesem Niveau in verschiedenen visuellen Sprachaufgaben, darunter Videoverstehen, Bildfragen, Fachaufgaben, OCR-Texterkennung, Dokumenten- und Diagramminterpretation, GUI-Agenten, Frontend-Web-Coding und Grounding. In vielen Aufgaben \xfcbertrifft es sogar das Qwen2.5-VL-72B mit achtmal so vielen Parametern. Durch fortschrittliche Verst\xe4rkungslernverfahren beherrscht das Modell die Chain-of-Thought-Schlussfolgerung, was die Genauigkeit und Detailtiefe der Antworten deutlich verbessert und in Bezug auf Endergebnis und Erkl\xe4rbarkeit traditionelle Nicht-Thinking-Modelle \xfcbertrifft."},"glm-4.1v-thinking-flashx":{"description":"Die GLM-4.1V-Thinking-Serie ist das leistungsst\xe4rkste visuelle Modell unter den bekannten 10-Milliarden-Parameter-VLMs und integriert SOTA-Leistungen auf diesem Niveau in verschiedenen visuellen Sprachaufgaben, darunter Videoverstehen, Bildfragen, Fachaufgaben, OCR-Texterkennung, Dokumenten- und Diagramminterpretation, GUI-Agenten, Frontend-Web-Coding und Grounding. In vielen Aufgaben \xfcbertrifft es sogar das Qwen2.5-VL-72B mit achtmal so vielen Parametern. Durch fortschrittliche Verst\xe4rkungslernverfahren beherrscht das Modell die Chain-of-Thought-Schlussfolgerung, was die Genauigkeit und Detailtiefe der Antworten deutlich verbessert und in Bezug auf Endergebnis und Erkl\xe4rbarkeit traditionelle Nicht-Thinking-Modelle \xfcbertrifft."},"glm-4.5":{"description":"Das Flaggschiff-Modell von Zhipu unterst\xfctzt den Wechsel zwischen Denkmodi und erreicht eine umfassende Leistungsf\xe4higkeit auf dem Niveau der besten Open-Source-Modelle. Die Kontextl\xe4nge betr\xe4gt bis zu 128K."},"glm-4.5-air":{"description":"Die leichtgewichtige Version von GLM-4.5, die Leistung und Kosten-Nutzen-Verh\xe4ltnis ausbalanciert und flexibel zwischen hybriden Denkmodellen wechseln kann."},"glm-4.5-airx":{"description":"Die Turbo-Version von GLM-4.5-Air mit schnellerer Reaktionszeit, speziell f\xfcr gro\xdfskalige und hochgeschwindigkeitsbed\xfcrftige Anwendungen entwickelt."},"glm-4.5-flash":{"description":"Die kostenlose Version von GLM-4.5, die bei Inferenz, Programmierung und Agentenaufgaben hervorragende Leistungen zeigt."},"glm-4.5-x":{"description":"Die Turbo-Version von GLM-4.5, die bei starker Leistung eine Generierungsgeschwindigkeit von bis zu 100 Tokens pro Sekunde erreicht."},"glm-4.5v":{"description":"Das neue visuelle Inferenzmodell der n\xe4chsten Generation von Zhipu, basierend auf der MOE-Architektur, verf\xfcgt \xfcber 106B Gesamtparameter und 12B aktivierte Parameter und erzielt in verschiedenen Benchmarks State-of-the-Art‑Ergebnisse (SOTA) unter weltweit vergleichbaren Open‑Source‑multimodalen Modellen. Es deckt g\xe4ngige Aufgaben wie Bild-, Video- und Dokumentenverst\xe4ndnis sowie GUI‑Aufgaben ab."},"glm-4.6":{"description":"Das neueste Flaggschiff-Modell von Zhipu, GLM-4.6 (355B), \xfcbertrifft die Vorg\xe4ngergeneration in fortgeschrittener Codierung, Langtextverarbeitung, Inferenz und Agentenf\xe4higkeiten umfassend. Besonders in der Programmierf\xe4higkeit ist es mit Claude Sonnet 4 vergleichbar und gilt als eines der besten Coding-Modelle im Inland."},"glm-4v":{"description":"GLM-4V bietet starke F\xe4higkeiten zur Bildverst\xe4ndnis und -schlussfolgerung und unterst\xfctzt eine Vielzahl visueller Aufgaben."},"glm-4v-flash":{"description":"GLM-4V-Flash konzentriert sich auf die effiziente Verarbeitung einzelner Bilder und eignet sich f\xfcr Szenarien der schnellen Bildanalyse, wie z. B. die Echtzeitanalyse von Bildern oder die Verarbeitung von Bilddaten in gro\xdfen Mengen."},"glm-4v-plus":{"description":"GLM-4V-Plus hat die F\xe4higkeit, Videoinhalte und mehrere Bilder zu verstehen und eignet sich f\xfcr multimodale Aufgaben."},"glm-4v-plus-0111":{"description":"GLM-4V-Plus verf\xfcgt \xfcber die F\xe4higkeit, Videoinhalte und mehrere Bilder zu verstehen und eignet sich f\xfcr multimodale Aufgaben."},"glm-z1-air":{"description":"Schlussfolgerungsmodell: Verf\xfcgt \xfcber starke Schlussfolgerungsf\xe4higkeiten und eignet sich f\xfcr Aufgaben, die tiefes Denken erfordern."},"glm-z1-airx":{"description":"Blitzschlussfolgerung: Bietet extrem schnelle Schlussfolgerungsgeschwindigkeit und starke Schlussfolgerungseffekte."},"glm-z1-flash":{"description":"Die GLM-Z1-Serie verf\xfcgt \xfcber starke F\xe4higkeiten im komplexen logischen Denken und zeigt hervorragende Leistungen in Logik, Mathematik und Programmierung."},"glm-z1-flashx":{"description":"Hohe Geschwindigkeit zu niedrigem Preis: Flash-verbesserte Version mit ultraschneller Inferenzgeschwindigkeit und schnellerer gleichzeitiger Verarbeitung."},"glm-zero-preview":{"description":"GLM-Zero-Preview verf\xfcgt \xfcber starke F\xe4higkeiten zur komplexen Schlussfolgerung und zeigt hervorragende Leistungen in den Bereichen logisches Denken, Mathematik und Programmierung."},"google/gemini-2.0-flash":{"description":"Gemini 2.0 Flash bietet Funktionen der n\xe4chsten Generation und Verbesserungen, darunter herausragende Geschwindigkeit, integrierte Werkzeugnutzung, multimodale Generierung und ein Kontextfenster von 1 Million Tokens."},"google/gemini-2.0-flash-001":{"description":"Gemini 2.0 Flash bietet n\xe4chste Generation Funktionen und Verbesserungen, einschlie\xdflich au\xdfergew\xf6hnlicher Geschwindigkeit, nativer Werkzeugnutzung, multimodaler Generierung und einem Kontextfenster von 1M Tokens."},"google/gemini-2.0-flash-exp:free":{"description":"Gemini 2.0 Flash Experimental ist Googles neuestes experimentelles multimodales KI-Modell, das im Vergleich zu fr\xfcheren Versionen eine gewisse Qualit\xe4tsverbesserung aufweist, insbesondere in Bezug auf Weltwissen, Code und langen Kontext."},"google/gemini-2.0-flash-lite":{"description":"Gemini 2.0 Flash Lite bietet Funktionen der n\xe4chsten Generation und Verbesserungen, darunter herausragende Geschwindigkeit, integrierte Werkzeugnutzung, multimodale Generierung und ein Kontextfenster von 1 Million Tokens."},"google/gemini-2.5-flash":{"description":"Gemini 2.5 Flash ist ein Denkmodell mit hervorragenden umfassenden F\xe4higkeiten. Es ist auf ein ausgewogenes Verh\xe4ltnis von Preis und Leistung ausgelegt und unterst\xfctzt multimodale Eingaben sowie ein Kontextfenster von 1 Million Tokens."},"google/gemini-2.5-flash-image-preview":{"description":"Gemini 2.5 Flash Experimentelles Modell, unterst\xfctzt Bildgenerierung"},"google/gemini-2.5-flash-lite":{"description":"Gemini 2.5 Flash-Lite ist ein ausgewogenes, latenzarmes Modell mit konfigurierbarem Denkbudget und Werkzeuganbindung (z. B. Google Search Grounding und Codeausf\xfchrung). Es unterst\xfctzt multimodale Eingaben und bietet ein Kontextfenster von 1 Million Tokens."},"google/gemini-2.5-flash-preview":{"description":"Gemini 2.5 Flash ist Googles fortschrittlichstes Hauptmodell, das f\xfcr fortgeschrittenes Denken, Codierung, Mathematik und wissenschaftliche Aufgaben entwickelt wurde. Es enth\xe4lt die eingebaute F\xe4higkeit zu \\"denken\\", was es ihm erm\xf6glicht, Antworten mit h\xf6herer Genauigkeit und detaillierter Kontextverarbeitung zu liefern.\\n\\nHinweis: Dieses Modell hat zwei Varianten: Denken und Nicht-Denken. Die Ausgabepreise variieren erheblich, je nachdem, ob die Denkf\xe4higkeit aktiviert ist oder nicht. Wenn Sie die Standardvariante (ohne den Suffix \\":thinking\\") w\xe4hlen, wird das Modell ausdr\xfccklich vermeiden, Denk-Tokens zu generieren.\\n\\nUm die Denkf\xe4higkeit zu nutzen und Denk-Tokens zu erhalten, m\xfcssen Sie die \\":thinking\\"-Variante w\xe4hlen, was zu h\xf6heren Preisen f\xfcr Denk-Ausgaben f\xfchrt.\\n\\nDar\xfcber hinaus kann Gemini 2.5 Flash \xfcber den Parameter \\"maximale Tokenanzahl f\xfcr das Denken\\" konfiguriert werden, wie in der Dokumentation beschrieben (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."},"google/gemini-2.5-flash-preview:thinking":{"description":"Gemini 2.5 Flash ist Googles fortschrittlichstes Hauptmodell, das f\xfcr fortgeschrittenes Denken, Codierung, Mathematik und wissenschaftliche Aufgaben entwickelt wurde. Es enth\xe4lt die eingebaute F\xe4higkeit zu \\"denken\\", was es ihm erm\xf6glicht, Antworten mit h\xf6herer Genauigkeit und detaillierter Kontextverarbeitung zu liefern.\\n\\nHinweis: Dieses Modell hat zwei Varianten: Denken und Nicht-Denken. Die Ausgabepreise variieren erheblich, je nachdem, ob die Denkf\xe4higkeit aktiviert ist oder nicht. Wenn Sie die Standardvariante (ohne den Suffix \\":thinking\\") w\xe4hlen, wird das Modell ausdr\xfccklich vermeiden, Denk-Tokens zu generieren.\\n\\nUm die Denkf\xe4higkeit zu nutzen und Denk-Tokens zu erhalten, m\xfcssen Sie die \\":thinking\\"-Variante w\xe4hlen, was zu h\xf6heren Preisen f\xfcr Denk-Ausgaben f\xfchrt.\\n\\nDar\xfcber hinaus kann Gemini 2.5 Flash \xfcber den Parameter \\"maximale Tokenanzahl f\xfcr das Denken\\" konfiguriert werden, wie in der Dokumentation beschrieben (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."},"google/gemini-2.5-pro":{"description":"Gemini 2.5 Pro ist unser fortschrittlichstes Inferenz-Gemini-Modell, das komplexe Probleme l\xf6sen kann. Es verf\xfcgt \xfcber ein Kontextfenster von 2 Millionen Tokens und unterst\xfctzt multimodale Eingaben, darunter Text, Bilder, Audio, Video und PDF-Dokumente."},"google/gemini-2.5-pro-preview":{"description":"Gemini 2.5 Pro Preview ist Googles fortschrittlichstes Denkmodell, das in der Lage ist, komplexe Probleme in den Bereichen Code, Mathematik und MINT zu analysieren sowie gro\xdfe Datens\xe4tze, Codebasen und Dokumente mit langem Kontext zu untersuchen."},"google/gemini-embedding-001":{"description":"Modernstes Einbettungsmodell mit hervorragender Leistung bei englischen, mehrsprachigen und Code-Aufgaben."},"google/gemini-flash-1.5":{"description":"Gemini 1.5 Flash bietet optimierte multimodale Verarbeitungsf\xe4higkeiten, die f\xfcr verschiedene komplexe Aufgabenszenarien geeignet sind."},"google/gemini-pro-1.5":{"description":"Gemini 1.5 Pro kombiniert die neuesten Optimierungstechnologien und bietet eine effizientere Verarbeitung multimodaler Daten."},"google/gemma-2-27b":{"description":"Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexer Datenverarbeitung abdeckt."},"google/gemma-2-27b-it":{"description":"Gemma 2 setzt das Designkonzept von Leichtbau und Effizienz fort."},"google/gemma-2-2b-it":{"description":"Das leichtgewichtige Anweisungsoptimierungsmodell von Google."},"google/gemma-2-9b":{"description":"Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexer Datenverarbeitung abdeckt."},"google/gemma-2-9b-it":{"description":"Gemma 2 ist eine leichtgewichtige Open-Source-Textmodellreihe von Google."},"google/gemma-2-9b-it:free":{"description":"Gemma 2 ist eine leichtgewichtige Open-Source-Textmodellreihe von Google."},"google/gemma-2b-it":{"description":"Gemma Instruct (2B) bietet grundlegende Anweisungsverarbeitungsf\xe4higkeiten und eignet sich f\xfcr leichte Anwendungen."},"google/gemma-3-12b-it":{"description":"Gemma 3 12B ist ein Open-Source-Sprachmodell von Google, das neue Ma\xdfst\xe4be in Effizienz und Leistung setzt."},"google/gemma-3-27b-it":{"description":"Gemma 3 27B ist ein Open-Source-Sprachmodell von Google, das neue Ma\xdfst\xe4be in Bezug auf Effizienz und Leistung setzt."},"google/text-embedding-005":{"description":"Englisch-fokussiertes Texteingebettetes Modell, optimiert f\xfcr Code- und englischsprachige Aufgaben."},"google/text-multilingual-embedding-002":{"description":"Mehrsprachiges Texteingebettetes Modell, optimiert f\xfcr sprach\xfcbergreifende Aufgaben und unterst\xfctzt mehrere Sprachen."},"gpt-3.5-turbo":{"description":"GPT 3.5 Turbo eignet sich f\xfcr eine Vielzahl von Textgenerierungs- und Verst\xe4ndnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."},"gpt-3.5-turbo-0125":{"description":"GPT 3.5 Turbo eignet sich f\xfcr eine Vielzahl von Textgenerierungs- und Verst\xe4ndnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."},"gpt-3.5-turbo-1106":{"description":"GPT 3.5 Turbo eignet sich f\xfcr eine Vielzahl von Textgenerierungs- und Verst\xe4ndnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."},"gpt-3.5-turbo-instruct":{"description":"GPT 3.5 Turbo eignet sich f\xfcr eine Vielzahl von Textgenerierungs- und Verst\xe4ndnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."},"gpt-35-turbo":{"description":"GPT 3.5 Turbo ist ein effizientes Modell von OpenAI, das f\xfcr Chat- und Textgenerierungsaufgaben geeignet ist und parallele Funktionsaufrufe unterst\xfctzt."},"gpt-35-turbo-16k":{"description":"GPT 3.5 Turbo 16k ist ein hochkapazitives Textgenerierungsmodell, das sich f\xfcr komplexe Aufgaben eignet."},"gpt-4":{"description":"GPT-4 bietet ein gr\xf6\xdferes Kontextfenster, das in der Lage ist, l\xe4ngere Texteingaben zu verarbeiten, und eignet sich f\xfcr Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."},"gpt-4-0125-preview":{"description":"Das neueste GPT-4 Turbo-Modell verf\xfcgt \xfcber visuelle Funktionen. Jetzt k\xf6nnen visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterst\xfctzung f\xfcr multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich f\xfcr Anwendungen, die Echtzeitanpassungen erfordern."},"gpt-4-0613":{"description":"GPT-4 bietet ein gr\xf6\xdferes Kontextfenster, das in der Lage ist, l\xe4ngere Texteingaben zu verarbeiten, und eignet sich f\xfcr Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."},"gpt-4-1106-preview":{"description":"Das neueste GPT-4 Turbo-Modell verf\xfcgt \xfcber visuelle Funktionen. Jetzt k\xf6nnen visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterst\xfctzung f\xfcr multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich f\xfcr Anwendungen, die Echtzeitanpassungen erfordern."},"gpt-4-32k":{"description":"GPT-4 bietet ein gr\xf6\xdferes Kontextfenster, das in der Lage ist, l\xe4ngere Texteingaben zu verarbeiten, und eignet sich f\xfcr Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."},"gpt-4-32k-0613":{"description":"GPT-4 bietet ein gr\xf6\xdferes Kontextfenster, das in der Lage ist, l\xe4ngere Texteingaben zu verarbeiten, und eignet sich f\xfcr Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."},"gpt-4-turbo":{"description":"Das neueste GPT-4 Turbo-Modell verf\xfcgt \xfcber visuelle Funktionen. Jetzt k\xf6nnen visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterst\xfctzung f\xfcr multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich f\xfcr Anwendungen, die Echtzeitanpassungen erfordern."},"gpt-4-turbo-2024-04-09":{"description":"Das neueste GPT-4 Turbo-Modell verf\xfcgt \xfcber visuelle Funktionen. Jetzt k\xf6nnen visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterst\xfctzung f\xfcr multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich f\xfcr Anwendungen, die Echtzeitanpassungen erfordern."},"gpt-4-turbo-preview":{"description":"Das neueste GPT-4 Turbo-Modell verf\xfcgt \xfcber visuelle Funktionen. Jetzt k\xf6nnen visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterst\xfctzung f\xfcr multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich f\xfcr Anwendungen, die Echtzeitanpassungen erfordern."},"gpt-4-vision-preview":{"description":"Das neueste GPT-4 Turbo-Modell verf\xfcgt \xfcber visuelle Funktionen. Jetzt k\xf6nnen visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterst\xfctzung f\xfcr multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich f\xfcr Anwendungen, die Echtzeitanpassungen erfordern."},"gpt-4.1":{"description":"GPT-4.1 ist unser Flaggschiffmodell f\xfcr komplexe Aufgaben. Es eignet sich hervorragend zur L\xf6sung von Problemen \xfcber verschiedene Bereiche hinweg."},"gpt-4.1-mini":{"description":"GPT-4.1 mini bietet ein Gleichgewicht zwischen Intelligenz, Geschwindigkeit und Kosten, was es zu einem attraktiven Modell f\xfcr viele Anwendungsf\xe4lle macht."},"gpt-4.1-nano":{"description":"GPT-4.1 mini bietet ein Gleichgewicht zwischen Intelligenz, Geschwindigkeit und Kosten, was es zu einem attraktiven Modell f\xfcr viele Anwendungsf\xe4lle macht."},"gpt-4.5-preview":{"description":"GPT-4.5-preview ist das neueste Allzweckmodell, verf\xfcgt \xfcber fundiertes Weltwissen und ein verbessertes Verst\xe4ndnis der Nutzerintentionen und ist besonders leistungsf\xe4hig bei kreativen Aufgaben sowie in der Planung von Agenten. Das Wissen des Modells reicht bis Oktober 2023."},"gpt-4o":{"description":"ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gew\xe4hrleisten. Es kombiniert starke Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und eignet sich f\xfcr gro\xdfangelegte Anwendungsszenarien, einschlie\xdflich Kundenservice, Bildung und technische Unterst\xfctzung."},"gpt-4o-2024-05-13":{"description":"ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gew\xe4hrleisten. Es kombiniert starke Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und eignet sich f\xfcr gro\xdfangelegte Anwendungsszenarien, einschlie\xdflich Kundenservice, Bildung und technische Unterst\xfctzung."},"gpt-4o-2024-08-06":{"description":"ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gew\xe4hrleisten. Es kombiniert starke Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und eignet sich f\xfcr gro\xdfangelegte Anwendungsszenarien, einschlie\xdflich Kundenservice, Bildung und technische Unterst\xfctzung."},"gpt-4o-2024-11-20":{"description":"ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gew\xe4hrleisten. Es kombiniert starke Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und eignet sich f\xfcr gro\xdfangelegte Anwendungsbereiche, einschlie\xdflich Kundenservice, Bildung und technischen Support."},"gpt-4o-audio-preview":{"description":"GPT-4o Audio Preview Modell, unterst\xfctzt Audioeingabe und -ausgabe."},"gpt-4o-mini":{"description":"GPT-4o mini ist das neueste Modell von OpenAI, das nach GPT-4 Omni ver\xf6ffentlicht wurde und sowohl Text- als auch Bildinput unterst\xfctzt. Als ihr fortschrittlichstes kleines Modell ist es viel g\xfcnstiger als andere neueste Modelle und kostet \xfcber 60 % weniger als GPT-3.5 Turbo. Es beh\xe4lt die fortschrittliche Intelligenz bei und bietet gleichzeitig ein hervorragendes Preis-Leistungs-Verh\xe4ltnis. GPT-4o mini erzielte 82 % im MMLU-Test und rangiert derzeit in den Chat-Pr\xe4ferenzen \xfcber GPT-4."},"gpt-4o-mini-audio-preview":{"description":"GPT-4o mini Audio Modell, unterst\xfctzt Audioeingabe und -ausgabe."},"gpt-4o-mini-realtime-preview":{"description":"Echtzeitversion von GPT-4o-mini, unterst\xfctzt Audio- und Texteingabe sowie -ausgabe in Echtzeit."},"gpt-4o-mini-search-preview":{"description":"Die GPT-4o mini Suchvorschau ist ein speziell trainiertes Modell zur Interpretation und Ausf\xfchrung von Websuchanfragen, das die Chat Completions API verwendet. Neben den Token-Geb\xfchren fallen f\xfcr Websuchanfragen zus\xe4tzliche Geb\xfchren pro Tool-Aufruf an."},"gpt-4o-mini-transcribe":{"description":"GPT-4o Mini Transcribe ist ein Sprach-zu-Text-Modell, das GPT-4o zur Transkription von Audio verwendet. Im Vergleich zum urspr\xfcnglichen Whisper-Modell verbessert es die Wortfehlerrate sowie die Spracherkennung und Genauigkeit. Verwenden Sie es f\xfcr genauere Transkriptionen."},"gpt-4o-mini-tts":{"description":"GPT-4o mini TTS ist ein Text-to-Speech-Modell, das auf GPT-4o mini basiert und hochwertige Sprachgenerierung bei niedrigeren Kosten bietet."},"gpt-4o-realtime-preview":{"description":"Echtzeitversion von GPT-4o, unterst\xfctzt Audio- und Texteingabe sowie -ausgabe in Echtzeit."},"gpt-4o-realtime-preview-2024-10-01":{"description":"Echtzeitversion von GPT-4o, unterst\xfctzt Audio- und Texteingabe sowie -ausgabe in Echtzeit."},"gpt-4o-realtime-preview-2025-06-03":{"description":"Echtzeitversion von GPT-4o, unterst\xfctzt Echtzeit-Ein- und Ausgabe von Audio und Text."},"gpt-4o-search-preview":{"description":"Die GPT-4o Suchvorschau ist ein speziell trainiertes Modell zur Interpretation und Ausf\xfchrung von Websuchanfragen, das die Chat Completions API verwendet. Neben den Token-Geb\xfchren fallen f\xfcr Websuchanfragen zus\xe4tzliche Geb\xfchren pro Tool-Aufruf an."},"gpt-4o-transcribe":{"description":"GPT-4o Transcribe ist ein Sprach-zu-Text-Modell, das GPT-4o zur Transkription von Audio verwendet. Im Vergleich zum urspr\xfcnglichen Whisper-Modell verbessert es die Wortfehlerrate sowie die Spracherkennung und Genauigkeit. Verwenden Sie es f\xfcr genauere Transkriptionen."},"gpt-5":{"description":"Das beste Modell f\xfcr bereichs\xfcbergreifende Codierungs- und Agentenaufgaben. GPT-5 erzielt Durchbr\xfcche in Genauigkeit, Geschwindigkeit, Schlussfolgerungen, Kontextverst\xe4ndnis, strukturiertem Denken und Probleml\xf6sung."},"gpt-5-chat":{"description":"GPT-5 Chat ist eine Vorschauversion, die speziell f\xfcr Dialogszenarien optimiert wurde. Unterst\xfctzt Text- und Bildeingaben, gibt jedoch nur Text aus – ideal f\xfcr Chatbots und dialogbasierte KI-Anwendungen."},"gpt-5-chat-latest":{"description":"Das in ChatGPT verwendete GPT-5 Modell. Vereint starke Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten, ideal f\xfcr dialogorientierte Anwendungen."},"gpt-5-codex":{"description":"GPT-5 Codex ist eine f\xfcr Codex oder \xe4hnliche Umgebungen optimierte GPT-5-Version f\xfcr Agenten-Codierungsaufgaben."},"gpt-5-mini":{"description":"Eine schnellere und kosteneffizientere Version von GPT-5, geeignet f\xfcr klar definierte Aufgaben. Bietet schnellere Reaktionszeiten bei gleichbleibend hoher Ausgabequalit\xe4t."},"gpt-5-nano":{"description":"Die schnellste und kosteng\xfcnstigste Version von GPT-5. Besonders geeignet f\xfcr Anwendungen, die schnelle Reaktionen und Kostenbewusstsein erfordern."},"gpt-5-pro":{"description":"GPT-5 Pro nutzt mehr Rechenleistung f\xfcr tiefgreifendere \xdcberlegungen und liefert kontinuierlich bessere Antworten."},"gpt-5.1":{"description":"GPT-5.1 – Flaggschiffmodell, optimiert f\xfcr Programmier- und Agentenaufgaben, unterst\xfctzt konfigurierbare Rechenintensit\xe4t und l\xe4ngere Kontexte."},"gpt-5.1-chat-latest":{"description":"GPT-5.1 Chat: Eine Variante von GPT-5.1 f\xfcr ChatGPT, ideal f\xfcr Konversationsszenarien."},"gpt-5.1-codex":{"description":"GPT-5.1 Codex: Eine f\xfcr agentenbasierte Programmieraufgaben optimierte Version von GPT-5.1, geeignet f\xfcr komplexe Code- und Agenten-Workflows \xfcber die Responses API."},"gpt-5.1-codex-mini":{"description":"GPT-5.1 Codex mini: Eine kompaktere und kosteng\xfcnstigere Codex-Variante, optimiert f\xfcr agentenbasierte Programmieraufgaben."},"gpt-audio":{"description":"GPT Audio ist ein universelles Chatmodell f\xfcr Audioeingabe und -ausgabe, das Audio-I/O in der Chat Completions API unterst\xfctzt."},"gpt-image-1":{"description":"ChatGPT natives multimodales Bildgenerierungsmodell"},"gpt-image-1-mini":{"description":"Kosteng\xfcnstigere Version von GPT Image 1 mit nativer Unterst\xfctzung f\xfcr Text- und Bildeingaben sowie Bildausgaben."},"gpt-oss-120b":{"description":"Dieses Modell erfordert eine Zugangsanfrage. GPT-OSS-120B ist ein quelloffenes, gro\xdfskaliges Sprachmodell von OpenAI mit leistungsstarken Textgenerierungsf\xe4higkeiten."},"gpt-oss-20b":{"description":"Dieses Modell erfordert eine Zugangsanfrage. GPT-OSS-20B ist ein quelloffenes, mittelgro\xdfes Sprachmodell von OpenAI mit effizienter Textgenerierung."},"gpt-oss:120b":{"description":"GPT-OSS 120B ist ein von OpenAI ver\xf6ffentlichtes gro\xdfes Open-Source-Sprachmodell, das die MXFP4-Quantisierungstechnologie verwendet und als Flaggschiff-Modell gilt. Es erfordert den Betrieb auf Multi-GPU- oder Hochleistungs-Workstation-Umgebungen und bietet herausragende Leistungen bei komplexen Inferenzaufgaben, Codegenerierung und mehrsprachiger Verarbeitung. Es unterst\xfctzt fortgeschrittene Funktionsaufrufe und die Integration von Werkzeugen."},"gpt-oss:20b":{"description":"GPT-OSS 20B ist ein von OpenAI ver\xf6ffentlichtes Open-Source-Sprachmodell, das MXFP4-Quantisierung verwendet und f\xfcr den Einsatz auf High-End-Consumer-GPUs oder Apple Silicon Macs geeignet ist. Das Modell zeigt hervorragende Leistungen bei Dialoggenerierung, Codeerstellung und Inferenzaufgaben und unterst\xfctzt Funktionsaufrufe sowie Tool-Nutzung."},"gpt-realtime":{"description":"Universelles Echtzeitmodell, das Echtzeit-Text- und Audioeingabe/-ausgabe unterst\xfctzt und zudem Bildinput erm\xf6glicht."},"grok-2-image-1212":{"description":"Unser neuestes Bildgenerierungsmodell kann lebendige und realistische Bilder basierend auf Text-Prompts erzeugen. Es zeigt hervorragende Leistungen in den Bereichen Marketing, soziale Medien und Unterhaltung."},"grok-2-vision-1212":{"description":"Dieses Modell hat Verbesserungen in Bezug auf Genauigkeit, Befolgung von Anweisungen und Mehrsprachigkeit erfahren."},"grok-3":{"description":"Ein Flaggschiffmodell, spezialisiert auf Datenextraktion, Programmierung und Textzusammenfassung f\xfcr Unternehmensanwendungen, mit tiefgreifendem Wissen in den Bereichen Finanzen, Medizin, Recht und Wissenschaft."},"grok-3-mini":{"description":"Ein leichtgewichtiges Modell, das vor der Antwort nachdenkt. Es arbeitet schnell und intelligent, eignet sich f\xfcr logische Aufgaben ohne tiefgehendes Fachwissen und erm\xf6glicht die Nachverfolgung des urspr\xfcnglichen Denkprozesses."},"grok-4":{"description":"Unser neuestes und leistungsst\xe4rkstes Flaggschiffmodell, das in der Verarbeitung nat\xfcrlicher Sprache, mathematischen Berechnungen und logischem Denken herausragende Leistungen erbringt – ein perfekter Allrounder."},"grok-4-0709":{"description":"xAI\'s Grok 4 mit starker Schlussfolgerungsf\xe4higkeit."},"grok-4-1-fast-non-reasoning":{"description":"Modernes multimodales Modell, speziell optimiert f\xfcr die Nutzung leistungsstarker Agenten-Tools."},"grok-4-1-fast-reasoning":{"description":"Modernes multimodales Modell, speziell optimiert f\xfcr die Nutzung leistungsstarker Agenten-Tools."},"grok-4-fast-non-reasoning":{"description":"Wir freuen uns, Grok 4 Fast vorzustellen, unseren neuesten Fortschritt bei kosteneffizienten Inferenzmodellen."},"grok-4-fast-reasoning":{"description":"Wir freuen uns, Grok 4 Fast vorzustellen, unseren neuesten Fortschritt bei kosteneffizienten Inferenzmodellen."},"grok-code-fast-1":{"description":"Wir freuen uns, grok-code-fast-1 vorzustellen, ein schnelles und kosteneffizientes Inferenzmodell, das sich durch hervorragende Leistung bei der Agentencodierung auszeichnet."},"groq/compound":{"description":"Compound ist ein zusammengesetztes KI-System, das von mehreren bereits in GroqCloud unterst\xfctzten \xf6ffentlich verf\xfcgbaren Modellen getragen wird und intelligent sowie selektiv Werkzeuge zur Beantwortung von Nutzeranfragen einsetzt."},"groq/compound-mini":{"description":"Compound-mini ist ein zusammengesetztes KI-System, das von \xf6ffentlich verf\xfcgbaren Modellen unterst\xfctzt wird, die bereits in GroqCloud verf\xfcgbar sind, und intelligent sowie selektiv Werkzeuge zur Beantwortung von Nutzeranfragen einsetzt."},"gryphe/mythomax-l2-13b":{"description":"MythoMax l2 13B ist ein Sprachmodell, das Kreativit\xe4t und Intelligenz kombiniert und mehrere f\xfchrende Modelle integriert."},"hunyuan-a13b":{"description":"Hunyuan ist das erste hybride Schlussfolgerungsmodell, eine Weiterentwicklung von hunyuan-standard-256K mit insgesamt 80 Milliarden Parametern und 13 Milliarden aktivierten Parametern. Standardm\xe4\xdfig im langsamen Denkmodus, unterst\xfctzt es den Wechsel zwischen schnellem und langsamem Denkmodus \xfcber Parameter oder Anweisungen, wobei der Wechsel durch Voranstellen von query mit / no_think erfolgt. Die Gesamtleistung wurde gegen\xfcber der Vorg\xe4ngergeneration deutlich verbessert, insbesondere in Mathematik, Naturwissenschaften, Langtextverst\xe4ndnis und Agentenf\xe4higkeiten."},"hunyuan-code":{"description":"Das neueste Code-Generierungsmodell von Hunyuan, das auf einem Basismodell mit 200B hochwertigen Code-Daten trainiert wurde, hat ein halbes Jahr lang mit hochwertigen SFT-Daten trainiert, das Kontextfenster auf 8K erh\xf6ht und belegt in den automatischen Bewertungsmetriken f\xfcr die f\xfcnf gro\xdfen Programmiersprachen Spitzenpl\xe4tze; in den zehn Aspekten der umfassenden Codeaufgabenbewertung f\xfcr die f\xfcnf gro\xdfen Sprachen liegt die Leistung in der ersten Reihe."},"hunyuan-functioncall":{"description":"Das neueste MOE-Architektur-FunctionCall-Modell von Hunyuan, das mit hochwertigen FunctionCall-Daten trainiert wurde, hat ein Kontextfenster von 32K und f\xfchrt in mehreren Bewertungsmetriken."},"hunyuan-large":{"description":"Das Hunyuan-large Modell hat insgesamt etwa 389B Parameter, davon etwa 52B aktivierte Parameter, und ist das derzeit gr\xf6\xdfte und leistungsst\xe4rkste Open-Source MoE-Modell mit Transformer-Architektur in der Branche."},"hunyuan-large-longcontext":{"description":"Besonders gut geeignet f\xfcr lange Textaufgaben wie Dokumentenzusammenfassungen und Dokumentenfragen, verf\xfcgt es auch \xfcber die F\xe4higkeit, allgemeine Textgenerierungsaufgaben zu bearbeiten. Es zeigt hervorragende Leistungen bei der Analyse und Generierung von langen Texten und kann effektiv mit komplexen und detaillierten Anforderungen an die Verarbeitung von langen Inhalten umgehen."},"hunyuan-large-vision":{"description":"Dieses Modell eignet sich f\xfcr Szenarien mit Bild- und Textverst\xe4ndnis. Es basiert auf dem Hunyuan Large-Modell und ist ein gro\xdfes visuelles Sprachmodell, das beliebige Aufl\xf6sungen und mehrere Bilder plus Texteingaben unterst\xfctzt und Textinhalte generiert. Der Fokus liegt auf Aufgaben im Bereich Bild-Text-Verst\xe4ndnis mit deutlichen Verbesserungen in mehrsprachigen Bild-Text-Verst\xe4ndnisf\xe4higkeiten."},"hunyuan-lite":{"description":"Aufger\xfcstet auf eine MOE-Struktur mit einem Kontextfenster von 256k, f\xfchrt es in mehreren Bewertungssets in NLP, Code, Mathematik und Industrie zahlreiche Open-Source-Modelle an."},"hunyuan-lite-vision":{"description":"Das neueste 7B multimodale Modell von Hunyuan, mit einem Kontextfenster von 32K, unterst\xfctzt multimodale Dialoge in Chinesisch und Englisch, Objekterkennung in Bildern, Dokumenten- und Tabellenverst\xe4ndnis sowie multimodale Mathematik und \xfcbertrifft in mehreren Dimensionen die Bewertungskennzahlen von 7B Wettbewerbsmodellen."},"hunyuan-pro":{"description":"Ein MOE-32K-Modell f\xfcr lange Texte mit einer Billion Parametern. Es erreicht in verschiedenen Benchmarks ein absolut f\xfchrendes Niveau, hat komplexe Anweisungen und Schlussfolgerungen, verf\xfcgt \xfcber komplexe mathematische F\xe4higkeiten und unterst\xfctzt Funktionsaufrufe, mit Schwerpunkt auf Optimierung in den Bereichen mehrsprachige \xdcbersetzung, Finanzrecht und Medizin."},"hunyuan-role":{"description":"Das neueste Rollenspielmodell von Hunyuan, das auf dem offiziellen feinabgestimmten Training von Hunyuan basiert, wurde mit einem Datensatz f\xfcr Rollenspiel-Szenarien weiter trainiert und bietet in Rollenspiel-Szenarien bessere Grundeffekte."},"hunyuan-standard":{"description":"Verwendet eine verbesserte Routing-Strategie und mildert gleichzeitig die Probleme der Lastenverteilung und Expertenkonvergenz. Bei langen Texten erreicht der Needle-in-a-Haystack-Indikator 99,9%. MOE-32K bietet ein besseres Preis-Leistungs-Verh\xe4ltnis und erm\xf6glicht die Verarbeitung von langen Texteingaben bei ausgewogenem Effekt und Preis."},"hunyuan-standard-256K":{"description":"Verwendet eine verbesserte Routing-Strategie und mildert gleichzeitig die Probleme der Lastenverteilung und Expertenkonvergenz. Bei langen Texten erreicht der Needle-in-a-Haystack-Indikator 99,9%. MOE-256K bricht in L\xe4nge und Effektivit\xe4t weiter durch und erweitert die eingabef\xe4hige L\xe4nge erheblich."},"hunyuan-standard-vision":{"description":"Das neueste multimodale Modell von Hunyuan, das mehrsprachige Antworten unterst\xfctzt und sowohl in Chinesisch als auch in Englisch ausgewogen ist."},"hunyuan-t1-20250321":{"description":"Umfassende Entwicklung der Modellf\xe4higkeiten in Geistes- und Naturwissenschaften, starke F\xe4higkeit zur Erfassung langer Textinformationen. Unterst\xfctzt die L\xf6sung von wissenschaftlichen Problemen in verschiedenen Schwierigkeitsgraden, einschlie\xdflich Mathematik, logischem Denken, Wissenschaft und Code."},"hunyuan-t1-20250403":{"description":"Verbesserung der Codegenerierungsf\xe4higkeiten auf Projektebene; Steigerung der Qualit\xe4t von Textgenerierung und Schreibstil; Verbesserung des Verst\xe4ndnisses von Themen in mehrstufigen Dialogen, Befehlsbefolgung und Wortverst\xe4ndnis; Optimierung von Ausgaben mit gemischten traditionellen und vereinfachten chinesischen Schriftzeichen sowie gemischten chinesisch-englischen Texten."},"hunyuan-t1-20250529":{"description":"Optimiert f\xfcr Textkreation und Aufsatzschreiben, verbessert die F\xe4higkeiten in Frontend-Programmierung, Mathematik und logischem Denken sowie die Befolgung von Anweisungen."},"hunyuan-t1-20250711":{"description":"Erhebliche Verbesserungen bei anspruchsvoller Mathematik, Logik und Programmierf\xe4higkeiten, Optimierung der Modellstabilit\xe4t und Steigerung der Leistungsf\xe4higkeit bei langen Texten."},"hunyuan-t1-latest":{"description":"Erhebliche Verbesserung der F\xe4higkeiten des Hauptmodells im langsamen Denkmodus bei anspruchsvoller Mathematik, komplexen Schlussfolgerungen, anspruchsvollem Code, Befolgung von Anweisungen und Textkreation."},"hunyuan-t1-vision-20250619":{"description":"Die neueste Version des hunyuan t1-vision multimodalen tiefen Denkmodells unterst\xfctzt native multimodale Chain-of-Thought-Mechanismen und bietet im Vergleich zur vorherigen Standardversion umfassende Verbesserungen."},"hunyuan-t1-vision-20250916":{"description":"Die neueste Version des Hunyuan t1-vision Modells f\xfcr visuelles, tiefes Denken bietet im Vergleich zur Vorg\xe4ngerversion umfassende Verbesserungen bei allgemeinen Bild-Text-Fragen, visueller Lokalisierung, OCR, Diagrammverarbeitung, Aufgabenl\xf6sung per Foto und kreativer Bildinterpretation. Die F\xe4higkeiten in Englisch und kleineren Sprachen wurden deutlich optimiert."},"hunyuan-turbo":{"description":"Die Vorschauversion des neuen gro\xdfen Sprachmodells von Hunyuan verwendet eine neuartige hybride Expertenmodellstruktur (MoE) und bietet im Vergleich zu Hunyuan-Pro eine schnellere Inferenz und bessere Leistung."},"hunyuan-turbo-20241223":{"description":"Diese Version optimiert: Datenanweisungs-Skalierung, erhebliche Verbesserung der allgemeinen Generalisierungsf\xe4higkeit des Modells; erhebliche Verbesserung der mathematischen, programmierbaren und logischen Denkf\xe4higkeiten; Optimierung der F\xe4higkeiten im Textverst\xe4ndnis und der Wortverst\xe4ndnisf\xe4higkeiten; Optimierung der Qualit\xe4t der Inhaltserzeugung in der Texterstellung."},"hunyuan-turbo-latest":{"description":"Allgemeine Optimierung der Benutzererfahrung, einschlie\xdflich NLP-Verst\xe4ndnis, Texterstellung, Smalltalk, Wissensfragen, \xdcbersetzung, Fachgebieten usw.; Verbesserung der Menschlichkeit, Optimierung der emotionalen Intelligenz des Modells; Verbesserung der F\xe4higkeit des Modells, bei unklaren Absichten aktiv Klarheit zu schaffen; Verbesserung der Bearbeitungsf\xe4higkeit von Fragen zur Wort- und Satzanalyse; Verbesserung der Qualit\xe4t und Interaktivit\xe4t der Kreation; Verbesserung der Mehrfachinteraktionserfahrung."},"hunyuan-turbo-vision":{"description":"Das neue Flaggschiff-Modell der visuellen Sprache von Hunyuan, das eine brandneue Struktur des gemischten Expertenmodells (MoE) verwendet, bietet umfassende Verbesserungen in den F\xe4higkeiten zur grundlegenden Erkennung, Inhaltserstellung, Wissensfragen und Analyse sowie Schlussfolgerungen im Vergleich zum vorherigen Modell."},"hunyuan-turbos-20250313":{"description":"Vereinheitlichung des Stils bei mathematischen L\xf6sungswegen und Verst\xe4rkung der mehrstufigen mathematischen Frage-Antwort-Interaktion. Optimierung des Antwortstils bei Textkreationen, Entfernung von KI-typischen Merkmalen und Steigerung der literarischen Ausdruckskraft."},"hunyuan-turbos-20250416":{"description":"Upgrade der vortrainierten Basis zur St\xe4rkung des Befehlsverst\xe4ndnisses und der Befehlsbefolgung; Verbesserung der naturwissenschaftlichen F\xe4higkeiten in Mathematik, Programmierung, Logik und Wissenschaft w\xe4hrend der Feinabstimmungsphase; Steigerung der Qualit\xe4t in literarischer Kreativit\xe4t, Textverst\xe4ndnis, \xdcbersetzungsgenauigkeit und Wissensfragen; Verst\xe4rkung der Agentenf\xe4higkeiten in verschiedenen Bereichen mit Schwerpunkt auf dem Verst\xe4ndnis mehrstufiger Dialoge."},"hunyuan-turbos-20250604":{"description":"Upgrade der vortrainierten Basis, verbessert Schreib- und Leseverst\xe4ndnisf\xe4higkeiten, steigert deutlich die Programmier- und naturwissenschaftlichen Kompetenzen und verbessert kontinuierlich die Befolgung komplexer Anweisungen."},"hunyuan-turbos-20250926":{"description":"Qualit\xe4tsverbesserung der Pretraining-Basisdaten. Optimierung der Trainingsstrategie in der Posttrain-Phase, kontinuierliche Verbesserung der Agenten-, Englisch- und kleinen Sprachf\xe4higkeiten, Befolgung von Anweisungen, Code- und naturwissenschaftlichen F\xe4higkeiten."},"hunyuan-turbos-latest":{"description":"hunyuan-TurboS ist die neueste Version des Hunyuan-Flaggschiffmodells, das \xfcber verbesserte Denkf\xe4higkeiten und ein besseres Nutzungserlebnis verf\xfcgt."},"hunyuan-turbos-longtext-128k-20250325":{"description":"Experte f\xfcr die Verarbeitung von langen Textaufgaben wie Dokumentenzusammenfassungen und Dokumentenfragen, mit der F\xe4higkeit, allgemeine Textgenerierungsaufgaben zu bew\xe4ltigen. Es zeigt hervorragende Leistungen bei der Analyse und Generierung von langen Texten und kann komplexe und detaillierte Anforderungen an die Verarbeitung langer Inhalte effektiv bew\xe4ltigen."},"hunyuan-turbos-role-plus":{"description":"Die neueste Version des Hunyuan-Rollenspielsmodells, feinabgestimmt und trainiert von Hunyuan, basiert auf dem Hunyuan-Modell und wurde mit Datens\xe4tzen f\xfcr Rollenspielszenarien weiter trainiert, um in Rollenspielszenarien bessere Grundleistungen zu erzielen."},"hunyuan-turbos-vision":{"description":"Dieses Modell eignet sich f\xfcr Szenarien mit Bild- und Textverst\xe4ndnis und basiert auf dem neuesten hunyuan turbos. Es ist ein neues Flaggschiff-Visuell-Sprachmodell, das sich auf Aufgaben des Bild-Text-Verstehens konzentriert, einschlie\xdflich bildbasierter Entit\xe4tenerkennung, Wissensfragen, Textkreation und fotografiebasierter Probleml\xf6sung, mit umfassenden Verbesserungen gegen\xfcber der Vorg\xe4ngerversion."},"hunyuan-turbos-vision-20250619":{"description":"Die neueste Version des hunyuan turbos-vision Flaggschiff-Visuell-Sprachmodells bietet umfassende Verbesserungen bei Aufgaben des Bild-Text-Verstehens, einschlie\xdflich bildbasierter Entit\xe4tenerkennung, Wissensfragen, Textkreation und fotografiebasierter Probleml\xf6sung, im Vergleich zur vorherigen Standardversion."},"hunyuan-vision":{"description":"Das neueste multimodale Modell von Hunyuan unterst\xfctzt die Eingabe von Bildern und Text zur Generierung von Textinhalten."},"image-01":{"description":"Neues Bildgenerierungsmodell mit feiner Bilddarstellung, unterst\xfctzt Text-zu-Bild und Bild-zu-Bild."},"image-01-live":{"description":"Bildgenerierungsmodell mit feiner Bilddarstellung, unterst\xfctzt Text-zu-Bild und Stil-Einstellungen."},"imagen-4.0-fast-generate-001":{"description":"Imagen – Text-zu-Bild-Modellreihe der 4. Generation (Fast-Version)"},"imagen-4.0-generate-001":{"description":"Imagen, Text-zu-Bild-Modellreihe der 4. Generation"},"imagen-4.0-generate-preview-06-06":{"description":"Vierte Generation der Imagen-Modelle zur Text-zu-Bild-Generierung."},"imagen-4.0-ultra-generate-001":{"description":"Imagen, Text-zu-Bild-Modell der 4. Generation (Ultra-Version)"},"imagen-4.0-ultra-generate-preview-06-06":{"description":"Ultra-Version der vierten Generation der Imagen-Modelle zur Text-zu-Bild-Generierung."},"inception/mercury-coder-small":{"description":"Mercury Coder Small ist ideal f\xfcr Codegenerierung, Debugging und Refactoring-Aufgaben mit minimaler Latenz."},"inclusionAI/Ling-1T":{"description":"Ling-1T ist das erste Flaggschiffmodell der „Ling 2.0“-Reihe ohne Denkfunktion (non-thinking), mit insgesamt einer Billion Parametern und etwa 50 Milliarden aktiven Parametern pro Token. Es basiert auf der Ling 2.0-Architektur und zielt darauf ab, die Grenzen effizienter Schlussfolgerung und skalierbarer Kognition zu durchbrechen. Ling-1T-base wurde mit \xfcber 20 Billionen hochwertigen, reasoning-intensiven Tokens trainiert."},"inclusionAI/Ling-flash-2.0":{"description":"Ling-flash-2.0 ist das dritte Modell der Ling 2.0 Architekturserie, ver\xf6ffentlicht vom Ant Group Bailing Team. Es handelt sich um ein Mixture-of-Experts (MoE)-Modell mit insgesamt 100 Milliarden Parametern, wobei pro Token nur 6,1 Milliarden Parameter aktiviert werden (ohne Wortvektoren 4,8 Milliarden). Als leichtgewichtige Konfiguration zeigt Ling-flash-2.0 in mehreren renommierten Benchmarks Leistungen, die mit 40-Milliarden-Dense-Modellen und gr\xf6\xdferen MoE-Modellen vergleichbar oder \xfcberlegen sind. Das Modell zielt darauf ab, durch exzellentes Architekturdesign und Trainingsstrategien effiziente Wege zu erforschen, um die g\xe4ngige Annahme „gro\xdfes Modell = viele Parameter“ zu hinterfragen."},"inclusionAI/Ling-mini-2.0":{"description":"Ling-mini-2.0 ist ein kleines, leistungsstarkes Sprachmodell basierend auf der MoE-Architektur. Es verf\xfcgt \xfcber 16 Milliarden Gesamtparameter, aktiviert jedoch pro Token nur 1,4 Milliarden (ohne Einbettungen 789 Millionen), was eine sehr hohe Generierungsgeschwindigkeit erm\xf6glicht. Dank effizientem MoE-Design und gro\xdfem, hochwertigem Trainingsdatensatz zeigt Ling-mini-2.0 trotz der geringen aktivierten Parameter eine Spitzenleistung, die mit dichten LLMs unter 10 Milliarden und gr\xf6\xdferen MoE-Modellen in nachgelagerten Aufgaben vergleichbar ist."},"inclusionAI/Ring-1T":{"description":"Ring-1T ist ein Open-Source-Modell f\xfcr kognitives Denken im Billionen-Parameter-Ma\xdfstab, ver\xf6ffentlicht vom Bailing-Team. Es basiert auf der Ling 2.0-Architektur und dem Ling-1T-base-Modell, mit insgesamt einer Billion Parametern und 50 Milliarden aktiven Parametern. Es unterst\xfctzt ein Kontextfenster von bis zu 128K und wurde durch gro\xdf angelegtes, verifizierbares, belohnungsbasiertes Reinforcement Learning optimiert."},"inclusionAI/Ring-flash-2.0":{"description":"Ring-flash-2.0 ist ein hochleistungsf\xe4higes Denkmodell, das auf Ling-flash-2.0-base tief optimiert wurde. Es verwendet eine Mixture-of-Experts (MoE)-Architektur mit insgesamt 100 Milliarden Parametern, aktiviert jedoch bei jeder Inferenz nur 6,1 Milliarden Parameter. Durch den innovativen Icepop-Algorithmus l\xf6st es die Instabilit\xe4tsprobleme gro\xdfer MoE-Modelle im Reinforcement Learning (RL) Training und verbessert kontinuierlich seine komplexen Inferenzf\xe4higkeiten \xfcber lange Trainingszyklen. Ring-flash-2.0 erzielt bedeutende Durchbr\xfcche in anspruchsvollen Benchmarks wie Mathematikwettbewerben, Codegenerierung und logischem Schlie\xdfen. Seine Leistung \xfcbertrifft nicht nur dichte Spitzenmodelle unter 40 Milliarden Parametern, sondern ist auch vergleichbar mit gr\xf6\xdferen Open-Source-MoE-Modellen und propriet\xe4ren Hochleistungs-Denkmodellen. Obwohl es auf komplexe Inferenz spezialisiert ist, zeigt es auch bei kreativen Schreibaufgaben hervorragende Ergebnisse. Dank seiner effizienten Architektur bietet Ring-flash-2.0 starke Leistung bei gleichzeitig hoher Inferenzgeschwindigkeit und senkt deutlich die Bereitstellungskosten in hochparallelen Szenarien."},"internlm/internlm2_5-7b-chat":{"description":"InternLM2.5 bietet intelligente Dialogl\xf6sungen in mehreren Szenarien."},"internlm2.5-latest":{"description":"Unsere neueste Modellreihe mit herausragender Schlussfolgerungsleistung, die eine Kontextl\xe4nge von 1M unterst\xfctzt und \xfcber verbesserte Anweisungsbefolgung und Toolaufrufm\xf6glichkeiten verf\xfcgt."},"internlm3-latest":{"description":"Unsere neueste Modellreihe bietet herausragende Inferenzleistungen und f\xfchrt die Open-Source-Modelle in ihrer Gewichtsklasse an. Standardm\xe4\xdfig verweist sie auf unser neuestes ver\xf6ffentlichtes InternLM3-Modell."},"internvl2.5-38b-mpo":{"description":"InternVL2.5 38B MPO, ein multimodales vortrainiertes Modell, das komplexe Bild-Text-Inferenzaufgaben unterst\xfctzt."},"internvl2.5-latest":{"description":"Die von uns weiterhin unterst\xfctzte Version InternVL2.5 bietet hervorragende und stabile Leistungen. Standardm\xe4\xdfig verweist es auf unser neuestes ver\xf6ffentlichtes InternVL2.5-Modell, derzeit auf internvl2.5-78b."},"internvl3-14b":{"description":"InternVL3 14B, ein mittelgro\xdfes multimodales Modell mit ausgewogenem Verh\xe4ltnis zwischen Leistung und Kosten."},"internvl3-1b":{"description":"InternVL3 1B, ein leichtgewichtiges multimodales Modell, geeignet f\xfcr den Einsatz in ressourcenbeschr\xe4nkten Umgebungen."},"internvl3-38b":{"description":"InternVL3 38B, ein gro\xdfskaliges Open-Source-Multimodalmodell, geeignet f\xfcr hochpr\xe4zises Bild-Text-Verst\xe4ndnis."},"internvl3-latest":{"description":"Unser neuestes multimodales Gro\xdfmodell bietet verbesserte F\xe4higkeiten im Verst\xe4ndnis von Text und Bildern sowie im langfristigen Verst\xe4ndnis von Bildern und erreicht eine Leistung, die mit f\xfchrenden propriet\xe4ren Modellen vergleichbar ist. Standardm\xe4\xdfig verweist es auf unser neuestes ver\xf6ffentlichtes InternVL-Modell, derzeit auf internvl3-78b."},"irag-1.0":{"description":"ERNIE iRAG, ein bildgest\xfctztes Retrieval-Augmented-Generation-Modell mit Unterst\xfctzung f\xfcr Bildsuche, Bild-Text-Retrieval und Inhaltserzeugung."},"jamba-large":{"description":"Unser leistungsst\xe4rkstes und fortschrittlichstes Modell, das speziell f\xfcr die Bew\xe4ltigung komplexer Aufgaben auf Unternehmensebene entwickelt wurde und herausragende Leistung bietet."},"jamba-mini":{"description":"Das effizienteste Modell seiner Klasse, das Geschwindigkeit und Qualit\xe4t vereint und eine kompakte Bauweise aufweist."},"jina-deepsearch-v1":{"description":"Die Tiefensuche kombiniert Websuche, Lesen und Schlussfolgern und erm\xf6glicht umfassende Untersuchungen. Sie k\xf6nnen es als einen Agenten betrachten, der Ihre Forschungsaufgaben \xfcbernimmt – er f\xfchrt eine umfassende Suche durch und iteriert mehrfach, bevor er eine Antwort gibt. Dieser Prozess umfasst kontinuierliche Forschung, Schlussfolgerungen und die L\xf6sung von Problemen aus verschiedenen Perspektiven. Dies unterscheidet sich grundlegend von den Standard-Gro\xdfmodellen, die Antworten direkt aus vortrainierten Daten generieren, sowie von traditionellen RAG-Systemen, die auf einmaligen Oberfl\xe4chensuchen basieren."},"kimi-k2":{"description":"Kimi-K2 ist ein von Moonshot AI entwickeltes MoE-Basis-Modell mit herausragenden Code- und Agentenf\xe4higkeiten, insgesamt 1 Billion Parameter und 32 Milliarden aktivierten Parametern. In Benchmark-Tests zu allgemeinem Wissen, Programmierung, Mathematik und Agentenaufgaben \xfcbertrifft das K2-Modell andere f\xfchrende Open-Source-Modelle."},"kimi-k2-0711-preview":{"description":"kimi-k2 ist ein MoE-Architektur-Basis-Modell mit au\xdfergew\xf6hnlichen F\xe4higkeiten in Code und Agentenfunktionen, mit insgesamt 1 Billion Parametern und 32 Milliarden aktiven Parametern. In Benchmark-Tests zu allgemeinem Wissen, Programmierung, Mathematik und Agenten \xfcbertrifft das K2-Modell andere f\xfchrende Open-Source-Modelle."},"kimi-k2-0905-preview":{"description":"Das Modell kimi-k2-0905-preview hat eine Kontextl\xe4nge von 256k, verf\xfcgt \xfcber st\xe4rkere Agentic-Coding-F\xe4higkeiten, eine herausragendere \xc4sthetik und Praktikabilit\xe4t von Frontend-Code sowie ein besseres Kontextverst\xe4ndnis."},"kimi-k2-instruct":{"description":"Kimi K2 Instruct, das offizielle Inferenzmodell von Kimi mit Unterst\xfctzung f\xfcr Langkontext, Code, QA und mehr."},"kimi-k2-turbo-preview":{"description":"kimi-k2 ist ein Basis-Modell mit MoE-Architektur und besonders starken F\xe4higkeiten im Bereich Code und Agenten. Es verf\xfcgt \xfcber insgesamt 1T Parameter und 32B aktivierte Parameter. In Benchmark-Tests der wichtigsten Kategorien – allgemeines Wissens-Reasoning, Programmierung, Mathematik und Agenten – \xfcbertrifft das K2-Modell die Leistung anderer g\xe4ngiger Open‑Source‑Modelle."},"kimi-k2:1t":{"description":"Kimi K2 ist ein von Moon\'s Dark Side AI entwickeltes gro\xdfes gemischtes Expertenmodell (MoE) mit insgesamt 1 Billion Parametern und 32 Milliarden aktivierten Parametern pro Vorw\xe4rtsdurchlauf. Es ist auf Agentenf\xe4higkeiten optimiert, einschlie\xdflich fortgeschrittener Werkzeugnutzung, Schlussfolgerungen und Code-Synthese."},"kimi-latest":{"description":"Das Kimi intelligente Assistenzprodukt verwendet das neueste Kimi Gro\xdfmodell, das m\xf6glicherweise noch instabile Funktionen enth\xe4lt. Es unterst\xfctzt die Bildverarbeitung und w\xe4hlt automatisch das Abrechnungsmodell 8k/32k/128k basierend auf der L\xe4nge des angeforderten Kontexts aus."},"kimi-thinking-preview":{"description":"Das kimi-thinking-preview Modell von Moon’s Dark Side ist ein multimodales Denkmodell mit F\xe4higkeiten zu multimodalem und allgemeinem logischem Denken. Es ist spezialisiert auf tiefgehende Schlussfolgerungen und hilft dabei, komplexere und schwierigere Aufgaben zu l\xf6sen."},"learnlm-1.5-pro-experimental":{"description":"LearnLM ist ein experimentelles, aufgabenorientiertes Sprachmodell, das darauf trainiert wurde, den Prinzipien der Lernwissenschaft zu entsprechen und in Lehr- und Lernszenarien systematische Anweisungen zu befolgen, als Expertenmentor zu fungieren usw."},"learnlm-2.0-flash-experimental":{"description":"LearnLM ist ein experimentelles, aufgabenbezogenes Sprachmodell, das darauf trainiert wurde, den Prinzipien der Lernwissenschaft zu entsprechen und in Lehr- und Lernszenarien systematische Anweisungen zu befolgen, als Expertenmentor zu fungieren usw."},"lite":{"description":"Spark Lite ist ein leichtgewichtiges gro\xdfes Sprachmodell mit extrem niedriger Latenz und effizienter Verarbeitung, das vollst\xe4ndig kostenlos und offen ist und Echtzeitsuchfunktionen unterst\xfctzt. Seine schnelle Reaktionsf\xe4higkeit macht es besonders geeignet f\xfcr Inferenzanwendungen und Modellanpassungen auf Ger\xe4ten mit geringer Rechenleistung und bietet den Nutzern ein hervorragendes Kosten-Nutzen-Verh\xe4ltnis sowie ein intelligentes Erlebnis, insbesondere in den Bereichen Wissensabfragen, Inhaltserstellung und Suchszenarien."},"llama-3.1-70b-versatile":{"description":"Llama 3.1 70B bietet leistungsstarke KI-Schlussfolgerungsf\xe4higkeiten, die f\xfcr komplexe Anwendungen geeignet sind und eine hohe Rechenverarbeitung bei gleichzeitiger Effizienz und Genauigkeit unterst\xfctzen."},"llama-3.1-8b-instant":{"description":"Llama 3.1 8B ist ein leistungsstarkes Modell, das schnelle Textgenerierungsf\xe4higkeiten bietet und sich hervorragend f\xfcr Anwendungen eignet, die gro\xdfe Effizienz und Kosteneffektivit\xe4t erfordern."},"llama-3.1-instruct":{"description":"Das Llama 3.1 Instruktionstuning-Modell ist f\xfcr Dialogszenarien optimiert und \xfcbertrifft in g\xe4ngigen Branchenbenchmarks viele bestehende Open-Source-Chatmodelle."},"llama-3.2-11b-vision-instruct":{"description":"\xdcberlegene Bildverarbeitungsf\xe4higkeiten auf hochaufl\xf6senden Bildern, geeignet f\xfcr visuelle Verst\xe4ndnisanwendungen."},"llama-3.2-11b-vision-preview":{"description":"Llama 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellen Fragen und Antworten und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"llama-3.2-90b-vision-instruct":{"description":"Erweiterte Bildverarbeitungsf\xe4higkeiten f\xfcr visuelle Verst\xe4ndnisagentenanwendungen."},"llama-3.2-90b-vision-preview":{"description":"Llama 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellen Fragen und Antworten und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"llama-3.2-vision-instruct":{"description":"Das Llama 3.2-Vision-Instruct-Modell ist optimiert f\xfcr visuelle Erkennung, Bildschlussfolgerungen, Bildbeschreibungen und das Beantworten von allgemeinen Fragen, die mit Bildern zusammenh\xe4ngen."},"llama-3.3-70b":{"description":"Llama 3.3 70B: Ein mittelgro\xdfes Llama-Modell, das eine ausgewogene Kombination aus logischem Denken und hoher Verarbeitungskapazit\xe4t bietet."},"llama-3.3-70b-versatile":{"description":"Das Meta Llama 3.3 ist ein mehrsprachiges, gro\xdfes Sprachmodell (LLM), das aus einem vortrainierten und anweisungsorientierten generativen Modell mit 70B (Text-Eingabe/Text-Ausgabe) besteht. Das anweisungsorientierte Modell von Llama 3.3 ist f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertrifft viele verf\xfcgbare Open-Source- und Closed-Source-Chat-Modelle bei g\xe4ngigen Branchenbenchmarks."},"llama-3.3-instruct":{"description":"Das Llama 3.3 Instruct-Modell ist f\xfcr Dialogszenarien optimiert und \xfcbertrifft in g\xe4ngigen Branchenbenchmarks viele bestehende Open-Source-Chatmodelle."},"llama-4-scout-17b-16e-instruct":{"description":"Llama 4 Scout: Ein leistungsstarkes Modell der Llama-Serie, optimiert f\xfcr Szenarien mit hoher Verarbeitungsgeschwindigkeit und niedriger Latenz."},"llama3-70b-8192":{"description":"Meta Llama 3 70B bietet unvergleichliche F\xe4higkeiten zur Verarbeitung von Komplexit\xe4t und ist ma\xdfgeschneidert f\xfcr Projekte mit hohen Anforderungen."},"llama3-8b-8192":{"description":"Meta Llama 3 8B bietet hervorragende Schlussfolgerungsf\xe4higkeiten und eignet sich f\xfcr eine Vielzahl von Anwendungsanforderungen."},"llama3-groq-70b-8192-tool-use-preview":{"description":"Llama 3 Groq 70B Tool Use bietet leistungsstarke Werkzeugaufruf-F\xe4higkeiten und unterst\xfctzt die effiziente Verarbeitung komplexer Aufgaben."},"llama3-groq-8b-8192-tool-use-preview":{"description":"Llama 3 Groq 8B Tool Use ist ein Modell, das f\xfcr die effiziente Nutzung von Werkzeugen optimiert ist und schnelle parallele Berechnungen unterst\xfctzt."},"llama3.1":{"description":"Llama 3.1 ist ein f\xfchrendes Modell von Meta, das bis zu 405B Parameter unterst\xfctzt und in den Bereichen komplexe Dialoge, mehrsprachige \xdcbersetzungen und Datenanalysen eingesetzt werden kann."},"llama3.1-8b":{"description":"Llama 3.1 8B: Eine kompakte, latenzarme Variante des Llama-Modells, ideal f\xfcr leichte Online-Inferenz- und Interaktionsszenarien."},"llama3.1:405b":{"description":"Llama 3.1 ist ein f\xfchrendes Modell von Meta, das bis zu 405B Parameter unterst\xfctzt und in den Bereichen komplexe Dialoge, mehrsprachige \xdcbersetzungen und Datenanalysen eingesetzt werden kann."},"llama3.1:70b":{"description":"Llama 3.1 ist ein f\xfchrendes Modell von Meta, das bis zu 405B Parameter unterst\xfctzt und in den Bereichen komplexe Dialoge, mehrsprachige \xdcbersetzungen und Datenanalysen eingesetzt werden kann."},"llava":{"description":"LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und f\xfcr starke visuelle und sprachliche Verst\xe4ndnisse sorgt."},"llava-v1.5-7b-4096-preview":{"description":"LLaVA 1.5 7B bietet integrierte visuelle Verarbeitungsf\xe4higkeiten, um komplexe Ausgaben aus visuellen Informationen zu generieren."},"llava:13b":{"description":"LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und f\xfcr starke visuelle und sprachliche Verst\xe4ndnisse sorgt."},"llava:34b":{"description":"LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und f\xfcr starke visuelle und sprachliche Verst\xe4ndnisse sorgt."},"magistral-medium-latest":{"description":"Magistral Medium 1.2 ist ein fortschrittliches Inferenzmodell mit visueller Unterst\xfctzung, das von Mistral AI im September 2025 ver\xf6ffentlicht wurde."},"magistral-small-2509":{"description":"Magistral Small 1.2 ist ein Open-Source-Kleinmodell f\xfcr Inferenz mit visueller Unterst\xfctzung, das von Mistral AI im September 2025 ver\xf6ffentlicht wurde."},"mathstral":{"description":"MathΣtral ist f\xfcr wissenschaftliche Forschung und mathematische Schlussfolgerungen konzipiert und bietet effektive Rechenf\xe4higkeiten und Ergebnisinterpretationen."},"max-32k":{"description":"Spark Max 32K bietet eine gro\xdfe Kontextverarbeitungsf\xe4higkeit mit verbesserter Kontextverst\xe4ndnis und logischer Schlussfolgerungsf\xe4higkeit und unterst\xfctzt Texteingaben von bis zu 32K Tokens, was es ideal f\xfcr das Lesen langer Dokumente und private Wissensabfragen macht."},"megrez-3b-instruct":{"description":"Megrez 3B Instruct ist ein effizientes Modell mit geringer Parameteranzahl, entwickelt von Wuwen Xinqiong."},"meituan/longcat-flash-chat":{"description":"Ein von Meituan entwickeltes Open-Source-Basismodell, das speziell f\xfcr dialogorientierte Interaktionen und agentenbasierte Aufgaben optimiert wurde und sich besonders bei Werkzeugaufrufen und komplexen mehrstufigen Dialogszenarien auszeichnet."},"meta-llama-3-70b-instruct":{"description":"Ein leistungsstarkes Modell mit 70 Milliarden Parametern, das in den Bereichen Schlussfolgerungen, Programmierung und breiten Sprachanwendungen herausragt."},"meta-llama-3-8b-instruct":{"description":"Ein vielseitiges Modell mit 8 Milliarden Parametern, das f\xfcr Dialog- und Textgenerierungsaufgaben optimiert ist."},"meta-llama-3.1-405b-instruct":{"description":"Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertreffen viele der verf\xfcgbaren Open-Source- und geschlossenen Chat-Modelle in g\xe4ngigen Branchenbenchmarks."},"meta-llama-3.1-70b-instruct":{"description":"Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertreffen viele der verf\xfcgbaren Open-Source- und geschlossenen Chat-Modelle in g\xe4ngigen Branchenbenchmarks."},"meta-llama-3.1-8b-instruct":{"description":"Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertreffen viele der verf\xfcgbaren Open-Source- und geschlossenen Chat-Modelle in g\xe4ngigen Branchenbenchmarks."},"meta-llama/Llama-2-13b-chat-hf":{"description":"LLaMA-2 Chat (13B) bietet hervorragende Sprachverarbeitungsf\xe4higkeiten und ein ausgezeichnetes Interaktionserlebnis."},"meta-llama/Llama-2-70b-hf":{"description":"LLaMA-2 bietet hervorragende Sprachverarbeitungsf\xe4higkeiten und ein gro\xdfartiges Interaktionserlebnis."},"meta-llama/Llama-3-70b-chat-hf":{"description":"LLaMA-3 Chat (70B) ist ein leistungsstarkes Chat-Modell, das komplexe Dialoganforderungen unterst\xfctzt."},"meta-llama/Llama-3-8b-chat-hf":{"description":"LLaMA-3 Chat (8B) bietet mehrsprachige Unterst\xfctzung und deckt ein breites Spektrum an Fachwissen ab."},"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo":{"description":"LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bew\xe4ltigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"meta-llama/Llama-3.2-3B-Instruct-Turbo":{"description":"LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bew\xe4ltigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo":{"description":"LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bew\xe4ltigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"meta-llama/Llama-3.3-70B-Instruct-Turbo":{"description":"Das Meta Llama 3.3 mehrsprachige gro\xdfe Sprachmodell (LLM) ist ein vortrainiertes und anweisungsoptimiertes Generierungsmodell mit 70B (Textinput/Textoutput). Das anweisungsoptimierte reine Textmodell von Llama 3.3 wurde f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertrifft viele verf\xfcgbare Open-Source- und geschlossene Chat-Modelle in g\xe4ngigen Branchenbenchmarks."},"meta-llama/Llama-Vision-Free":{"description":"LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bew\xe4ltigen, die sowohl visuelle als auch Textdaten kombinieren. Es erzielt hervorragende Ergebnisse bei Aufgaben wie Bildbeschreibung und visueller Fragebeantwortung und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"meta-llama/Meta-Llama-3-70B-Instruct-Lite":{"description":"Llama 3 70B Instruct Lite ist f\xfcr Umgebungen geeignet, die hohe Leistung und niedrige Latenz erfordern."},"meta-llama/Meta-Llama-3-70B-Instruct-Turbo":{"description":"Llama 3 70B Instruct Turbo bietet hervorragende Sprachverst\xe4ndnis- und Generierungsf\xe4higkeiten und eignet sich f\xfcr die anspruchsvollsten Rechenaufgaben."},"meta-llama/Meta-Llama-3-8B-Instruct-Lite":{"description":"Llama 3 8B Instruct Lite ist f\xfcr ressourcenbeschr\xe4nkte Umgebungen geeignet und bietet eine hervorragende Balance zwischen Leistung und Effizienz."},"meta-llama/Meta-Llama-3-8B-Instruct-Turbo":{"description":"Llama 3 8B Instruct Turbo ist ein leistungsstarkes gro\xdfes Sprachmodell, das eine breite Palette von Anwendungsszenarien unterst\xfctzt."},"meta-llama/Meta-Llama-3.1-405B-Instruct":{"description":"LLaMA 3.1 405B ist ein leistungsstarkes Modell f\xfcr Vortraining und Anweisungsanpassung."},"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo":{"description":"Das 405B Llama 3.1 Turbo-Modell bietet eine enorme Kapazit\xe4t zur Unterst\xfctzung von Kontexten f\xfcr die Verarbeitung gro\xdfer Datenmengen und zeigt herausragende Leistungen in gro\xdf angelegten KI-Anwendungen."},"meta-llama/Meta-Llama-3.1-70B":{"description":"Llama 3.1 ist das f\xfchrende Modell von Meta, das bis zu 405B Parameter unterst\xfctzt und in komplexen Gespr\xe4chen, mehrsprachiger \xdcbersetzung und Datenanalyse eingesetzt werden kann."},"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo":{"description":"Das Llama 3.1 70B-Modell wurde feinabgestimmt und eignet sich f\xfcr hochbelastete Anwendungen, die auf FP8 quantisiert wurden, um eine effizientere Rechenleistung und Genauigkeit zu bieten und in komplexen Szenarien hervorragende Leistungen zu gew\xe4hrleisten."},"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo":{"description":"Das Llama 3.1 8B-Modell verwendet FP8-Quantisierung und unterst\xfctzt bis zu 131.072 Kontextmarkierungen, es ist eines der besten Open-Source-Modelle, das sich f\xfcr komplexe Aufgaben eignet und in vielen Branchenbenchmarks \xfcbertrifft."},"meta-llama/llama-3-70b-instruct":{"description":"Llama 3 70B Instruct ist optimiert f\xfcr qualitativ hochwertige Dialogszenarien und zeigt hervorragende Leistungen in verschiedenen menschlichen Bewertungen."},"meta-llama/llama-3-8b-instruct":{"description":"Llama 3 8B Instruct optimiert qualitativ hochwertige Dialogszenarien und bietet bessere Leistungen als viele geschlossene Modelle."},"meta-llama/llama-3.1-70b-instruct":{"description":"Llama 3.1 70B Instruct ist speziell f\xfcr qualitativ hochwertige Dialoge konzipiert und zeigt herausragende Leistungen in menschlichen Bewertungen, besonders geeignet f\xfcr hochinteraktive Szenarien."},"meta-llama/llama-3.1-8b-instruct":{"description":"Llama 3.1 8B Instruct ist die neueste Version von Meta, optimiert f\xfcr qualitativ hochwertige Dialogszenarien und \xfcbertrifft viele f\xfchrende geschlossene Modelle."},"meta-llama/llama-3.1-8b-instruct:free":{"description":"LLaMA 3.1 bietet Unterst\xfctzung f\xfcr mehrere Sprachen und geh\xf6rt zu den f\xfchrenden generativen Modellen der Branche."},"meta-llama/llama-3.2-11b-vision-instruct":{"description":"LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellem Fragen und Antworten und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"meta-llama/llama-3.2-3b-instruct":{"description":"meta-llama/llama-3.2-3b-instruct"},"meta-llama/llama-3.2-90b-vision-instruct":{"description":"LLaMA 3.2 ist darauf ausgelegt, Aufgaben zu bearbeiten, die visuelle und textuelle Daten kombinieren. Es zeigt hervorragende Leistungen bei Aufgaben wie Bildbeschreibung und visuellem Fragen und Antworten und \xfcberbr\xfcckt die Kluft zwischen Sprachgenerierung und visueller Schlussfolgerung."},"meta-llama/llama-3.3-70b-instruct":{"description":"Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und verbessert die N\xfctzlichkeit und Sicherheit durch \xfcberwachte Feinabstimmung (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF). Die auf Anweisungen optimierte Version ist speziell f\xfcr mehrsprachige Dialoge optimiert und \xfcbertrifft in mehreren Branchenbenchmarks viele Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."},"meta-llama/llama-3.3-70b-instruct:free":{"description":"Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Sprachmodell der Llama-Serie, das eine Leistung bietet, die mit einem 405B-Modell vergleichbar ist, und das zu extrem niedrigen Kosten. Es basiert auf der Transformer-Architektur und verbessert die N\xfctzlichkeit und Sicherheit durch \xfcberwachte Feinabstimmung (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF). Die auf Anweisungen optimierte Version ist speziell f\xfcr mehrsprachige Dialoge optimiert und \xfcbertrifft in mehreren Branchenbenchmarks viele Open-Source- und geschlossene Chat-Modelle. Das Wissensdatum endet im Dezember 2023."},"meta.llama3-1-405b-instruct-v1:0":{"description":"Meta Llama 3.1 405B Instruct ist das gr\xf6\xdfte und leistungsst\xe4rkste Modell innerhalb des Llama 3.1 Instruct Modells. Es handelt sich um ein hochentwickeltes Modell f\xfcr dialogbasierte Schlussfolgerungen und die Generierung synthetischer Daten, das auch als Grundlage f\xfcr die professionelle kontinuierliche Vorab- und Feinabstimmung in bestimmten Bereichen verwendet werden kann. Die mehrsprachigen gro\xdfen Sprachmodelle (LLMs) von Llama 3.1 sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, die in den Gr\xf6\xdfen 8B, 70B und 405B (Text-Eingabe/Ausgabe) verf\xfcgbar sind. Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind speziell f\xfcr mehrsprachige Dialoganwendungen optimiert und haben in g\xe4ngigen Branchenbenchmarks viele verf\xfcgbare Open-Source-Chat-Modelle \xfcbertroffen. Llama 3.1 ist f\xfcr kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich f\xfcr assistentengleiche Chats, w\xe4hrend die vortrainierten Modelle f\xfcr verschiedene Aufgaben der nat\xfcrlichen Sprachgenerierung angepasst werden k\xf6nnen. Das Llama 3.1 Modell unterst\xfctzt auch die Nutzung seiner Ausgaben zur Verbesserung anderer Modelle, einschlie\xdflich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das auf einer optimierten Transformer-Architektur basiert. Die angepasste Version verwendet \xfcberwachte Feinabstimmung (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Pr\xe4ferenzen f\xfcr Hilfsbereitschaft und Sicherheit zu entsprechen."},"meta.llama3-1-70b-instruct-v1:0":{"description":"Die aktualisierte Version von Meta Llama 3.1 70B Instruct umfasst eine erweiterte Kontextl\xe4nge von 128K, Mehrsprachigkeit und verbesserte Schlussfolgerungsf\xe4higkeiten. Die von Llama 3.1 bereitgestellten mehrsprachigen gro\xdfen Sprachmodelle (LLMs) sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, einschlie\xdflich Gr\xf6\xdfen von 8B, 70B und 405B (Textinput/-output). Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertreffen viele verf\xfcgbare Open-Source-Chat-Modelle in g\xe4ngigen Branchenbenchmarks. Llama 3.1 ist f\xfcr kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich f\xfcr assistentengleiche Chats, w\xe4hrend die vortrainierten Modelle f\xfcr eine Vielzahl von Aufgaben der nat\xfcrlichen Sprachgenerierung angepasst werden k\xf6nnen. Llama 3.1-Modelle unterst\xfctzen auch die Nutzung ihrer Ausgaben zur Verbesserung anderer Modelle, einschlie\xdflich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das mit einer optimierten Transformer-Architektur entwickelt wurde. Die angepassten Versionen verwenden \xfcberwachte Feinabstimmung (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Pr\xe4ferenzen f\xfcr Hilfsbereitschaft und Sicherheit zu entsprechen."},"meta.llama3-1-8b-instruct-v1:0":{"description":"Die aktualisierte Version von Meta Llama 3.1 8B Instruct umfasst eine erweiterte Kontextl\xe4nge von 128K, Mehrsprachigkeit und verbesserte Schlussfolgerungsf\xe4higkeiten. Die von Llama 3.1 bereitgestellten mehrsprachigen gro\xdfen Sprachmodelle (LLMs) sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, einschlie\xdflich Gr\xf6\xdfen von 8B, 70B und 405B (Textinput/-output). Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind f\xfcr mehrsprachige Dialoganwendungen optimiert und \xfcbertreffen viele verf\xfcgbare Open-Source-Chat-Modelle in g\xe4ngigen Branchenbenchmarks. Llama 3.1 ist f\xfcr kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich f\xfcr assistentengleiche Chats, w\xe4hrend die vortrainierten Modelle f\xfcr eine Vielzahl von Aufgaben der nat\xfcrlichen Sprachgenerierung angepasst werden k\xf6nnen. Llama 3.1-Modelle unterst\xfctzen auch die Nutzung ihrer Ausgaben zur Verbesserung anderer Modelle, einschlie\xdflich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das mit einer optimierten Transformer-Architektur entwickelt wurde. Die angepassten Versionen verwenden \xfcberwachte Feinabstimmung (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Pr\xe4ferenzen f\xfcr Hilfsbereitschaft und Sicherheit zu entsprechen."},"meta.llama3-70b-instruct-v1:0":{"description":"Meta Llama 3 ist ein offenes gro\xdfes Sprachmodell (LLM), das sich an Entwickler, Forscher und Unternehmen richtet und ihnen hilft, ihre Ideen f\xfcr generative KI zu entwickeln, zu experimentieren und verantwortungsbewusst zu skalieren. Als Teil eines globalen Innovationssystems ist es besonders geeignet f\xfcr die Erstellung von Inhalten, Dialog-KI, Sprachverst\xe4ndnis, Forschung und Unternehmensanwendungen."},"meta.llama3-8b-instruct-v1:0":{"description":"Meta Llama 3 ist ein offenes gro\xdfes Sprachmodell (LLM), das sich an Entwickler, Forscher und Unternehmen richtet und ihnen hilft, ihre Ideen f\xfcr generative KI zu entwickeln, zu experimentieren und verantwortungsbewusst zu skalieren. Als Teil eines globalen Innovationssystems ist es besonders geeignet f\xfcr Umgebungen mit begrenzter Rechenleistung und Ressourcen, f\xfcr Edge-Ger\xe4te und schnellere Trainingszeiten."},"meta/Llama-3.2-11B-Vision-Instruct":{"description":"Exzellente Bildinferenzf\xe4higkeiten bei hochaufl\xf6senden Bildern, ideal f\xfcr Anwendungen im Bereich visuelles Verst\xe4ndnis."},"meta/Llama-3.2-90B-Vision-Instruct":{"description":"Fortschrittliche Bildinferenzf\xe4higkeiten f\xfcr visuelle Verst\xe4ndnisagenten."},"meta/Llama-3.3-70B-Instruct":{"description":"Llama 3.3 ist das fortschrittlichste mehrsprachige Open-Source-Gro\xdfsprachmodell der Llama-Reihe, das Leistung vergleichbar mit einem 405B-Modell zu sehr niedrigen Kosten bietet. Basierend auf der Transformer-Architektur, verbessert durch \xfcberwachtes Feintuning (SFT) und verst\xe4rkendes Lernen mit menschlichem Feedback (RLHF) f\xfcr N\xfctzlichkeit und Sicherheit. Die instruktionsoptimierte Version ist f\xfcr mehrsprachige Dialoge optimiert und \xfcbertrifft viele offene und geschlossene Chatmodelle in verschiedenen Branchenbenchmarks. Wissensstand: Dezember 2023."},"meta/Meta-Llama-3-70B-Instruct":{"description":"Ein leistungsstarkes Modell mit 70 Milliarden Parametern, das hervorragende Leistungen bei Inferenz, Codierung und vielf\xe4ltigen Sprachaufgaben zeigt."},"meta/Meta-Llama-3-8B-Instruct":{"description":"Ein vielseitiges Modell mit 8 Milliarden Parametern, optimiert f\xfcr Dialog- und Textgenerierungsaufgaben."},"meta/Meta-Llama-3.1-405B-Instruct":{"description":"Llama 3.1 ist ein instruktionsoptimiertes Textmodell, das f\xfcr mehrsprachige Dialoganwendungen optimiert wurde und in vielen verf\xfcgbaren offenen und geschlossenen Chatmodellen bei g\xe4ngigen Branchenbenchmarks hervorragende Leistungen zeigt."},"meta/Meta-Llama-3.1-70B-Instruct":{"description":"Llama 3.1 ist ein instruktionsoptimiertes Textmodell, das f\xfcr mehrsprachige Dialoganwendungen optimiert wurde und in vielen verf\xfcgbaren offenen und geschlossenen Chatmodellen bei g\xe4ngigen Branchenbenchmarks hervorragende Leistungen zeigt."},"meta/Meta-Llama-3.1-8B-Instruct":{"description":"Llama 3.1 ist ein instruktionsoptimiertes Textmodell, das f\xfcr mehrsprachige Dialoganwendungen optimiert wurde und in vielen verf\xfcgbaren offenen und geschlossenen Chatmodellen bei g\xe4ngigen Branchenbenchmarks hervorragende Leistungen zeigt."},"meta/llama-3-70b":{"description":"Ein von Meta sorgf\xe4ltig f\xfcr die Befolgung von Anweisungen abgestimmtes Open-Source-Modell mit 70 Milliarden Parametern. Betrieben von Groq mit deren ma\xdfgeschneiderter Language Processing Unit (LPU) Hardware f\xfcr schnelle und effiziente Inferenz."},"meta/llama-3-8b":{"description":"Ein von Meta sorgf\xe4ltig f\xfcr die Befolgung von Anweisungen abgestimmtes Open-Source-Modell mit 8 Milliarden Parametern. Betrieben von Groq mit deren ma\xdfgeschneiderter Language Processing Unit (LPU) Hardware f\xfcr schnelle und effiziente Inferenz."},"meta/llama-3.1-405b-instruct":{"description":"Fortgeschrittenes LLM, das die Generierung synthetischer Daten, Wissensverdichtung und Schlussfolgerungen unterst\xfctzt, geeignet f\xfcr Chatbots, Programmierung und spezifische Aufgaben."},"meta/llama-3.1-70b":{"description":"Aktualisierte Version von Meta Llama 3 70B Instruct mit erweitertem 128K Kontextfenster, Mehrsprachigkeit und verbesserter Inferenzf\xe4higkeit."},"meta/llama-3.1-70b-instruct":{"description":"Erm\xf6glicht komplexe Gespr\xe4che mit hervorragendem Kontextverst\xe4ndnis, Schlussfolgerungsf\xe4higkeiten und Textgenerierungsf\xe4higkeiten."},"meta/llama-3.1-8b":{"description":"Llama 3.1 8B unterst\xfctzt ein 128K Kontextfenster und ist ideal f\xfcr Echtzeit-Dialogschnittstellen und Datenanalysen, w\xe4hrend es im Vergleich zu gr\xf6\xdferen Modellen erhebliche Kosteneinsparungen bietet. Betrieben von Groq mit deren ma\xdfgeschneiderter Language Processing Unit (LPU) Hardware f\xfcr schnelle und effiziente Inferenz."},"meta/llama-3.1-8b-instruct":{"description":"Fortschrittliches, hochmodernes Modell mit Sprachverst\xe4ndnis, hervorragenden Schlussfolgerungsf\xe4higkeiten und Textgenerierungsf\xe4higkeiten."},"meta/llama-3.2-11b":{"description":"Anweisungsabgestimmtes Bildinferenz-Generierungsmodell (Text + Bildeingabe / Textausgabe), optimiert f\xfcr visuelle Erkennung, Bildinferenz, Bildunterschriftenerstellung und allgemeine Fragen zu Bildern."},"meta/llama-3.2-11b-vision-instruct":{"description":"Spitzenm\xe4\xdfiges visuelles Sprachmodell, das in der Lage ist, qualitativ hochwertige Schlussfolgerungen aus Bildern zu ziehen."},"meta/llama-3.2-1b":{"description":"Reines Textmodell, unterst\xfctzt On-Device-Anwendungsf\xe4lle wie mehrsprachige lokale Wissenssuche, Zusammenfassung und Umschreibung."},"meta/llama-3.2-1b-instruct":{"description":"Fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverst\xe4ndnis, hervorragenden Schlussfolgerungsf\xe4higkeiten und Textgenerierungsf\xe4higkeiten."},"meta/llama-3.2-3b":{"description":"Reines Textmodell, sorgf\xe4ltig abgestimmt zur Unterst\xfctzung von On-Device-Anwendungsf\xe4llen wie mehrsprachige lokale Wissenssuche, Zusammenfassung und Umschreibung."},"meta/llama-3.2-3b-instruct":{"description":"Fortschrittliches, hochmodernes kleines Sprachmodell mit Sprachverst\xe4ndnis, hervorragenden Schlussfolgerungsf\xe4higkeiten und Textgenerierungsf\xe4higkeiten."},"meta/llama-3.2-90b":{"description":"Anweisungsabgestimmtes Bildinferenz-Generierungsmodell (Text + Bildeingabe / Textausgabe), optimiert f\xfcr visuelle Erkennung, Bildinferenz, Bildunterschriftenerstellung und allgemeine Fragen zu Bildern."},"meta/llama-3.2-90b-vision-instruct":{"description":"Spitzenm\xe4\xdfiges visuelles Sprachmodell, das in der Lage ist, qualitativ hochwertige Schlussfolgerungen aus Bildern zu ziehen."},"meta/llama-3.3-70b":{"description":"Perfekte Kombination aus Leistung und Effizienz. Das Modell unterst\xfctzt leistungsstarke Dialog-KI, ist f\xfcr Inhaltserstellung, Unternehmensanwendungen und Forschung konzipiert und bietet fortschrittliche Sprachverst\xe4ndnisf\xe4higkeiten, einschlie\xdflich Textzusammenfassung, Klassifikation, Sentimentanalyse und Codegenerierung."},"meta/llama-3.3-70b-instruct":{"description":"Fortschrittliches LLM, das auf Schlussfolgern, Mathematik, Allgemeinwissen und Funktionsaufrufen spezialisiert ist."},"meta/llama-4-maverick":{"description":"Die Llama 4 Modellreihe sind native multimodale KI-Modelle, die Text- und multimodale Erlebnisse unterst\xfctzen. Diese Modelle nutzen eine gemischte Expertenarchitektur und bieten branchenf\xfchrende Leistung bei Text- und Bildverst\xe4ndnis. Llama 4 Maverick ist ein 17 Milliarden Parameter Modell mit 128 Experten. Bereitgestellt von DeepInfra."},"meta/llama-4-scout":{"description":"Die Llama 4 Modellreihe sind native multimodale KI-Modelle, die Text- und multimodale Erlebnisse unterst\xfctzen. Diese Modelle nutzen eine gemischte Expertenarchitektur und bieten branchenf\xfchrende Leistung bei Text- und Bildverst\xe4ndnis. Llama 4 Scout ist ein 17 Milliarden Parameter Modell mit 16 Experten. Bereitgestellt von DeepInfra."},"microsoft/Phi-3-medium-128k-instruct":{"description":"Dasselbe Phi-3-medium-Modell, jedoch mit gr\xf6\xdferem Kontextfenster, geeignet f\xfcr RAG oder wenige Eingabeaufforderungen."},"microsoft/Phi-3-medium-4k-instruct":{"description":"Ein Modell mit 14 Milliarden Parametern, das qualitativ besser als Phi-3-mini ist und sich auf hochwertige, inferenzintensive Daten konzentriert."},"microsoft/Phi-3-mini-128k-instruct":{"description":"Dasselbe Phi-3-mini-Modell, jedoch mit gr\xf6\xdferem Kontextfenster, geeignet f\xfcr RAG oder wenige Eingabeaufforderungen."},"microsoft/Phi-3-mini-4k-instruct":{"description":"Das kleinste Mitglied der Phi-3-Familie, optimiert f\xfcr Qualit\xe4t und geringe Latenz."},"microsoft/Phi-3-small-128k-instruct":{"description":"Dasselbe Phi-3-small-Modell, jedoch mit gr\xf6\xdferem Kontextfenster, geeignet f\xfcr RAG oder wenige Eingabeaufforderungen."},"microsoft/Phi-3-small-8k-instruct":{"description":"Ein Modell mit 7 Milliarden Parametern, das qualitativ besser als Phi-3-mini ist und sich auf hochwertige, inferenzintensive Daten konzentriert."},"microsoft/Phi-3.5-mini-instruct":{"description":"Aktualisierte Version des Phi-3-mini-Modells."},"microsoft/Phi-3.5-vision-instruct":{"description":"Aktualisierte Version des Phi-3-vision-Modells."},"microsoft/WizardLM-2-8x22B":{"description":"WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, Mehrsprachigkeit, Inferenz und intelligenten Assistenten besonders gut abschneidet."},"microsoft/wizardlm-2-8x22b":{"description":"WizardLM-2 8x22B ist das fortschrittlichste Wizard-Modell von Microsoft AI und zeigt \xe4u\xdferst wettbewerbsf\xe4hige Leistungen."},"minicpm-v":{"description":"MiniCPM-V ist das neue multimodale Gro\xdfmodell von OpenBMB, das \xfcber hervorragende OCR-Erkennungs- und multimodale Verst\xe4ndnisf\xe4higkeiten verf\xfcgt und eine Vielzahl von Anwendungsszenarien unterst\xfctzt."},"minimax-m2":{"description":"MiniMax M2 ist ein leistungsstarkes, effizientes Sprachmodell, das speziell f\xfcr Programmier- und Agenten-Workflows entwickelt wurde."},"minimax/minimax-m2":{"description":"Speziell entwickelt f\xfcr effizientes Codieren und Agenten-Workflows."},"minimaxai/minimax-m2":{"description":"MiniMax-M2 ist ein kompaktes, schnelles und kosteneffizientes Mixture-of-Experts (MoE)-Modell mit 230 Milliarden Gesamtparametern und 10 Milliarden aktiven Parametern. Es wurde f\xfcr h\xf6chste Leistung bei Codierungs- und Agentenaufgaben entwickelt und bietet gleichzeitig eine starke allgemeine Intelligenz. Das Modell \xfcberzeugt bei Aufgaben wie der Bearbeitung mehrerer Dateien, dem Code-Ausf\xfchren-Fehlerbeheben-Zyklus, Testverifikation und -korrektur sowie bei komplexen, lang verkn\xfcpften Toolchains – und ist damit die ideale Wahl f\xfcr Entwickler-Workflows."},"ministral-3b-latest":{"description":"Ministral 3B ist das weltbeste Edge-Modell von Mistral."},"ministral-8b-latest":{"description":"Ministral 8B ist das kosteneffizienteste Edge-Modell von Mistral."},"mistral":{"description":"Mistral ist ein 7B-Modell von Mistral AI, das sich f\xfcr vielf\xe4ltige Anforderungen an die Sprachverarbeitung eignet."},"mistral-ai/Mistral-Large-2411":{"description":"Das Flaggschiffmodell von Mistral, geeignet f\xfcr komplexe Aufgaben mit gro\xdfem Inferenzbedarf oder hoher Spezialisierung (Textgenerierung, Codegenerierung, RAG oder Agenten)."},"mistral-ai/Mistral-Nemo":{"description":"Mistral Nemo ist ein hochmodernes Sprachmodell (LLM) mit f\xfchrenden F\xe4higkeiten in seiner Gr\xf6\xdfenklasse f\xfcr Inferenz, Weltwissen und Codierung."},"mistral-ai/mistral-small-2503":{"description":"Mistral Small eignet sich f\xfcr alle sprachbasierten Aufgaben, die hohe Effizienz und geringe Latenz erfordern."},"mistral-large":{"description":"Mixtral Large ist das Flaggschiff-Modell von Mistral, das die F\xe4higkeiten zur Codegenerierung, Mathematik und Schlussfolgerungen kombiniert und ein Kontextfenster von 128k unterst\xfctzt."},"mistral-large-instruct":{"description":"Mistral-Large-Instruct-2407 ist ein fortschrittliches dichtes gro\xdfes Sprachmodell (LLM) mit 123 Milliarden Parametern und verf\xfcgt \xfcber state-of-the-art-Schlie\xdfen, Wissen und Codierungsf\xe4higkeiten."},"mistral-large-latest":{"description":"Mistral Large ist das Flaggschiff-Modell, das sich gut f\xfcr mehrsprachige Aufgaben, komplexe Schlussfolgerungen und Codegenerierung eignet und die ideale Wahl f\xfcr hochentwickelte Anwendungen ist."},"mistral-medium-latest":{"description":"Mistral Medium 3 bietet mit 8-fachen Kosten erstklassige Leistung und vereinfacht grundlegend die Unternehmensbereitstellung."},"mistral-nemo":{"description":"Mistral Nemo wurde in Zusammenarbeit mit Mistral AI und NVIDIA entwickelt und ist ein leistungsstarkes 12B-Modell."},"mistral-nemo-instruct":{"description":"Das gro\xdfe Sprachmodell (LLM) Mistral-Nemo-Instruct-2407 ist eine auf Befehle angepasste Version von Mistral-Nemo-Base-2407."},"mistral-small":{"description":"Mistral Small kann f\xfcr jede sprachbasierte Aufgabe verwendet werden, die hohe Effizienz und geringe Latenz erfordert."},"mistral-small-latest":{"description":"Mistral Small ist eine kosteneffiziente, schnelle und zuverl\xe4ssige Option f\xfcr Anwendungsf\xe4lle wie \xdcbersetzung, Zusammenfassung und Sentimentanalyse."},"mistral/codestral":{"description":"Mistral Codestral 25.01 ist ein hochmodernes Codierungsmodell, optimiert f\xfcr latenzarme und hochfrequente Anwendungsf\xe4lle. Es beherrscht \xfcber 80 Programmiersprachen und zeigt hervorragende Leistungen bei Aufgaben wie Fill-in-the-Middle (FIM), Codekorrektur und Testgenerierung."},"mistral/codestral-embed":{"description":"Ein Code-Einbettungsmodell, das in Code-Datenbanken und Repositories eingebettet werden kann, um Codierungsassistenten zu unterst\xfctzen."},"mistral/devstral-small":{"description":"Devstral ist ein agentenf\xe4higes gro\xdfes Sprachmodell f\xfcr Software-Engineering-Aufgaben und somit eine ausgezeichnete Wahl f\xfcr Software-Engineering-Agenten."},"mistral/magistral-medium":{"description":"Komplexes Denken, unterst\xfctzt durch tiefes Verst\xe4ndnis mit nachvollziehbarer und \xfcberpr\xfcfbarer transparenter Argumentation. Das Modell beh\xe4lt auch bei Sprachwechseln w\xe4hrend der Aufgabe eine hohe Genauigkeit in vielen Sprachen bei."},"mistral/magistral-small":{"description":"Komplexes Denken, unterst\xfctzt durch tiefes Verst\xe4ndnis mit nachvollziehbarer und \xfcberpr\xfcfbarer transparenter Argumentation. Das Modell beh\xe4lt auch bei Sprachwechseln w\xe4hrend der Aufgabe eine hohe Genauigkeit in vielen Sprachen bei."},"mistral/ministral-3b":{"description":"Ein kompaktes, effizientes Modell f\xfcr On-Device-Aufgaben wie intelligente Assistenten und lokale Analysen mit niedriger Latenz."},"mistral/ministral-8b":{"description":"Ein leistungsf\xe4higeres Modell mit schnellerer und speichereffizienter Inferenz, ideal f\xfcr komplexe Workflows und anspruchsvolle Edge-Anwendungen."},"mistral/mistral-embed":{"description":"Universelles Texteingebettetes Modell f\xfcr semantische Suche, \xc4hnlichkeit, Clustering und RAG-Workflows."},"mistral/mistral-large":{"description":"Mistral Large ist ideal f\xfcr komplexe Aufgaben, die gro\xdfe Inferenzkapazit\xe4ten oder hohe Spezialisierung erfordern – wie synthetische Textgenerierung, Codegenerierung, RAG oder Agenten."},"mistral/mistral-small":{"description":"Mistral Small ist ideal f\xfcr einfache Aufgaben, die in gro\xdfen Mengen ausgef\xfchrt werden k\xf6nnen – wie Klassifikation, Kundensupport oder Textgenerierung. Es bietet hervorragende Leistung zu einem erschwinglichen Preis."},"mistral/mixtral-8x22b-instruct":{"description":"8x22b Instruct Modell. 8x22b ist ein von Mistral bereitgestelltes gemischtes Experten-Open-Source-Modell."},"mistral/pixtral-12b":{"description":"Ein 12 Milliarden Parameter Modell mit Bildverst\xe4ndnisf\xe4higkeiten sowie Text."},"mistral/pixtral-large":{"description":"Pixtral Large ist das zweite Modell unserer multimodalen Familie und demonstriert Spitzenleistungen im Bildverst\xe4ndnis. Insbesondere kann das Modell Dokumente, Diagramme und nat\xfcrliche Bilder verstehen und beh\xe4lt dabei die f\xfchrenden Textverst\xe4ndnisf\xe4higkeiten von Mistral Large 2 bei."},"mistralai/Mistral-7B-Instruct-v0.1":{"description":"Mistral (7B) Instruct ist bekannt f\xfcr seine hohe Leistung und eignet sich f\xfcr eine Vielzahl von Sprachaufgaben."},"mistralai/Mistral-7B-Instruct-v0.2":{"description":"Mistral 7B ist ein nach Bedarf feinabgestimmtes Modell, das optimierte Antworten auf Aufgaben bietet."},"mistralai/Mistral-7B-Instruct-v0.3":{"description":"Mistral (7B) Instruct v0.3 bietet effiziente Rechenleistung und nat\xfcrliche Sprachverst\xe4ndnisf\xe4higkeiten und eignet sich f\xfcr eine Vielzahl von Anwendungen."},"mistralai/Mistral-7B-v0.1":{"description":"Mistral 7B ist ein kompaktes, aber leistungsstarkes Modell, das gut f\xfcr Batch-Verarbeitung und einfache Aufgaben wie Klassifizierung und Textgenerierung geeignet ist und \xfcber gute Schlussfolgerungsf\xe4higkeiten verf\xfcgt."},"mistralai/Mixtral-8x22B-Instruct-v0.1":{"description":"Mixtral-8x22B Instruct (141B) ist ein super gro\xdfes Sprachmodell, das extrem hohe Verarbeitungsanforderungen unterst\xfctzt."},"mistralai/Mixtral-8x7B-Instruct-v0.1":{"description":"Mixtral 8x7B ist ein vortrainiertes sparsames Mischmodell, das f\xfcr allgemeine Textaufgaben verwendet wird."},"mistralai/Mixtral-8x7B-v0.1":{"description":"Mixtral 8x7B ist ein sparsames Expertenmodell, das mehrere Parameter nutzt, um die Schlussfolgerungsgeschwindigkeit zu erh\xf6hen, und sich gut f\xfcr mehrsprachige und Code-Generierungsaufgaben eignet."},"mistralai/mistral-nemo":{"description":"Mistral Nemo ist ein 7,3B-Parameter-Modell mit Unterst\xfctzung f\xfcr mehrere Sprachen und hoher Programmierleistung."},"mixtral":{"description":"Mixtral ist das Expertenmodell von Mistral AI, das \xfcber Open-Source-Gewichte verf\xfcgt und Unterst\xfctzung bei der Codegenerierung und Sprachverst\xe4ndnis bietet."},"mixtral-8x7b-32768":{"description":"Mixtral 8x7B bietet hochgradig fehlertolerante parallele Berechnungsf\xe4higkeiten und eignet sich f\xfcr komplexe Aufgaben."},"mixtral:8x22b":{"description":"Mixtral ist das Expertenmodell von Mistral AI, das \xfcber Open-Source-Gewichte verf\xfcgt und Unterst\xfctzung bei der Codegenerierung und Sprachverst\xe4ndnis bietet."},"moonshot-v1-128k":{"description":"Moonshot V1 128K ist ein Modell mit \xfcberragenden F\xe4higkeiten zur Verarbeitung von langen Kontexten, das f\xfcr die Generierung von sehr langen Texten geeignet ist und die Anforderungen komplexer Generierungsaufgaben erf\xfcllt. Es kann Inhalte mit bis zu 128.000 Tokens verarbeiten und eignet sich hervorragend f\xfcr Anwendungen in der Forschung, Wissenschaft und der Erstellung gro\xdfer Dokumente."},"moonshot-v1-128k-vision-preview":{"description":"Das Kimi-Visionsmodell (einschlie\xdflich moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview usw.) kann Bildinhalte verstehen, einschlie\xdflich Bildtext, Bildfarbe und Objektformen."},"moonshot-v1-32k":{"description":"Moonshot V1 32K bietet die F\xe4higkeit zur Verarbeitung von mittellangen Kontexten und kann 32.768 Tokens verarbeiten, was es besonders geeignet f\xfcr die Generierung verschiedener langer Dokumente und komplexer Dialoge macht, die in den Bereichen Inhaltserstellung, Berichtsgenerierung und Dialogsysteme eingesetzt werden."},"moonshot-v1-32k-vision-preview":{"description":"Das Kimi-Visionsmodell (einschlie\xdflich moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview usw.) kann Bildinhalte verstehen, einschlie\xdflich Bildtext, Bildfarbe und Objektformen."},"moonshot-v1-8k":{"description":"Moonshot V1 8K ist f\xfcr die Generierung von Kurztextaufgaben konzipiert und bietet eine effiziente Verarbeitungsleistung, die 8.192 Tokens verarbeiten kann. Es eignet sich hervorragend f\xfcr kurze Dialoge, Notizen und schnelle Inhaltserstellung."},"moonshot-v1-8k-vision-preview":{"description":"Das Kimi-Visionsmodell (einschlie\xdflich moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview usw.) kann Bildinhalte verstehen, einschlie\xdflich Bildtext, Bildfarbe und Objektformen."},"moonshot-v1-auto":{"description":"Moonshot V1 Auto kann basierend auf der Anzahl der im aktuellen Kontext verwendeten Tokens das geeignete Modell ausw\xe4hlen."},"moonshotai/Kimi-Dev-72B":{"description":"Kimi-Dev-72B ist ein Open-Source-Gro\xdfmodell f\xfcr Quellcode, das durch umfangreiche Verst\xe4rkungslernoptimierung robuste und direkt produktionsreife Patches erzeugen kann. Dieses Modell erreichte auf SWE-bench Verified eine neue H\xf6chstpunktzahl von 60,4 % und stellte damit einen Rekord f\xfcr Open-Source-Modelle bei automatisierten Software-Engineering-Aufgaben wie Fehlerbehebung und Code-Review auf."},"moonshotai/Kimi-K2-Instruct-0905":{"description":"Kimi K2-Instruct-0905 ist die neueste und leistungsst\xe4rkste Version von Kimi K2. Es handelt sich um ein erstklassiges Mixture-of-Experts (MoE) Sprachmodell mit insgesamt 1 Billion Parametern und 32 Milliarden aktivierten Parametern. Die Hauptmerkmale dieses Modells umfassen: verbesserte Agenten-Codierungsintelligenz, die in \xf6ffentlichen Benchmark-Tests und realen Agenten-Codierungsaufgaben eine signifikante Leistungssteigerung zeigt; verbesserte Frontend-Codierungserfahrung mit Fortschritten in \xc4sthetik und Praktikabilit\xe4t der Frontend-Programmierung."},"moonshotai/kimi-k2":{"description":"Kimi K2 ist ein von Moonshot AI entwickeltes gro\xdfes gemischtes Experten (MoE) Sprachmodell mit insgesamt 1 Billion Parametern und 32 Milliarden aktiven Parametern pro Vorw\xe4rtsdurchlauf. Es ist auf Agentenf\xe4higkeiten optimiert, einschlie\xdflich fortgeschrittener Werkzeugnutzung, Inferenz und Code-Synthese."},"moonshotai/kimi-k2-0905":{"description":"Das Modell kimi-k2-0905-preview hat eine Kontextl\xe4nge von 256k, verf\xfcgt \xfcber st\xe4rkere Agentic-Coding-F\xe4higkeiten, eine herausragendere \xc4sthetik und Praktikabilit\xe4t von Frontend-Code sowie ein besseres Kontextverst\xe4ndnis."},"moonshotai/kimi-k2-instruct-0905":{"description":"Das Modell kimi-k2-0905-preview hat eine Kontextl\xe4nge von 256k, verf\xfcgt \xfcber st\xe4rkere Agentic-Coding-F\xe4higkeiten, eine herausragendere \xc4sthetik und Praktikabilit\xe4t von Frontend-Code sowie ein besseres Kontextverst\xe4ndnis."},"morph/morph-v3-fast":{"description":"Morph bietet ein spezialisiertes KI-Modell, das von f\xfchrenden Modellen wie Claude oder GPT-4o vorgeschlagene Code\xe4nderungen schnell auf Ihre bestehenden Code-Dateien anwendet – mit \xfcber 4500 Tokens pro Sekunde. Es fungiert als letzter Schritt im KI-Codierungsworkflow und unterst\xfctzt 16k Eingabe- und 16k Ausgabe-Tokens."},"morph/morph-v3-large":{"description":"Morph bietet ein spezialisiertes KI-Modell, das von f\xfchrenden Modellen wie Claude oder GPT-4o vorgeschlagene Code\xe4nderungen schnell auf Ihre bestehenden Code-Dateien anwendet – mit \xfcber 2500 Tokens pro Sekunde. Es fungiert als letzter Schritt im KI-Codierungsworkflow und unterst\xfctzt 16k Eingabe- und 16k Ausgabe-Tokens."},"nousresearch/hermes-2-pro-llama-3-8b":{"description":"Hermes 2 Pro Llama 3 8B ist die aktualisierte Version von Nous Hermes 2 und enth\xe4lt die neuesten intern entwickelten Datens\xe4tze."},"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF":{"description":"Llama 3.1 Nemotron 70B ist ein von NVIDIA ma\xdfgeschneidertes gro\xdfes Sprachmodell, das darauf abzielt, die Hilfsf\xe4higkeit der von LLM generierten Antworten auf Benutzeranfragen zu verbessern. Dieses Modell hat in Benchmark-Tests wie Arena Hard, AlpacaEval 2 LC und GPT-4-Turbo MT-Bench hervorragende Leistungen gezeigt und belegt bis zum 1. Oktober 2024 den ersten Platz in allen drei automatischen Ausrichtungsbenchmarks. Das Modell wurde mit RLHF (insbesondere REINFORCE), Llama-3.1-Nemotron-70B-Reward und HelpSteer2-Preference-Prompts auf dem Llama-3.1-70B-Instruct-Modell trainiert."},"nvidia/llama-3.1-nemotron-51b-instruct":{"description":"Einzigartiges Sprachmodell, das unvergleichliche Genauigkeit und Effizienz bietet."},"nvidia/llama-3.1-nemotron-70b-instruct":{"description":"Llama-3.1-Nemotron-70B-Instruct ist ein von NVIDIA ma\xdfgeschneidertes gro\xdfes Sprachmodell, das darauf abzielt, die Hilfsbereitschaft der von LLM generierten Antworten zu verbessern."},"o1":{"description":"Konzentriert sich auf fortgeschrittene Inferenz und die L\xf6sung komplexer Probleme, einschlie\xdflich mathematischer und wissenschaftlicher Aufgaben. Besonders geeignet f\xfcr Anwendungen, die ein tiefes Verst\xe4ndnis des Kontexts und die Abwicklung von Arbeitsabl\xe4ufen erfordern."},"o1-mini":{"description":"o1-mini ist ein schnelles und kosteneffizientes Inferenzmodell, das f\xfcr Programmier-, Mathematik- und Wissenschaftsanwendungen entwickelt wurde. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."},"o1-preview":{"description":"Konzentriert auf fortgeschrittenes Schlussfolgern und die L\xf6sung komplexer Probleme, einschlie\xdflich mathematischer und naturwissenschaftlicher Aufgaben. Sehr gut geeignet f\xfcr Anwendungen, die ein tiefes Kontextverst\xe4ndnis und autonome Arbeitsabl\xe4ufe ben\xf6tigen."},"o1-pro":{"description":"Die o1-Serie wurde durch verst\xe4rkendes Lernen trainiert, um vor der Antwort nachzudenken und komplexe Schlussfolgerungen zu ziehen. Das o1-pro Modell nutzt mehr Rechenressourcen f\xfcr tiefere \xdcberlegungen und liefert dadurch kontinuierlich qualitativ hochwertigere Antworten."},"o3":{"description":"o3 ist ein vielseitiges und leistungsstarkes Modell, das in mehreren Bereichen hervorragende Leistungen zeigt. Es setzt neue Ma\xdfst\xe4be f\xfcr mathematische, wissenschaftliche, programmiertechnische und visuelle Schlussfolgerungsaufgaben. Es ist auch versiert in technischer Schreibweise und der Befolgung von Anweisungen. Benutzer k\xf6nnen es nutzen, um Texte, Code und Bilder zu analysieren und komplexe Probleme mit mehreren Schritten zu l\xf6sen."},"o3-2025-04-16":{"description":"o3 ist das neue Schlussfolgerungsmodell von OpenAI, unterst\xfctzt Bild- und Texteingaben und gibt Text aus, geeignet f\xfcr komplexe Aufgaben mit umfangreichem Allgemeinwissen."},"o3-deep-research":{"description":"o3-deep-research ist unser fortschrittlichstes Deep-Research-Modell, das speziell f\xfcr die Bearbeitung komplexer, mehrstufiger Forschungsaufgaben entwickelt wurde. Es kann Informationen aus dem Internet suchen und zusammenfassen sowie \xfcber den MCP-Connector auf Ihre eigenen Daten zugreifen und diese nutzen."},"o3-mini":{"description":"o3-mini ist unser neuestes kompaktes Inferenzmodell, das bei den gleichen Kosten- und Verz\xf6gerungszielen wie o1-mini hohe Intelligenz bietet."},"o3-pro":{"description":"Das o3-pro Modell verwendet mehr Rechenleistung f\xfcr tiefere \xdcberlegungen und liefert stets bessere Antworten. Es ist ausschlie\xdflich \xfcber die Responses API nutzbar."},"o3-pro-2025-06-10":{"description":"o3 Pro ist das neue Schlussfolgerungsmodell von OpenAI, unterst\xfctzt Bild- und Texteingaben und gibt Text aus, geeignet f\xfcr komplexe Aufgaben mit umfangreichem Allgemeinwissen."},"o4-mini":{"description":"o4-mini ist unser neuestes kompaktes Modell der o-Serie. Es wurde f\xfcr schnelle und effektive Inferenz optimiert und zeigt in Programmier- und visuellen Aufgaben eine hohe Effizienz und Leistung."},"o4-mini-2025-04-16":{"description":"o4-mini ist ein Schlussfolgerungsmodell von OpenAI, unterst\xfctzt Bild- und Texteingaben und gibt Text aus, geeignet f\xfcr komplexe Aufgaben mit umfangreichem Allgemeinwissen. Das Modell verf\xfcgt \xfcber einen Kontextumfang von 200.000 Token."},"o4-mini-deep-research":{"description":"o4-mini-deep-research ist unser schnelleres und kosteng\xfcnstigeres Deep-Research-Modell – ideal f\xfcr die Bearbeitung komplexer, mehrstufiger Forschungsaufgaben. Es kann Informationen aus dem Internet suchen und zusammenfassen sowie \xfcber den MCP-Connector auf Ihre eigenen Daten zugreifen und diese nutzen."},"open-codestral-mamba":{"description":"Codestral Mamba ist ein auf die Codegenerierung spezialisiertes Mamba 2-Sprachmodell, das starke Unterst\xfctzung f\xfcr fortschrittliche Code- und Schlussfolgerungsaufgaben bietet."},"open-mistral-7b":{"description":"Mistral 7B ist ein kompaktes, aber leistungsstarkes Modell, das sich gut f\xfcr Batch-Verarbeitung und einfache Aufgaben wie Klassifizierung und Textgenerierung eignet und \xfcber gute Schlussfolgerungsf\xe4higkeiten verf\xfcgt."},"open-mistral-nemo":{"description":"Mistral Nemo ist ein 12B-Modell, das in Zusammenarbeit mit Nvidia entwickelt wurde und hervorragende Schlussfolgerungs- und Codierungsf\xe4higkeiten bietet, die leicht zu integrieren und zu ersetzen sind."},"open-mixtral-8x22b":{"description":"Mixtral 8x22B ist ein gr\xf6\xdferes Expertenmodell, das sich auf komplexe Aufgaben konzentriert und hervorragende Schlussfolgerungsf\xe4higkeiten sowie eine h\xf6here Durchsatzrate bietet."},"open-mixtral-8x7b":{"description":"Mixtral 8x7B ist ein sp\xe4rliches Expertenmodell, das mehrere Parameter nutzt, um die Schlussfolgerungsgeschwindigkeit zu erh\xf6hen und sich f\xfcr die Verarbeitung mehrsprachiger und Codegenerierungsaufgaben eignet."},"openai/gpt-3.5-turbo":{"description":"OpenAIs leistungsf\xe4higstes und kosteneffizientestes Modell der GPT-3.5-Reihe, optimiert f\xfcr Chat-Anwendungen, aber auch gut f\xfcr traditionelle Completion-Aufgaben geeignet."},"openai/gpt-3.5-turbo-instruct":{"description":"F\xe4higkeiten \xe4hnlich den Modellen der GPT-3-\xc4ra. Kompatibel mit traditionellen Completion-Endpunkten, nicht mit Chat-Completion-Endpunkten."},"openai/gpt-4-turbo":{"description":"OpenAIs gpt-4-turbo verf\xfcgt \xfcber umfangreiches Allgemeinwissen und Fachkenntnisse, kann komplexen nat\xfcrlichen Sprachbefehlen folgen und schwierige Probleme pr\xe4zise l\xf6sen. Wissensstand bis April 2023, Kontextfenster von 128.000 Tokens."},"openai/gpt-4.1":{"description":"GPT 4.1 ist das Flaggschiffmodell von OpenAI, geeignet f\xfcr komplexe Aufgaben. Es ist hervorragend f\xfcr interdisziplin\xe4re Probleml\xf6sungen."},"openai/gpt-4.1-mini":{"description":"GPT 4.1 mini bietet eine ausgewogene Kombination aus Intelligenz, Geschwindigkeit und Kosten und ist damit f\xfcr viele Anwendungsf\xe4lle attraktiv."},"openai/gpt-4.1-nano":{"description":"GPT-4.1 nano ist das schnellste und kosteneffizienteste Modell der GPT 4.1 Reihe."},"openai/gpt-4o":{"description":"GPT-4o von OpenAI verf\xfcgt \xfcber umfangreiches Allgemeinwissen und Fachkenntnisse, kann komplexen nat\xfcrlichen Sprachbefehlen folgen und schwierige Probleme pr\xe4zise l\xf6sen. Es bietet die Leistung von GPT-4 Turbo mit schnellerem und kosteng\xfcnstigerem API-Zugriff."},"openai/gpt-4o-mini":{"description":"GPT-4o mini von OpenAI ist ihr fortschrittlichstes und kosteneffizientestes kleines Modell. Es ist multimodal (akzeptiert Text- oder Bildeingaben und gibt Text aus) und intelligenter als gpt-3.5-turbo, bei gleicher Geschwindigkeit."},"openai/gpt-5":{"description":"GPT-5 ist OpenAIs Flaggschiff-Sprachmodell mit herausragender Leistung bei komplexer Inferenz, umfangreichem Weltwissen, codeintensiven und mehrstufigen Agentenaufgaben."},"openai/gpt-5-mini":{"description":"GPT-5 mini ist ein kostenoptimiertes Modell mit hervorragender Leistung bei Inferenz- und Chat-Aufgaben. Es bietet die beste Balance zwischen Geschwindigkeit, Kosten und F\xe4higkeiten."},"openai/gpt-5-nano":{"description":"GPT-5 nano ist ein Modell mit hohem Durchsatz, das bei einfachen Anweisungen oder Klassifizierungsaufgaben hervorragende Leistungen zeigt."},"openai/gpt-oss-120b":{"description":"Extrem leistungsf\xe4higes universelles gro\xdfes Sprachmodell mit starker, kontrollierbarer Inferenzf\xe4higkeit."},"openai/gpt-oss-20b":{"description":"Ein kompaktes, Open-Source-Gewichtsmodell, optimiert f\xfcr niedrige Latenz und ressourcenbeschr\xe4nkte Umgebungen, einschlie\xdflich lokaler und Edge-Bereitstellungen."},"openai/o1":{"description":"OpenAIs o1 ist ein Flaggschiff-Inferenzmodell, entwickelt f\xfcr komplexe Probleme, die tiefes Nachdenken erfordern. Es bietet starke Inferenzf\xe4higkeiten und h\xf6here Genauigkeit bei komplexen mehrstufigen Aufgaben."},"openai/o1-mini":{"description":"o1-mini ist ein schnelles und kosteneffizientes Inferenzmodell, das f\xfcr Programmier-, Mathematik- und Wissenschaftsanwendungen entwickelt wurde. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."},"openai/o1-preview":{"description":"o1 ist OpenAIs neues Inferenzmodell, das f\xfcr komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."},"openai/o3":{"description":"OpenAIs o3 ist das leistungsst\xe4rkste Inferenzmodell mit neuen Spitzenleistungen in Codierung, Mathematik, Wissenschaft und visueller Wahrnehmung. Es ist besonders gut bei komplexen Anfragen, die multidisziplin\xe4re Analyse erfordern, und hat besondere St\xe4rken bei der Analyse von Bildern, Diagrammen und Grafiken."},"openai/o3-mini":{"description":"o3-mini ist OpenAIs neuestes kleines Inferenzmodell, das bei gleichen Kosten- und Latenzzielen wie o1-mini hohe Intelligenz bietet."},"openai/o3-mini-high":{"description":"o3-mini high ist eine hochintelligente Version mit dem gleichen Kosten- und Verz\xf6gerungsziel wie o1-mini."},"openai/o4-mini":{"description":"OpenAIs o4-mini bietet schnelle, kosteneffiziente Inferenz mit hervorragender Leistung f\xfcr seine Gr\xf6\xdfe, insbesondere bei Mathematik (beste Leistung im AIME-Benchmark), Codierung und visuellen Aufgaben."},"openai/o4-mini-high":{"description":"o4-mini Hochleistungsmodell, optimiert f\xfcr schnelle und effektive Inferenz, zeigt in Programmier- und visuellen Aufgaben eine hohe Effizienz und Leistung."},"openai/text-embedding-3-large":{"description":"OpenAIs leistungsf\xe4higstes Einbettungsmodell, geeignet f\xfcr englische und nicht-englische Aufgaben."},"openai/text-embedding-3-small":{"description":"OpenAIs verbesserte, leistungsst\xe4rkere Version des ada-Einbettungsmodells."},"openai/text-embedding-ada-002":{"description":"OpenAIs traditionelles Texteingebettetes Modell."},"openrouter/auto":{"description":"Je nach Kontextl\xe4nge, Thema und Komplexit\xe4t wird Ihre Anfrage an Llama 3 70B Instruct, Claude 3.5 Sonnet (selbstregulierend) oder GPT-4o gesendet."},"perplexity/sonar":{"description":"Perplexitys leichtgewichtiges Produkt mit Suchanbindung, schneller und g\xfcnstiger als Sonar Pro."},"perplexity/sonar-pro":{"description":"Perplexitys Flaggschiffprodukt mit Suchanbindung, unterst\xfctzt erweiterte Abfragen und Folgeaktionen."},"perplexity/sonar-reasoning":{"description":"Ein auf Inferenz fokussiertes Modell, das Denkprozesse (CoT) in Antworten ausgibt und detaillierte Erkl\xe4rungen mit Suchanbindung bietet."},"perplexity/sonar-reasoning-pro":{"description":"Ein fortgeschrittenes, auf Inferenz fokussiertes Modell, das Denkprozesse (CoT) in Antworten ausgibt und umfassende Erkl\xe4rungen mit verbesserter Suchf\xe4higkeit und mehreren Suchanfragen pro Anfrage bietet."},"phi3":{"description":"Phi-3 ist ein leichtgewichtiges offenes Modell von Microsoft, das f\xfcr effiziente Integration und gro\xdfangelegte Wissensschl\xfcsse geeignet ist."},"phi3:14b":{"description":"Phi-3 ist ein leichtgewichtiges offenes Modell von Microsoft, das f\xfcr effiziente Integration und gro\xdfangelegte Wissensschl\xfcsse geeignet ist."},"pixtral-12b-2409":{"description":"Das Pixtral-Modell zeigt starke F\xe4higkeiten in Aufgaben wie Diagramm- und Bildverst\xe4ndnis, Dokumentenfragen, multimodale Schlussfolgerungen und Befolgung von Anweisungen. Es kann Bilder in nat\xfcrlicher Aufl\xf6sung und Seitenverh\xe4ltnis aufnehmen und in einem langen Kontextfenster von bis zu 128K Tokens beliebig viele Bilder verarbeiten."},"pixtral-large-latest":{"description":"Pixtral Large ist ein Open-Source-Multimodalmodell mit 124 Milliarden Parametern, das auf Mistral Large 2 basiert. Dies ist unser zweites Modell in der multimodalen Familie und zeigt fortschrittliche F\xe4higkeiten im Bereich der Bildverst\xe4ndnis."},"pro-128k":{"description":"Spark Pro 128K verf\xfcgt \xfcber eine au\xdfergew\xf6hnliche Kontextverarbeitungsf\xe4higkeit und kann bis zu 128K Kontextinformationen verarbeiten, was es besonders geeignet f\xfcr die Analyse langer Texte und die Verarbeitung langfristiger logischer Zusammenh\xe4nge macht. Es bietet in komplexen Textkommunikationen fl\xfcssige und konsistente Logik sowie vielf\xe4ltige Unterst\xfctzung f\xfcr Zitate."},"pro-deepseek-r1":{"description":"Modell f\xfcr exklusive Unternehmensdienste, inklusive paralleler Serviceunterst\xfctzung."},"pro-deepseek-v3":{"description":"Modell f\xfcr exklusive Unternehmensdienste, inklusive paralleler Serviceunterst\xfctzung."},"qianfan-70b":{"description":"Qianfan 70B, ein gro\xdfparametrisches chinesisches Modell, geeignet f\xfcr hochwertige Inhaltserstellung und komplexe Schlussfolgerungsaufgaben."},"qianfan-8b":{"description":"Qianfan 8B, ein mittelgro\xdfes Allzweckmodell, ideal f\xfcr Textgenerierung und Frage-Antwort-Szenarien mit ausgewogenem Kosten-Nutzen-Verh\xe4ltnis."},"qianfan-agent-intent-32k":{"description":"Qianfan Agent Intent 32K, ein Modell f\xfcr Absichtserkennung und Agenten-Orchestrierung, unterst\xfctzt Szenarien mit langem Kontext."},"qianfan-agent-lite-8k":{"description":"Qianfan Agent Lite 8K, ein leichtgewichtiges Agentenmodell, geeignet f\xfcr kosteng\xfcnstige Mehrfachdialoge und Gesch\xe4ftsprozesse."},"qianfan-agent-speed-32k":{"description":"Qianfan Agent Speed 32K, ein hochperformantes Agentenmodell mit hoher Durchsatzrate, ideal f\xfcr gro\xdf angelegte, mehrfache Aufgabenanwendungen."},"qianfan-agent-speed-8k":{"description":"Qianfan Agent Speed 8K, ein hochgradig paralleles Agentenmodell f\xfcr mittlere bis kurze Dialoge und schnelle Reaktionen."},"qianfan-check-vl":{"description":"Qianfan Check VL, ein multimodales Modell zur Inhaltspr\xfcfung und -erkennung, unterst\xfctzt Aufgaben zur Einhaltung von Bild-Text-Richtlinien."},"qianfan-composition":{"description":"Qianfan Composition, ein multimodales Kreativmodell, unterst\xfctzt integriertes Verst\xe4ndnis und Generierung von Bild und Text."},"qianfan-engcard-vl":{"description":"Qianfan EngCard VL, ein multimodales Erkennungsmodell, spezialisiert auf englischsprachige Szenarien."},"qianfan-lightning-128b-a19b":{"description":"Qianfan Lightning 128B A19B, ein leistungsstarkes chinesisches Allzweckmodell, geeignet f\xfcr komplexe Frage-Antwort- und gro\xdf angelegte Schlussfolgerungsaufgaben."},"qianfan-llama-vl-8b":{"description":"Qianfan Llama VL 8B, ein auf Llama basierendes multimodales Modell f\xfcr allgemeines Bild-Text-Verst\xe4ndnis."},"qianfan-multipicocr":{"description":"Qianfan MultiPicOCR, ein OCR-Modell f\xfcr mehrere Bilder, unterst\xfctzt Texterkennung und -extraktion aus mehreren Bildern."},"qianfan-qi-vl":{"description":"Qianfan QI VL, ein multimodales Frage-Antwort-Modell, erm\xf6glicht pr\xe4zise Suche und Beantwortung in komplexen Bild-Text-Szenarien."},"qianfan-singlepicocr":{"description":"Qianfan SinglePicOCR, ein OCR-Modell f\xfcr Einzelbilder, unterst\xfctzt hochpr\xe4zise Zeichenerkennung."},"qianfan-vl-70b":{"description":"Qianfan VL 70B, ein gro\xdfparametrisches visuell-sprachliches Modell, geeignet f\xfcr komplexe Bild-Text-Verst\xe4ndnisaufgaben."},"qianfan-vl-8b":{"description":"Qianfan VL 8B, ein leichtgewichtiges visuell-sprachliches Modell, ideal f\xfcr allt\xe4gliche Bild-Text-Fragen und Analysen."},"qvq-72b-preview":{"description":"Das QVQ-Modell ist ein experimentelles Forschungsmodell, das vom Qwen-Team entwickelt wurde und sich auf die Verbesserung der visuellen Schlussfolgerungsf\xe4higkeiten konzentriert, insbesondere im Bereich der mathematischen Schlussfolgerungen."},"qvq-max":{"description":"Tongyi Qianwen QVQ visuelles Schlussfolgerungsmodell, unterst\xfctzt visuelle Eingaben und Denkprozessketten-Ausgaben, zeigt st\xe4rkere F\xe4higkeiten in Mathematik, Programmierung, visueller Analyse, Kreativit\xe4t und allgemeinen Aufgaben."},"qvq-plus":{"description":"Visuelles Schlussfolgerungsmodell. Unterst\xfctzt visuelle Eingaben und Denkprozess-Ausgaben. Die Plus-Version, die auf dem qvq-max-Modell basiert, bietet schnellere Inferenzgeschwindigkeit sowie ein ausgewogeneres Verh\xe4ltnis von Leistung und Kosten."},"qwen-3-32b":{"description":"Qwen 3 32B: Ein Modell der Qwen-Serie mit starker Leistung bei mehrsprachigen und Programmieraufgaben, geeignet f\xfcr mittelgro\xdfe produktive Eins\xe4tze."},"qwen-3-coder-480b":{"description":"Qwen 3 Coder 480B: Ein Modell mit langem Kontext, das f\xfcr Codegenerierung und komplexe Programmieraufgaben entwickelt wurde."},"qwen-coder-plus":{"description":"Tongyi Qianwen Codierungsmodell."},"qwen-coder-turbo":{"description":"Tongyi Qianwen Codierungsmodell."},"qwen-coder-turbo-latest":{"description":"Das Tongyi Qianwen Code-Modell."},"qwen-flash":{"description":"Die Tongyi-Qianwen-Reihe bietet besonders schnelle und sehr kosteng\xfcnstige Modelle und eignet sich f\xfcr einfache Aufgaben."},"qwen-image":{"description":"Qwen-Image ist ein universelles Bildgenerierungsmodell, das zahlreiche Kunststile unterst\xfctzt und sich besonders bei der Wiedergabe komplexer Texte auszeichnet, insbesondere bei chinesischen und englischen Schriftz\xfcgen. Das Modell unterst\xfctzt mehrzeilige Layouts, absatzweises Textgenerieren sowie die pr\xe4zise Darstellung feiner Details und erm\xf6glicht die Erstellung komplexer Bild-Text-Kombinationen."},"qwen-image-edit":{"description":"Qwen Image Edit ist ein Bild-zu-Bild-Modell, das die Bearbeitung und Modifikation von Bildern basierend auf Eingabebildern und Textanweisungen unterst\xfctzt. Es erm\xf6glicht pr\xe4zise Anpassungen und kreative Umgestaltungen des Originalbildes entsprechend den Anforderungen der Nutzer."},"qwen-long":{"description":"Qwen ist ein gro\xdf angelegtes Sprachmodell, das lange Textkontexte unterst\xfctzt und Dialogfunktionen f\xfcr verschiedene Szenarien wie lange Dokumente und mehrere Dokumente bietet."},"qwen-math-plus":{"description":"Tongyi Qianwen Mathematikmodell, speziell f\xfcr mathematische Probleml\xf6sungen entwickelt."},"qwen-math-plus-latest":{"description":"Das Tongyi Qianwen Mathematikmodell ist speziell f\xfcr die L\xf6sung von mathematischen Problemen konzipiert."},"qwen-math-turbo":{"description":"Tongyi Qianwen Mathematikmodell, speziell f\xfcr mathematische Probleml\xf6sungen entwickelt."},"qwen-math-turbo-latest":{"description":"Das Tongyi Qianwen Mathematikmodell ist speziell f\xfcr die L\xf6sung von mathematischen Problemen konzipiert."},"qwen-max":{"description":"Qwen Max ist ein gro\xdfangelegtes Sprachmodell auf Billionenebene, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterst\xfctzt und das API-Modell hinter der aktuellen Produktversion von Qwen 2.5 ist."},"qwen-omni-turbo":{"description":"Die Qwen-Omni-Modellreihe unterst\xfctzt die Eingabe verschiedener Modalit\xe4ten, einschlie\xdflich Video, Audio, Bild und Text, und gibt Audio und Text aus."},"qwen-plus":{"description":"Qwen Plus ist die verbesserte Version des gro\xdfangelegten Sprachmodells, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterst\xfctzt."},"qwen-turbo":{"description":"通义千问 Turbo wird k\xfcnftig nicht mehr aktualisiert. Es wird empfohlen, auf 通义千问 Flash umzusteigen. 通义千问 ist ein \xe4u\xdferst gro\xdf angelegtes Sprachmodell und unterst\xfctzt Eingaben in Chinesisch, Englisch und weiteren Sprachen."},"qwen-vl-chat-v1":{"description":"Qwen VL unterst\xfctzt flexible Interaktionsmethoden, einschlie\xdflich Mehrbild-, Mehrfachfragen und kreativen F\xe4higkeiten."},"qwen-vl-max":{"description":"Tongyi Qianwen extrem gro\xdfskaliges visuelles Sprachmodell. Im Vergleich zur erweiterten Version weitere Steigerung der visuellen Schlussfolgerungs- und Befehlsbefolgungsf\xe4higkeiten, bietet ein h\xf6heres Niveau visueller Wahrnehmung und Kognition."},"qwen-vl-max-latest":{"description":"Das Tongyi Qianwen Ultra-Scale Visuelle Sprachmodell. Im Vergleich zur verbesserten Version wurden die F\xe4higkeiten zur visuellen Schlussfolgerung und Befolgung von Anweisungen weiter gesteigert, was ein h\xf6heres Niveau an visueller Wahrnehmung und Kognition bietet."},"qwen-vl-ocr":{"description":"Tongyi Qianwen OCR ist ein spezialisiertes Modell zur Textextraktion, fokussiert auf Dokumente, Tabellen, Pr\xfcfungsaufgaben, Handschrift und andere Bildtypen. Es erkennt verschiedene Sprachen, darunter Chinesisch, Englisch, Franz\xf6sisch, Japanisch, Koreanisch, Deutsch, Russisch, Italienisch, Vietnamesisch und Arabisch."},"qwen-vl-plus":{"description":"Erweiterte Version des Tongyi Qianwen gro\xdfskaligen visuellen Sprachmodells. Deutliche Verbesserung der Detail- und Texterkennungsf\xe4higkeiten, unterst\xfctzt Bildaufl\xf6sungen von \xfcber einer Million Pixeln und beliebige Seitenverh\xe4ltnisse."},"qwen-vl-plus-latest":{"description":"Die verbesserte Version des Tongyi Qianwen, eines gro\xdfangelegten visuellen Sprachmodells. Deutlich verbesserte F\xe4higkeiten zur Detailerkennung und Texterkennung, unterst\xfctzt Bildaufl\xf6sungen von \xfcber einer Million Pixel und beliebige Seitenverh\xe4ltnisse."},"qwen-vl-v1":{"description":"Initiiert mit dem Qwen-7B-Sprachmodell, f\xfcgt es ein Bildmodell hinzu, das f\xfcr Bildeingaben mit einer Aufl\xf6sung von 448 vortrainiert wurde."},"qwen/qwen-2-7b-instruct":{"description":"Qwen2 ist die brandneue Serie von gro\xdfen Sprachmodellen von Qwen. Qwen2 7B ist ein transformerbasiertes Modell, das in den Bereichen Sprachverst\xe4ndnis, Mehrsprachigkeit, Programmierung, Mathematik und logisches Denken hervorragende Leistungen zeigt."},"qwen/qwen-2-7b-instruct:free":{"description":"Qwen2 ist eine neue Serie gro\xdfer Sprachmodelle mit st\xe4rkeren Verst\xe4ndnis- und Generierungsf\xe4higkeiten."},"qwen/qwen-2-vl-72b-instruct":{"description":"Qwen2-VL ist die neueste Iteration des Qwen-VL-Modells und hat in Benchmark-Tests zur visuellen Verst\xe4ndlichkeit eine fortschrittliche Leistung erreicht, einschlie\xdflich MathVista, DocVQA, RealWorldQA und MTVQA. Qwen2-VL kann \xfcber 20 Minuten Video verstehen und erm\xf6glicht qualitativ hochwertige, videobasierte Fragen und Antworten, Dialoge und Inhaltserstellung. Es verf\xfcgt auch \xfcber komplexe Denk- und Entscheidungsf\xe4higkeiten und kann mit mobilen Ger\xe4ten, Robotern usw. integriert werden, um basierend auf visuellen Umgebungen und Textanweisungen automatisch zu agieren. Neben Englisch und Chinesisch unterst\xfctzt Qwen2-VL jetzt auch das Verst\xe4ndnis von Text in Bildern in verschiedenen Sprachen, einschlie\xdflich der meisten europ\xe4ischen Sprachen, Japanisch, Koreanisch, Arabisch und Vietnamesisch."},"qwen/qwen-2.5-72b-instruct":{"description":"Qwen2.5-72B-Instruct ist eines der neuesten gro\xdfen Sprachmodell-Serien, die von Alibaba Cloud ver\xf6ffentlicht wurden. Dieses 72B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterst\xfctzung und deckt \xfcber 29 Sprachen ab, einschlie\xdflich Chinesisch und Englisch. Das Modell hat signifikante Verbesserungen in der Befolgung von Anweisungen, im Verst\xe4ndnis von strukturierten Daten und in der Generierung von strukturierten Ausgaben (insbesondere JSON) erzielt."},"qwen/qwen2.5-32b-instruct":{"description":"Qwen2.5-32B-Instruct ist eines der neuesten gro\xdfen Sprachmodell-Serien, die von Alibaba Cloud ver\xf6ffentlicht wurden. Dieses 32B-Modell hat signifikante Verbesserungen in den Bereichen Codierung und Mathematik. Das Modell bietet auch mehrsprachige Unterst\xfctzung und deckt \xfcber 29 Sprachen ab, einschlie\xdflich Chinesisch und Englisch. Das Modell hat signifikante Verbesserungen in der Befolgung von Anweisungen, im Verst\xe4ndnis von strukturierten Daten und in der Generierung von strukturierten Ausgaben (insbesondere JSON) erzielt."},"qwen/qwen2.5-7b-instruct":{"description":"LLM, das auf Chinesisch und Englisch ausgerichtet ist und sich auf Sprache, Programmierung, Mathematik, Schlussfolgern und andere Bereiche konzentriert."},"qwen/qwen2.5-coder-32b-instruct":{"description":"Fortgeschrittenes LLM, das die Codegenerierung, Schlussfolgerungen und Korrekturen unterst\xfctzt und g\xe4ngige Programmiersprachen abdeckt."},"qwen/qwen2.5-coder-7b-instruct":{"description":"Leistungsstarkes, mittelgro\xdfes Codierungsmodell, das 32K Kontextl\xe4ngen unterst\xfctzt und in der mehrsprachigen Programmierung versiert ist."},"qwen/qwen3-14b":{"description":"Qwen3-14B ist ein kompaktes, 14,8 Milliarden Parameter umfassendes kausales Sprachmodell aus der Qwen3-Serie, das speziell f\xfcr komplexe Inferenz und effiziente Dialoge entwickelt wurde. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr Mathematik, Programmierung und logische Inferenz und dem \\"Nicht-Denk\\"-Modus f\xfcr allgemeine Gespr\xe4che. Dieses Modell wurde feinabgestimmt und kann f\xfcr die Befolgung von Anweisungen, die Nutzung von Agentenwerkzeugen, kreatives Schreiben sowie mehrsprachige Aufgaben in \xfcber 100 Sprachen und Dialekten verwendet werden. Es verarbeitet nativ 32K Token-Kontext und kann mithilfe von YaRN auf 131K Token erweitert werden."},"qwen/qwen3-14b:free":{"description":"Qwen3-14B ist ein kompaktes, 14,8 Milliarden Parameter umfassendes kausales Sprachmodell aus der Qwen3-Serie, das speziell f\xfcr komplexe Inferenz und effiziente Dialoge entwickelt wurde. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr Mathematik, Programmierung und logische Inferenz und dem \\"Nicht-Denk\\"-Modus f\xfcr allgemeine Gespr\xe4che. Dieses Modell wurde feinabgestimmt und kann f\xfcr die Befolgung von Anweisungen, die Nutzung von Agentenwerkzeugen, kreatives Schreiben sowie mehrsprachige Aufgaben in \xfcber 100 Sprachen und Dialekten verwendet werden. Es verarbeitet nativ 32K Token-Kontext und kann mithilfe von YaRN auf 131K Token erweitert werden."},"qwen/qwen3-235b-a22b":{"description":"Qwen3-235B-A22B ist ein 235B Parameter Expertenmischungsmodell (MoE), das von Qwen entwickelt wurde und bei jedem Vorw\xe4rtsdurchlauf 22B Parameter aktiviert. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr komplexe Inferenz, Mathematik und Programmieraufgaben und dem \\"Nicht-Denk\\"-Modus f\xfcr allgemeine Gespr\xe4che. Dieses Modell zeigt starke Inferenzf\xe4higkeiten, mehrsprachige Unterst\xfctzung (\xfcber 100 Sprachen und Dialekte), fortgeschrittene Befolgung von Anweisungen und die Nutzung von Agentenwerkzeugen. Es verarbeitet nativ ein Kontextfenster von 32K Token und kann mithilfe von YaRN auf 131K Token erweitert werden."},"qwen/qwen3-235b-a22b:free":{"description":"Qwen3-235B-A22B ist ein 235B Parameter Expertenmischungsmodell (MoE), das von Qwen entwickelt wurde und bei jedem Vorw\xe4rtsdurchlauf 22B Parameter aktiviert. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr komplexe Inferenz, Mathematik und Programmieraufgaben und dem \\"Nicht-Denk\\"-Modus f\xfcr allgemeine Gespr\xe4che. Dieses Modell zeigt starke Inferenzf\xe4higkeiten, mehrsprachige Unterst\xfctzung (\xfcber 100 Sprachen und Dialekte), fortgeschrittene Befolgung von Anweisungen und die Nutzung von Agentenwerkzeugen. Es verarbeitet nativ ein Kontextfenster von 32K Token und kann mithilfe von YaRN auf 131K Token erweitert werden."},"qwen/qwen3-30b-a3b":{"description":"Qwen3 ist die neueste Generation der Qwen gro\xdfen Sprachmodellreihe, die \xfcber eine dichte und Expertenmischung (MoE) Architektur verf\xfcgt und in den Bereichen Inferenz, mehrsprachige Unterst\xfctzung und anspruchsvolle Agentenaufgaben hervorragende Leistungen zeigt. Ihre einzigartige F\xe4higkeit, nahtlos zwischen komplexen Denkmodi und effizienten Dialogmodi zu wechseln, gew\xe4hrleistet eine vielseitige und qualitativ hochwertige Leistung.\\n\\nQwen3 \xfcbertrifft deutlich fr\xfchere Modelle wie QwQ und Qwen2.5 und bietet herausragende F\xe4higkeiten in Mathematik, Programmierung, allgemeinem Wissen, kreativem Schreiben und interaktiven Dialogen. Die Variante Qwen3-30B-A3B enth\xe4lt 30,5 Milliarden Parameter (3,3 Milliarden aktivierte Parameter), 48 Schichten, 128 Experten (jeweils 8 aktivierte f\xfcr jede Aufgabe) und unterst\xfctzt bis zu 131K Token-Kontext (unter Verwendung von YaRN), was einen neuen Standard f\xfcr Open-Source-Modelle setzt."},"qwen/qwen3-30b-a3b:free":{"description":"Qwen3 ist die neueste Generation der Qwen gro\xdfen Sprachmodellreihe, die \xfcber eine dichte und Expertenmischung (MoE) Architektur verf\xfcgt und in den Bereichen Inferenz, mehrsprachige Unterst\xfctzung und anspruchsvolle Agentenaufgaben hervorragende Leistungen zeigt. Ihre einzigartige F\xe4higkeit, nahtlos zwischen komplexen Denkmodi und effizienten Dialogmodi zu wechseln, gew\xe4hrleistet eine vielseitige und qualitativ hochwertige Leistung.\\n\\nQwen3 \xfcbertrifft deutlich fr\xfchere Modelle wie QwQ und Qwen2.5 und bietet herausragende F\xe4higkeiten in Mathematik, Programmierung, allgemeinem Wissen, kreativem Schreiben und interaktiven Dialogen. Die Variante Qwen3-30B-A3B enth\xe4lt 30,5 Milliarden Parameter (3,3 Milliarden aktivierte Parameter), 48 Schichten, 128 Experten (jeweils 8 aktivierte f\xfcr jede Aufgabe) und unterst\xfctzt bis zu 131K Token-Kontext (unter Verwendung von YaRN), was einen neuen Standard f\xfcr Open-Source-Modelle setzt."},"qwen/qwen3-32b":{"description":"Qwen3-32B ist ein kompaktes, 32,8 Milliarden Parameter umfassendes kausales Sprachmodell aus der Qwen3-Serie, das f\xfcr komplexe Inferenz und effiziente Dialoge optimiert wurde. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr Mathematik, Programmierung und logische Inferenz und dem \\"Nicht-Denk\\"-Modus f\xfcr schnellere, allgemeine Gespr\xe4che. Dieses Modell zeigt starke Leistungen in der Befolgung von Anweisungen, der Nutzung von Agentenwerkzeugen, kreativem Schreiben sowie mehrsprachigen Aufgaben in \xfcber 100 Sprachen und Dialekten. Es verarbeitet nativ 32K Token-Kontext und kann mithilfe von YaRN auf 131K Token erweitert werden."},"qwen/qwen3-32b:free":{"description":"Qwen3-32B ist ein kompaktes, 32,8 Milliarden Parameter umfassendes kausales Sprachmodell aus der Qwen3-Serie, das f\xfcr komplexe Inferenz und effiziente Dialoge optimiert wurde. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr Mathematik, Programmierung und logische Inferenz und dem \\"Nicht-Denk\\"-Modus f\xfcr schnellere, allgemeine Gespr\xe4che. Dieses Modell zeigt starke Leistungen in der Befolgung von Anweisungen, der Nutzung von Agentenwerkzeugen, kreativem Schreiben sowie mehrsprachigen Aufgaben in \xfcber 100 Sprachen und Dialekten. Es verarbeitet nativ 32K Token-Kontext und kann mithilfe von YaRN auf 131K Token erweitert werden."},"qwen/qwen3-8b:free":{"description":"Qwen3-8B ist ein kompaktes, 8,2 Milliarden Parameter umfassendes kausales Sprachmodell aus der Qwen3-Serie, das speziell f\xfcr inferenzintensive Aufgaben und effiziente Dialoge entwickelt wurde. Es unterst\xfctzt den nahtlosen Wechsel zwischen dem \\"Denk\\"-Modus f\xfcr Mathematik, Programmierung und logische Inferenz und dem \\"Nicht-Denk\\"-Modus f\xfcr allgemeine Gespr\xe4che. Dieses Modell wurde feinabgestimmt und kann f\xfcr die Befolgung von Anweisungen, die Integration von Agenten, kreatives Schreiben sowie die mehrsprachige Nutzung in \xfcber 100 Sprachen und Dialekten verwendet werden. Es unterst\xfctzt nativ ein Kontextfenster von 32K Token und kann \xfcber YaRN auf 131K Token erweitert werden."},"qwen2":{"description":"Qwen2 ist das neue gro\xdfe Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterst\xfctzt."},"qwen2.5":{"description":"Qwen2.5 ist das neue, gro\xdf angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterst\xfctzung vielf\xe4ltiger Anwendungsbed\xfcrfnisse bietet."},"qwen2.5-14b-instruct":{"description":"Das 14B-Modell von Tongyi Qianwen 2.5 ist \xf6ffentlich zug\xe4nglich."},"qwen2.5-14b-instruct-1m":{"description":"Tongyi Qianwen 2.5 ist ein Open-Source-Modell mit einer Gr\xf6\xdfe von 72B."},"qwen2.5-32b-instruct":{"description":"Das 32B-Modell von Tongyi Qianwen 2.5 ist \xf6ffentlich zug\xe4nglich."},"qwen2.5-72b-instruct":{"description":"Das 72B-Modell von Tongyi Qianwen 2.5 ist \xf6ffentlich zug\xe4nglich."},"qwen2.5-7b-instruct":{"description":"Qwen2.5 7B Instruct, ein ausgereiftes Open-Source-Instruktionsmodell, geeignet f\xfcr Dialoge und Generierung in verschiedenen Szenarien."},"qwen2.5-coder-1.5b-instruct":{"description":"Die Open-Source-Version des Qwen-Codemodells."},"qwen2.5-coder-14b-instruct":{"description":"Open-Source-Version des Tongyi Qianwen Codierungsmodells."},"qwen2.5-coder-32b-instruct":{"description":"Open-Source-Version des Tongyi Qianwen Code-Modells."},"qwen2.5-coder-7b-instruct":{"description":"Die Open-Source-Version des Tongyi Qianwen Code-Modells."},"qwen2.5-coder-instruct":{"description":"Qwen2.5-Coder ist das neueste Modell der Qwen-Serie, speziell f\xfcr den Codeentwicklungsbereich entwickelt (fr\xfcher bekannt als CodeQwen)."},"qwen2.5-instruct":{"description":"Qwen2.5 ist die neueste Serie des Qwen-Sprachmodells. F\xfcr Qwen2.5 haben wir mehrere Basis-Sprachmodelle und instruktionsfeinjustierte Sprachmodelle ver\xf6ffentlicht, deren Parameter von 500 Millionen bis 7,2 Milliarden reichen."},"qwen2.5-math-1.5b-instruct":{"description":"Das Qwen-Math-Modell verf\xfcgt \xfcber starke F\xe4higkeiten zur L\xf6sung mathematischer Probleme."},"qwen2.5-math-72b-instruct":{"description":"Das Qwen-Math-Modell verf\xfcgt \xfcber starke F\xe4higkeiten zur L\xf6sung mathematischer Probleme."},"qwen2.5-math-7b-instruct":{"description":"Das Qwen-Math-Modell verf\xfcgt \xfcber starke F\xe4higkeiten zur L\xf6sung mathematischer Probleme."},"qwen2.5-omni-7b":{"description":"Das Qwen-Omni-Modell der Serie unterst\xfctzt die Eingabe verschiedener Modalit\xe4ten, einschlie\xdflich Video, Audio, Bilder und Text, und gibt Audio und Text aus."},"qwen2.5-vl-32b-instruct":{"description":"Qwen2.5 VL 32B Instruct, ein multimodales Open-Source-Modell, ideal f\xfcr private Bereitstellung und vielseitige Anwendungen."},"qwen2.5-vl-72b-instruct":{"description":"Verbesserte Befolgung von Anweisungen, Mathematik, Probleml\xf6sung und Programmierung, gesteigerte Erkennungsf\xe4higkeiten f\xfcr alle Arten von visuellen Elementen, Unterst\xfctzung f\xfcr die pr\xe4zise Lokalisierung visueller Elemente in verschiedenen Formaten, Verst\xe4ndnis von langen Videodateien (maximal 10 Minuten) und sekundengenauer Ereigniszeitpunktlokalisierung, F\xe4higkeit zur zeitlichen Einordnung und Geschwindigkeitsverst\xe4ndnis, Unterst\xfctzung f\xfcr die Steuerung von OS- oder Mobile-Agenten basierend auf Analyse- und Lokalisierungsf\xe4higkeiten, starke F\xe4higkeit zur Extraktion von Schl\xfcsselinformationen und JSON-Format-Ausgabe. Diese Version ist die leistungsst\xe4rkste Version der 72B-Serie."},"qwen2.5-vl-7b-instruct":{"description":"Qwen2.5 VL 7B Instruct, ein leichtgewichtiges multimodales Modell, das Kosten und Erkennungsleistung ausbalanciert."},"qwen2.5-vl-instruct":{"description":"Qwen2.5-VL ist die neueste Version des visuellen Sprachmodells in der Qwen-Modellfamilie."},"qwen2.5:0.5b":{"description":"Qwen2.5 ist das neue, gro\xdf angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterst\xfctzung vielf\xe4ltiger Anwendungsbed\xfcrfnisse bietet."},"qwen2.5:1.5b":{"description":"Qwen2.5 ist das neue, gro\xdf angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterst\xfctzung vielf\xe4ltiger Anwendungsbed\xfcrfnisse bietet."},"qwen2.5:72b":{"description":"Qwen2.5 ist das neue, gro\xdf angelegte Sprachmodell der Alibaba-Gruppe, das hervorragende Leistungen zur Unterst\xfctzung vielf\xe4ltiger Anwendungsbed\xfcrfnisse bietet."},"qwen2:0.5b":{"description":"Qwen2 ist das neue gro\xdfe Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterst\xfctzt."},"qwen2:1.5b":{"description":"Qwen2 ist das neue gro\xdfe Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterst\xfctzt."},"qwen2:72b":{"description":"Qwen2 ist das neue gro\xdfe Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterst\xfctzt."},"qwen3":{"description":"Qwen3 ist das neue, gro\xdfangelegte Sprachmodell von Alibaba, das mit hervorragender Leistung vielf\xe4ltige Anwendungsbed\xfcrfnisse unterst\xfctzt."},"qwen3-0.6b":{"description":"Qwen3 0.6B, ein Einstiegsmodell, geeignet f\xfcr einfache Schlussfolgerungen und stark ressourcenbeschr\xe4nkte Umgebungen."},"qwen3-1.7b":{"description":"Qwen3 1.7B, ein ultraleichtes Modell, ideal f\xfcr Edge- und Endger\xe4tebereitstellung."},"qwen3-14b":{"description":"Qwen3 14B, ein mittelgro\xdfes Modell, geeignet f\xfcr mehrsprachige Frage-Antwort- und Textgenerierungsaufgaben."},"qwen3-235b-a22b":{"description":"Qwen3 235B A22B, ein universelles Gro\xdfmodell f\xfcr eine Vielzahl komplexer Aufgaben."},"qwen3-235b-a22b-instruct-2507":{"description":"Qwen3 235B A22B Instruct 2507, ein universelles Flaggschiff-Instruktionsmodell, geeignet f\xfcr vielf\xe4ltige Generierungs- und Schlussfolgerungsaufgaben."},"qwen3-235b-a22b-thinking-2507":{"description":"Qwen3 235B A22B Thinking 2507, ein extrem gro\xdfskaliges Denkmodell, ideal f\xfcr hochkomplexe Schlussfolgerungen."},"qwen3-30b-a3b":{"description":"Qwen3 30B A3B, ein mittelgro\xdfes bis gro\xdfes Allzweckmodell mit ausgewogenem Verh\xe4ltnis zwischen Kosten und Leistung."},"qwen3-30b-a3b-instruct-2507":{"description":"Qwen3 30B A3B Instruct 2507, ein mittelgro\xdfes bis gro\xdfes Instruktionsmodell, geeignet f\xfcr hochwertige Generierung und Frage-Antwort-Aufgaben."},"qwen3-30b-a3b-thinking-2507":{"description":"Qwen3 30B A3B Thinking 2507, ein mittelgro\xdfes bis gro\xdfes Denkmodell mit ausgewogener Genauigkeit und Effizienz."},"qwen3-32b":{"description":"Qwen3 32B, geeignet f\xfcr allgemeine Aufgaben mit erh\xf6htem Verst\xe4ndnisbedarf."},"qwen3-4b":{"description":"Qwen3 4B, ideal f\xfcr mittelgro\xdfe bis kleine Anwendungen und lokale Inferenzszenarien."},"qwen3-8b":{"description":"Qwen3 8B, ein leichtgewichtiges Modell mit flexibler Bereitstellung, geeignet f\xfcr hochparallele Anwendungen."},"qwen3-coder-30b-a3b-instruct":{"description":"Open-Source-Version des Qwen-Codegenerierungsmodells. Das neueste qwen3-coder-30b-a3b-instruct basiert auf Qwen3 und bietet leistungsstarke Coding-Agent-F\xe4higkeiten. Es ist spezialisiert auf Tool-Nutzung und Interaktion mit Umgebungen, erm\xf6glicht autonomes Programmieren und kombiniert herausragende Programmierf\xe4higkeiten mit allgemeinen F\xe4higkeiten."},"qwen3-coder-480b-a35b-instruct":{"description":"Qwen3 Coder 480B A35B Instruct, ein Flaggschiff-Code-Modell, unterst\xfctzt mehrsprachige Programmierung und komplexes Codeverst\xe4ndnis."},"qwen3-coder-flash":{"description":"Tongyi Qianwen Code-Modell. Die neueste Qwen3-Coder Modellreihe basiert auf Qwen3 und ist ein Code-Generierungsmodell mit starker Coding-Agent-F\xe4higkeit, spezialisiert auf Werkzeugaufrufe und Umgebungsinteraktion, das selbstst\xe4ndiges Programmieren erm\xf6glicht und neben hervorragenden Code-F\xe4higkeiten auch allgemeine Kompetenzen besitzt."},"qwen3-coder-plus":{"description":"Tongyi Qianwen Code-Modell. Die neueste Qwen3-Coder Modellreihe basiert auf Qwen3 und ist ein Code-Generierungsmodell mit starker Coding-Agent-F\xe4higkeit, spezialisiert auf Werkzeugaufrufe und Umgebungsinteraktion, das selbstst\xe4ndiges Programmieren erm\xf6glicht und neben hervorragenden Code-F\xe4higkeiten auch allgemeine Kompetenzen besitzt."},"qwen3-coder:480b":{"description":"Alibaba\'s leistungsstarkes Langkontextmodell f\xfcr Agenten- und Codierungsaufgaben."},"qwen3-max":{"description":"Tongyi Qianwen 3 Max Modellserie, die im Vergleich zur 2.5 Serie eine deutliche Verbesserung der allgemeinen F\xe4higkeiten bietet, einschlie\xdflich verbesserter Textverst\xe4ndnisf\xe4higkeiten in Chinesisch und Englisch, komplexer Befolgung von Anweisungen, subjektiver offener Aufgaben, Mehrsprachigkeit und Tool-Integration; das Modell zeigt weniger Wissenshalluzinationen. Die neueste qwen3-max Version wurde speziell im Bereich Agentenprogrammierung und Tool-Integration weiterentwickelt. Die offizielle Ver\xf6ffentlichung erreicht SOTA-Niveau in Fachgebieten und ist f\xfcr komplexere Agentenanforderungen optimiert."},"qwen3-max-preview":{"description":"Das leistungsst\xe4rkste Modell der Tongyi Qianwen-Serie, geeignet f\xfcr komplexe, mehrstufige Aufgaben. Vorschauversion mit Denkf\xe4higkeit."},"qwen3-next-80b-a3b-instruct":{"description":"Ein neues Open-Source-Modell der n\xe4chsten Generation im Nicht-Denk-Modus basierend auf Qwen3. Im Vergleich zur vorherigen Version (Tongyi Qianwen 3-235B-A22B-Instruct-2507) bietet es eine verbesserte chinesische Textverst\xe4ndnisf\xe4higkeit, verst\xe4rkte logische Schlussfolgerungen und bessere Leistung bei textgenerierenden Aufgaben."},"qwen3-next-80b-a3b-thinking":{"description":"Qwen3 Next 80B A3B Thinking, eine Flaggschiff-Version f\xfcr komplexe Schlussfolgerungsaufgaben."},"qwen3-omni-flash":{"description":"Das Qwen-Omni-Modell kann kombinierte Eingaben aus Text, Bildern, Audio und Video verarbeiten und Antworten in Text- oder Sprachform generieren. Es bietet verschiedene menschen\xe4hnliche Sprachstile, unterst\xfctzt mehrsprachige und dialektale Sprachausgabe und eignet sich f\xfcr Anwendungen wie Textgenerierung, visuelle Erkennung und Sprachassistenten."},"qwen3-vl-235b-a22b-instruct":{"description":"Qwen3 VL 235B A22B Instruct, ein Flaggschiff-Multimodalmodell f\xfcr anspruchsvolle Verst\xe4ndnis- und Kreativaufgaben."},"qwen3-vl-235b-a22b-thinking":{"description":"Qwen3 VL 235B A22B Thinking, die Denkversion des Flaggschiffs f\xfcr komplexe multimodale Schlussfolgerungs- und Planungsaufgaben."},"qwen3-vl-30b-a3b-instruct":{"description":"Qwen3 VL 30B A3B Instruct, ein gro\xdfes multimodales Modell mit ausgewogener Genauigkeit und Schlussfolgerungsleistung."},"qwen3-vl-30b-a3b-thinking":{"description":"Qwen3 VL 30B A3B Thinking, eine tiefgreifende Denkversion f\xfcr komplexe multimodale Aufgaben."},"qwen3-vl-32b-instruct":{"description":"Qwen3 VL 32B Instruct, ein multimodales Instruktionsmodell, geeignet f\xfcr hochwertige Bild-Text-Fragen und kreative Aufgaben."},"qwen3-vl-32b-thinking":{"description":"Qwen3 VL 32B Thinking, eine multimodale Denkversion mit Fokus auf komplexe Schlussfolgerungen und Langkettenanalysen."},"qwen3-vl-8b-instruct":{"description":"Qwen3 VL 8B Instruct, ein leichtgewichtiges multimodales Modell, ideal f\xfcr allt\xe4gliche visuelle Fragen und Anwendungsintegration."},"qwen3-vl-8b-thinking":{"description":"Qwen3 VL 8B Thinking, ein multimodales Denkmodell, geeignet f\xfcr detaillierte Schlussfolgerungen aus visuellen Informationen."},"qwen3-vl-flash":{"description":"Qwen3 VL Flash: eine leichtgewichtige, hochperformante Version f\xfcr schnelle Inferenz, ideal f\xfcr latenzkritische oder gro\xdfvolumige Anfragen."},"qwen3-vl-plus":{"description":"Tongyi Qianwen VL ist ein Textgenerierungsmodell mit visuellen (Bild-)Verst\xe4ndnisf\xe4higkeiten. Es kann nicht nur OCR (Texterkennung in Bildern) durchf\xfchren, sondern auch weiterf\xfchrende Zusammenfassungen und Schlussfolgerungen ziehen, z. B. Attribute aus Produktfotos extrahieren oder Aufgaben anhand von \xdcbungsbildern l\xf6sen."},"qwq":{"description":"QwQ ist ein experimentelles Forschungsmodell, das sich auf die Verbesserung der KI-Inferenzf\xe4higkeiten konzentriert."},"qwq-32b":{"description":"Das QwQ-Inferenzmodell, das auf dem Qwen2.5-32B-Modell trainiert wurde, hat durch verst\xe4rktes Lernen die Inferenzf\xe4higkeiten des Modells erheblich verbessert. Die Kernmetriken des Modells, wie mathematische Codes (AIME 24/25, LiveCodeBench) sowie einige allgemeine Metriken (IFEval, LiveBench usw.), erreichen das Niveau der DeepSeek-R1 Vollversion, wobei alle Metriken deutlich die ebenfalls auf Qwen2.5-32B basierende DeepSeek-R1-Distill-Qwen-32B \xfcbertreffen."},"qwq-32b-preview":{"description":"Das QwQ-Modell ist ein experimentelles Forschungsmodell, das vom Qwen-Team entwickelt wurde und sich auf die Verbesserung der KI-Inferenzf\xe4higkeiten konzentriert."},"qwq-plus":{"description":"Das QwQ-Inferenzmodell basiert auf dem Qwen2.5-Modell und verbessert die Modellschlussfolgerungsf\xe4higkeiten durch verst\xe4rkendes Lernen erheblich. Die Kernmetriken in Mathematik und Programmierung (AIME 24/25, LiveCodeBench) sowie einige allgemeine Metriken (IFEval, LiveBench usw.) erreichen das volle Leistungsniveau von DeepSeek-R1."},"qwq_32b":{"description":"Ein mittelgro\xdfes Schlussfolgerungsmodell der Qwen-Serie. Im Vergleich zu traditionellen Modellen mit Anweisungsoptimierung zeigt QwQ, das \xfcber Denk- und Schlussfolgerungsf\xe4higkeiten verf\xfcgt, in nachgelagerten Aufgaben, insbesondere bei der L\xf6sung schwieriger Probleme, eine signifikante Leistungssteigerung."},"r1-1776":{"description":"R1-1776 ist eine Version des DeepSeek R1 Modells, die nachtrainiert wurde, um unverf\xe4lschte, unvoreingenommene Fakteninformationen bereitzustellen."},"solar-mini":{"description":"Solar Mini ist ein kompaktes LLM, das besser abschneidet als GPT-3.5 und \xfcber starke Mehrsprachigkeitsf\xe4higkeiten verf\xfcgt. Es unterst\xfctzt Englisch und Koreanisch und bietet eine effiziente und kompakte L\xf6sung."},"solar-mini-ja":{"description":"Solar Mini (Ja) erweitert die F\xe4higkeiten von Solar Mini und konzentriert sich auf Japanisch, w\xe4hrend es gleichzeitig in der Nutzung von Englisch und Koreanisch hohe Effizienz und hervorragende Leistung beibeh\xe4lt."},"solar-pro":{"description":"Solar Pro ist ein hochintelligentes LLM, das von Upstage entwickelt wurde und sich auf die Befolgung von Anweisungen mit einer einzigen GPU konzentriert, mit einem IFEval-Score von \xfcber 80. Derzeit unterst\xfctzt es Englisch, die offizielle Version ist f\xfcr November 2024 geplant und wird die Sprachunterst\xfctzung und Kontextl\xe4nge erweitern."},"sonar":{"description":"Ein leichtgewichtiges Suchprodukt, das auf kontextbezogener Suche basiert und schneller und g\xfcnstiger ist als Sonar Pro."},"sonar-deep-research":{"description":"Deep Research f\xfchrt umfassende Expertenforschung durch und fasst diese in zug\xe4nglichen, umsetzbaren Berichten zusammen."},"sonar-pro":{"description":"Ein fortschrittliches Suchprodukt, das kontextbezogene Suche unterst\xfctzt und erweiterte Abfragen sowie Nachverfolgung erm\xf6glicht."},"sonar-reasoning":{"description":"Ein neues API-Produkt, das von DeepSeek-Inferenzmodellen unterst\xfctzt wird."},"sonar-reasoning-pro":{"description":"Ein neues API-Produkt, das von dem DeepSeek-Inferenzmodell unterst\xfctzt wird."},"stable-diffusion-3-medium":{"description":"Das neueste Text-zu-Bild-Gro\xdfmodell von Stability AI. Diese Version verbessert signifikant Bildqualit\xe4t, Textverst\xe4ndnis und Stilvielfalt gegen\xfcber Vorg\xe4ngerversionen, kann komplexe nat\xfcrliche Sprachaufforderungen pr\xe4ziser interpretieren und erzeugt genauere und vielf\xe4ltigere Bilder."},"stable-diffusion-3.5-large":{"description":"stable-diffusion-3.5-large ist ein multimodaler Diffusions-Transformer (MMDiT) mit 800 Millionen Parametern f\xfcr Text-zu-Bild-Generierung, bietet herausragende Bildqualit\xe4t und Prompt-\xdcbereinstimmung, unterst\xfctzt die Erzeugung von Bildern mit bis zu 1 Million Pixeln und l\xe4uft effizient auf handels\xfcblicher Hardware."},"stable-diffusion-3.5-large-turbo":{"description":"stable-diffusion-3.5-large-turbo basiert auf stable-diffusion-3.5-large und verwendet adversariale Diffusionsdestillation (ADD) f\xfcr h\xf6here Geschwindigkeit."},"stable-diffusion-v1.5":{"description":"stable-diffusion-v1.5 wurde mit den Gewichten des stable-diffusion-v1.2 Checkpoints initialisiert und mit 595k Schritten bei 512x512 Aufl\xf6sung auf „laion-aesthetics v2 5+“ feinabgestimmt. Dabei wurde die Textkonditionierung um 10 % reduziert, um die gef\xfchrte Stichprobenahme ohne Klassifikator zu verbessern."},"stable-diffusion-xl":{"description":"stable-diffusion-xl bringt bedeutende Verbesserungen gegen\xfcber v1.5 und erreicht eine Qualit\xe4t, die mit dem aktuellen Open-Source-Text-zu-Bild-SOTA-Modell Midjourney vergleichbar ist. Zu den Verbesserungen z\xe4hlen ein dreimal gr\xf6\xdferes UNet-Backbone, ein Verfeinerungsmodul zur Qualit\xe4tssteigerung der generierten Bilder sowie effizientere Trainingstechniken."},"stable-diffusion-xl-base-1.0":{"description":"Ein von Stability AI entwickeltes und Open-Source-Text-zu-Bild-Gro\xdfmodell mit branchenf\xfchrender kreativer Bildgenerierungsf\xe4higkeit. Es verf\xfcgt \xfcber exzellente Instruktionsverst\xe4ndnisf\xe4higkeiten und unterst\xfctzt die Definition von Inverse Prompts zur pr\xe4zisen Inhaltserzeugung."},"step-1-128k":{"description":"Bietet ein ausgewogenes Verh\xe4ltnis zwischen Leistung und Kosten, geeignet f\xfcr allgemeine Szenarien."},"step-1-256k":{"description":"Verf\xfcgt \xfcber die F\xe4higkeit zur Verarbeitung ultra-langer Kontexte, besonders geeignet f\xfcr die Analyse langer Dokumente."},"step-1-32k":{"description":"Unterst\xfctzt mittellange Dialoge und eignet sich f\xfcr verschiedene Anwendungsszenarien."},"step-1-8k":{"description":"Kleinmodell, geeignet f\xfcr leichte Aufgaben."},"step-1-flash":{"description":"Hochgeschwindigkeitsmodell, geeignet f\xfcr Echtzeitdialoge."},"step-1.5v-mini":{"description":"Dieses Modell verf\xfcgt \xfcber starke F\xe4higkeiten zur Videoanalyse."},"step-1o-turbo-vision":{"description":"Dieses Modell verf\xfcgt \xfcber starke F\xe4higkeiten zur Bildverst\xe4ndnis und \xfcbertrifft 1o in den Bereichen Mathematik und Programmierung. Das Modell ist kleiner als 1o und bietet eine schnellere Ausgabegeschwindigkeit."},"step-1o-vision-32k":{"description":"Dieses Modell verf\xfcgt \xfcber starke F\xe4higkeiten zur Bildverst\xe4ndnis. Im Vergleich zu den Modellen der Schritt-1v-Serie bietet es eine verbesserte visuelle Leistung."},"step-1v-32k":{"description":"Unterst\xfctzt visuelle Eingaben und verbessert die multimodale Interaktionserfahrung."},"step-1v-8k":{"description":"Kleinvisualmodell, geeignet f\xfcr grundlegende Text- und Bildaufgaben."},"step-1x-edit":{"description":"Dieses Modell ist auf Bildbearbeitungsaufgaben spezialisiert und kann Bilder basierend auf vom Nutzer bereitgestellten Bildern und Textbeschreibungen modifizieren und verbessern. Es unterst\xfctzt verschiedene Eingabeformate, einschlie\xdflich Textbeschreibungen und Beispielbilder, versteht die Nutzerintention und erzeugt entsprechende Bildbearbeitungsergebnisse."},"step-1x-medium":{"description":"Dieses Modell verf\xfcgt \xfcber starke Bildgenerierungsf\xe4higkeiten und unterst\xfctzt Texteingaben. Es bietet native chinesische Unterst\xfctzung, versteht und verarbeitet chinesische Textbeschreibungen besser, erfasst semantische Informationen pr\xe4ziser und wandelt sie in Bildmerkmale um, um genauere Bildgenerierung zu erm\xf6glichen. Das Modell erzeugt hochaufl\xf6sende, qualitativ hochwertige Bilder und besitzt eine gewisse Stil\xfcbertragungsf\xe4higkeit."},"step-2-16k":{"description":"Unterst\xfctzt gro\xdf angelegte Kontextinteraktionen und eignet sich f\xfcr komplexe Dialogszenarien."},"step-2-16k-exp":{"description":"Experimentelle Version des step-2 Modells, die die neuesten Funktionen enth\xe4lt und kontinuierlich aktualisiert wird. Nicht f\xfcr den Einsatz in produktiven Umgebungen empfohlen."},"step-2-mini":{"description":"Ein ultraschnelles Gro\xdfmodell, das auf der neuen, selbstentwickelten Attention-Architektur MFA basiert. Es erreicht mit extrem niedrigen Kosten \xe4hnliche Ergebnisse wie Schritt 1 und bietet gleichzeitig eine h\xf6here Durchsatzrate und schnellere Reaktionszeiten. Es kann allgemeine Aufgaben bearbeiten und hat besondere F\xe4higkeiten im Bereich der Codierung."},"step-2x-large":{"description":"Das neue Generationen-Bildmodell von Step Star konzentriert sich auf Bildgenerierung und kann basierend auf Textbeschreibungen des Nutzers hochwertige Bilder erzeugen. Das neue Modell erzeugt realistischere Bildtexturen und bietet verbesserte F\xe4higkeiten bei der Erzeugung chinesischer und englischer Schriftzeichen."},"step-3":{"description":"Dieses Modell verf\xfcgt \xfcber eine leistungsf\xe4hige visuelle Wahrnehmung und ausgepr\xe4gte F\xe4higkeiten zum komplexen Schlussfolgern. Es kann fach\xfcbergreifendes Verst\xe4ndnis komplexer Zusammenh\xe4nge, die kombinierte Analyse von mathematischen und visuellen Informationen sowie vielf\xe4ltige visuelle Analyseaufgaben des Alltags pr\xe4zise bew\xe4ltigen."},"step-r1-v-mini":{"description":"Dieses Modell ist ein leistungsstarkes Schlussfolgerungsmodell mit starker Bildverst\xe4ndnisf\xe4higkeit, das in der Lage ist, Bild- und Textinformationen zu verarbeiten und nach tiefem Denken Textinhalte zu generieren. Es zeigt herausragende Leistungen im Bereich der visuellen Schlussfolgerung und verf\xfcgt \xfcber erstklassige F\xe4higkeiten in Mathematik, Programmierung und Textschlussfolgerung. Die Kontextl\xe4nge betr\xe4gt 100k."},"step3":{"description":"Step3 ist ein multimodales Modell von StepStar mit leistungsstarken F\xe4higkeiten im visuellen Verst\xe4ndnis."},"stepfun-ai/step3":{"description":"Step3 ist ein wegweisendes multimodales Inferenzmodell, ver\xf6ffentlicht von StepFun (阶跃星辰). Es basiert auf einer Mixture-of-Experts-(MoE)-Architektur mit insgesamt 321 Milliarden Parametern und 38 Milliarden Aktivierungsparametern. Das Modell ist als End-to-End-System konzipiert, um die Decodierungskosten zu minimieren und gleichzeitig erstklassige Leistung bei visuell-sprachlicher Inferenz zu bieten. Durch die synergistische Kombination von Multi-Matrix-Factorization-Attention (MFA) und Attention-FFN-Dekopplung (AFD) erzielt Step3 sowohl auf High-End- als auch auf ressourcenbeschr\xe4nkten Beschleunigern hohe Effizienz. In der Vortrainingsphase verarbeitete Step3 mehr als 20 Billionen Text-Tokens und 4 Billionen multimodale (Bild‑Text) Tokens und deckt dabei \xfcber zehn Sprachen ab. Das Modell erzielt in zahlreichen Benchmarks — etwa in Mathematik, Programmierung und Multimodalit\xe4t — f\xfchrende Ergebnisse unter den Open‑Source‑Modellen."},"taichu_llm":{"description":"Das Zīdōng Taichu Sprachmodell verf\xfcgt \xfcber au\xdfergew\xf6hnliche Sprachverst\xe4ndnisf\xe4higkeiten sowie F\xe4higkeiten in Textgenerierung, Wissensabfrage, Programmierung, mathematischen Berechnungen, logischem Denken, Sentimentanalyse und Textzusammenfassung. Es kombiniert innovativ gro\xdfe Datenvortrainings mit reichhaltigem Wissen aus mehreren Quellen, verfeinert kontinuierlich die Algorithmen und absorbiert st\xe4ndig neues Wissen aus umfangreichen Textdaten in Bezug auf Vokabular, Struktur, Grammatik und Semantik, um die Leistung des Modells kontinuierlich zu verbessern. Es bietet den Nutzern bequemere Informationen und Dienstleistungen sowie ein intelligenteres Erlebnis."},"taichu_o1":{"description":"taichu_o1 ist ein neues gro\xdfes Schlussfolgerungsmodell, das durch multimodale Interaktion und verst\xe4rktes Lernen menschen\xe4hnliche Denkprozesse erm\xf6glicht, komplexe Entscheidungsfindungen unterst\xfctzt und dabei pr\xe4zise Ausgaben liefert, w\xe4hrend es die Denkpfade des Modells zeigt. Es eignet sich f\xfcr Szenarien wie strategische Analysen und tiefes Denken."},"taichu_vl":{"description":"Integriert F\xe4higkeiten wie Bildverst\xe4ndnis, Wissens\xfcbertragung und logische Attribution und zeigt herausragende Leistungen im Bereich der Bild-Text-Fragen."},"tencent/Hunyuan-A13B-Instruct":{"description":"Hunyuan-A13B-Instruct verf\xfcgt \xfcber 80 Milliarden Parameter, von denen 13 Milliarden aktiviert werden k\xf6nnen, um mit gr\xf6\xdferen Modellen zu konkurrieren. Es unterst\xfctzt eine hybride Denkweise aus „schnellem Denken/langsamem Denken“; die Verarbeitung langer Texte ist stabil; durch BFCL-v3 und τ-Bench validiert, \xfcbertrifft die Agentenf\xe4higkeit andere Modelle; in Kombination mit GQA und mehreren Quantisierungsformaten erm\xf6glicht es effiziente Inferenz."},"tencent/Hunyuan-MT-7B":{"description":"Das Hunyuan-\xdcbersetzungsmodell besteht aus dem \xdcbersetzungsmodell Hunyuan-MT-7B und dem integrierten Modell Hunyuan-MT-Chimera. Hunyuan-MT-7B ist ein leichtgewichtiges \xdcbersetzungsmodell mit 7 Milliarden Parametern, das Quelltexte in Zielsprache \xfcbersetzt. Es unterst\xfctzt \xdcbersetzungen zwischen 33 Sprachen sowie 5 chinesischen Minderheitensprachen. Beim internationalen WMT25-Maschinen\xfcbersetzungswettbewerb belegte Hunyuan-MT-7B in 30 von 31 teilnehmenden Sprachpaaren den ersten Platz und demonstrierte damit seine herausragende \xdcbersetzungsleistung. F\xfcr \xdcbersetzungsszenarien hat Tencent Hunyuan ein vollst\xe4ndiges Trainingsparadigma entwickelt – von Pretraining \xfcber \xfcberwachtes Fine-Tuning bis hin zu \xdcbersetzungsverst\xe4rkung und integrierter Optimierung – und damit branchenf\xfchrende Leistung bei vergleichbarer Modellgr\xf6\xdfe erreicht. Das Modell ist recheneffizient, leicht zu implementieren und f\xfcr vielf\xe4ltige Anwendungsszenarien geeignet."},"text-embedding-3-large":{"description":"Das leistungsst\xe4rkste Vektormodell, geeignet f\xfcr englische und nicht-englische Aufgaben."},"text-embedding-3-small":{"description":"Effizientes und kosteng\xfcnstiges neues Embedding-Modell, geeignet f\xfcr Wissensabruf, RAG-Anwendungen und andere Szenarien."},"thudm/glm-4-32b":{"description":"GLM-4-32B-0414 ist ein 32B zweisprachiges (Chinesisch-Englisch) offenes Gewicht Sprachmodell, das f\xfcr die Codegenerierung, Funktionsaufrufe und agentenbasierte Aufgaben optimiert wurde. Es wurde auf 15T hochwertigen und wiederholten Daten vortrainiert und weiter verfeinert durch menschliche Pr\xe4ferenzanpassung, Ablehnungs-Sampling und Verst\xe4rkungslernen. Das Modell zeigt hervorragende Leistungen bei komplexem Denken, Artefakterstellung und strukturierten Ausgaben und erreicht in mehreren Benchmark-Tests eine Leistung, die mit GPT-4o und DeepSeek-V3-0324 vergleichbar ist."},"thudm/glm-4-32b:free":{"description":"GLM-4-32B-0414 ist ein 32B zweisprachiges (Chinesisch-Englisch) offenes Gewicht Sprachmodell, das f\xfcr die Codegenerierung, Funktionsaufrufe und agentenbasierte Aufgaben optimiert wurde. Es wurde auf 15T hochwertigen und wiederholten Daten vortrainiert und weiter verfeinert durch menschliche Pr\xe4ferenzanpassung, Ablehnungs-Sampling und Verst\xe4rkungslernen. Das Modell zeigt hervorragende Leistungen bei komplexem Denken, Artefakterstellung und strukturierten Ausgaben und erreicht in mehreren Benchmark-Tests eine Leistung, die mit GPT-4o und DeepSeek-V3-0324 vergleichbar ist."},"thudm/glm-4-9b-chat":{"description":"Die Open-Source-Version des neuesten vortrainierten Modells der GLM-4-Serie, das von Zhizhu AI ver\xf6ffentlicht wurde."},"thudm/glm-z1-32b":{"description":"GLM-Z1-32B-0414 ist eine verbesserte Denkvariante von GLM-4-32B, die f\xfcr tiefgehende Mathematik, Logik und codeorientierte Probleml\xf6sungen entwickelt wurde. Es verwendet erweiterte Verst\xe4rkungslernen (aufgabenspezifisch und basierend auf allgemeinen Paarpr\xe4ferenzen), um die Leistung bei komplexen mehrstufigen Aufgaben zu verbessern. Im Vergleich zum Basis-GLM-4-32B-Modell hat Z1 die F\xe4higkeiten im strukturierten Denken und im formalen Bereich erheblich verbessert.\\n\\nDieses Modell unterst\xfctzt die Durchsetzung von \\"Denk\\"-Schritten durch Prompt-Engineering und bietet verbesserte Koh\xe4renz f\xfcr Ausgaben im Langformat. Es ist f\xfcr Agenten-Workflows optimiert und unterst\xfctzt langen Kontext (\xfcber YaRN), JSON-Toolaufrufe und feink\xf6rnige Sampling-Konfigurationen f\xfcr stabiles Denken. Besonders geeignet f\xfcr Anwendungsf\xe4lle, die durchdachtes, mehrstufiges Denken oder formale Ableitungen erfordern."},"thudm/glm-z1-rumination-32b":{"description":"THUDM: GLM Z1 Rumination 32B ist ein tiefes Inferenzmodell mit 32B Parametern aus der GLM-4-Z1-Serie, das f\xfcr komplexe, offene Aufgaben optimiert wurde, die langes Nachdenken erfordern. Es basiert auf glm-4-32b-0414 und hat zus\xe4tzliche Phasen des verst\xe4rkten Lernens und mehrstufige Ausrichtungsstrategien hinzugef\xfcgt, die die \\"Reflexions\\"-F\xe4higkeit einf\xfchren, die darauf abzielt, erweiterte kognitive Prozesse zu simulieren. Dazu geh\xf6ren iterative Inferenz, mehrstufige Analysen und werkzeuggest\xfctzte Arbeitsabl\xe4ufe wie Suche, Abruf und zitationsbewusste Synthese.\\n\\nDieses Modell zeigt hervorragende Leistungen in forschungsorientiertem Schreiben, vergleichender Analyse und komplexen Fragen und Antworten. Es unterst\xfctzt Funktionsaufrufe f\xfcr Such- und Navigationsprimitiven (`search`, `click`, `open`, `finish`), sodass es in agentenbasierten Pipelines verwendet werden kann. Reflexionsverhalten wird durch ein mehrstufiges Regelbelohnungssystem und verz\xf6gerte Entscheidungsmechanismen geformt und wird an tiefen Forschungsrahmen wie dem internen Ausrichtungsstapel von OpenAI gemessen. Diese Variante eignet sich f\xfcr Szenarien, die Tiefe statt Geschwindigkeit erfordern."},"tngtech/deepseek-r1t-chimera:free":{"description":"DeepSeek-R1T-Chimera wurde durch die Kombination von DeepSeek-R1 und DeepSeek-V3 (0324) erstellt und vereint die Inferenzf\xe4higkeiten von R1 mit den Verbesserungen der Token-Effizienz von V3. Es basiert auf der DeepSeek-MoE Transformer-Architektur und wurde f\xfcr allgemeine Textgenerierungsaufgaben optimiert.\\n\\nDieses Modell kombiniert die vortrainierten Gewichte der beiden Quellmodelle, um die Leistung in Inferenz, Effizienz und Befolgung von Anweisungen auszugleichen. Es wird unter der MIT-Lizenz ver\xf6ffentlicht und ist f\xfcr Forschungs- und kommerzielle Zwecke gedacht."},"togethercomputer/StripedHyena-Nous-7B":{"description":"StripedHyena Nous (7B) bietet durch effiziente Strategien und Modellarchitekturen verbesserte Rechenf\xe4higkeiten."},"tts-1":{"description":"Das neueste Text-zu-Sprache-Modell, optimiert f\xfcr Geschwindigkeit in Echtzeitszenarien."},"tts-1-hd":{"description":"Das neueste Text-zu-Sprache-Modell, optimiert f\xfcr Qualit\xe4t."},"upstage/SOLAR-10.7B-Instruct-v1.0":{"description":"Upstage SOLAR Instruct v1 (11B) eignet sich f\xfcr pr\xe4zise Anweisungsaufgaben und bietet hervorragende Sprachverarbeitungsf\xe4higkeiten."},"us.anthropic.claude-3-5-sonnet-20241022-v2:0":{"description":"Claude 3.5 Sonnet hebt den Branchenstandard an, \xfcbertrifft die Konkurrenzmodelle und Claude 3 Opus und zeigt in umfangreichen Bewertungen hervorragende Leistungen, w\xe4hrend es die Geschwindigkeit und Kosten unserer mittelgro\xdfen Modelle beibeh\xe4lt."},"us.anthropic.claude-3-7-sonnet-20250219-v1:0":{"description":"Claude 3.7 Sonett ist das schnellste n\xe4chste Modell von Anthropic. Im Vergleich zu Claude 3 Haiku hat Claude 3.7 Sonett in allen F\xe4higkeiten Verbesserungen erfahren und \xfcbertrifft in vielen intellektuellen Benchmark-Tests das gr\xf6\xdfte Modell der vorherigen Generation, Claude 3 Opus."},"us.anthropic.claude-haiku-4-5-20251001-v1:0":{"description":"Claude Haiku 4.5 ist das schnellste und intelligenteste Haiku-Modell von Anthropic mit blitzschneller Reaktionszeit und erweiterten Denkf\xe4higkeiten."},"us.anthropic.claude-sonnet-4-5-20250929-v1:0":{"description":"Claude Sonnet 4.5 ist das bisher intelligenteste Modell von Anthropic."},"v0-1.0-md":{"description":"Das Modell v0-1.0-md ist ein \xe4lteres Modell, das \xfcber die v0 API bereitgestellt wird"},"v0-1.5-lg":{"description":"Das Modell v0-1.5-lg eignet sich f\xfcr anspruchsvolle Denk- oder Schlussfolgerungsaufgaben"},"v0-1.5-md":{"description":"Das Modell v0-1.5-md ist f\xfcr allt\xe4gliche Aufgaben und die Generierung von Benutzeroberfl\xe4chen (UI) geeignet"},"vercel/v0-1.0-md":{"description":"Zugriff auf das Modell hinter v0 zur Generierung, Reparatur und Optimierung moderner Webanwendungen mit frameworkspezifischer Inferenz und aktuellem Wissen."},"vercel/v0-1.5-md":{"description":"Zugriff auf das Modell hinter v0 zur Generierung, Reparatur und Optimierung moderner Webanwendungen mit frameworkspezifischer Inferenz und aktuellem Wissen."},"wan2.2-t2i-flash":{"description":"Wanxiang 2.2 Turbo-Version, das aktuell neueste Modell. Es bietet umfassende Verbesserungen in Kreativit\xe4t, Stabilit\xe4t und realistischer Textur, erzeugt schnell und bietet ein hervorragendes Preis-Leistungs-Verh\xe4ltnis."},"wan2.2-t2i-plus":{"description":"Wanxiang 2.2 Professional-Version, das aktuell neueste Modell. Es bietet umfassende Verbesserungen in Kreativit\xe4t, Stabilit\xe4t und realistischer Textur mit reichhaltigen Details."},"wanx-v1":{"description":"Basis-Text-zu-Bild-Modell. Entspricht dem allgemeinen Modell 1.0 auf der offiziellen Tongyi Wanxiang Webseite."},"wanx2.0-t2i-turbo":{"description":"Spezialisiert auf realistische Portr\xe4ts, mittlere Geschwindigkeit und niedrige Kosten. Entspricht dem Turbo-Modell 2.0 auf der offiziellen Tongyi Wanxiang Webseite."},"wanx2.1-t2i-plus":{"description":"Vollst\xe4ndig aufger\xfcstete Version mit reichhaltigeren Bilddetails, etwas langsamer. Entspricht dem professionellen Modell 2.1 auf der offiziellen Tongyi Wanxiang Webseite."},"wanx2.1-t2i-turbo":{"description":"Vollst\xe4ndig aufger\xfcstete Version mit schneller Generierung, umfassender Leistung und hervorragendem Preis-Leistungs-Verh\xe4ltnis. Entspricht dem Turbo-Modell 2.1 auf der offiziellen Tongyi Wanxiang Webseite."},"whisper-1":{"description":"Universelles Spracherkennungsmodell, unterst\xfctzt mehrsprachige Spracherkennung, Sprach\xfcbersetzung und Spracherkennung."},"wizardlm2":{"description":"WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, mehrsprachigen Anwendungen, Schlussfolgerungen und intelligenten Assistenten besonders gut abschneidet."},"wizardlm2:8x22b":{"description":"WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, mehrsprachigen Anwendungen, Schlussfolgerungen und intelligenten Assistenten besonders gut abschneidet."},"x-ai/grok-4-fast":{"description":"Wir freuen uns, Grok 4 Fast vorzustellen – unseren neuesten Fortschritt im Bereich kosteneffizienter Inferenzmodelle."},"x-ai/grok-code-fast-1":{"description":"Wir freuen uns, grok-code-fast-1 zu pr\xe4sentieren – ein schnelles und kosteneffizientes Inferenzmodell mit hervorragender Leistung im Bereich Agenten-Codierung."},"x1":{"description":"Das Spark X1 Modell wird weiter verbessert und erreicht in allgemeinen Aufgaben wie Schlussfolgerungen, Textgenerierung und Sprachverst\xe4ndnis Ergebnisse, die mit OpenAI o1 und DeepSeek R1 vergleichbar sind, basierend auf der bereits f\xfchrenden Leistung in mathematischen Aufgaben."},"xai/grok-2":{"description":"Grok 2 ist ein fortschrittliches Sprachmodell mit modernsten Inferenzf\xe4higkeiten. Es bietet fortschrittliche F\xe4higkeiten in Chat, Codierung und Inferenz und \xfcbertrifft Claude 3.5 Sonnet und GPT-4-Turbo in der LMSYS-Rangliste."},"xai/grok-2-vision":{"description":"Das visuelle Modell Grok 2 zeigt hervorragende Leistungen bei visuellen Aufgaben und bietet modernste Leistung bei visueller mathematischer Inferenz (MathVista) und dokumentenbasierter Fragebeantwortung (DocVQA). Es kann verschiedene visuelle Informationen verarbeiten, darunter Dokumente, Diagramme, Grafiken, Screenshots und Fotos."},"xai/grok-3":{"description":"xAIs Flaggschiffmodell mit hervorragender Leistung bei Unternehmensanwendungen wie Datenerfassung, Codierung und Textzusammenfassung. Es verf\xfcgt \xfcber tiefes Fachwissen in den Bereichen Finanzen, Gesundheitswesen, Recht und Wissenschaft."},"xai/grok-3-fast":{"description":"xAIs Flaggschiffmodell mit hervorragender Leistung bei Unternehmensanwendungen wie Datenerfassung, Codierung und Textzusammenfassung. Die schnelle Modellvariante wird auf schnellerer Infrastruktur bereitgestellt und bietet deutlich schnellere Antwortzeiten. Die erh\xf6hte Geschwindigkeit geht mit h\xf6heren Kosten pro ausgegebenem Token einher."},"xai/grok-3-mini":{"description":"xAIs leichtgewichtiges Modell, das vor der Antwort nachdenkt. Ideal f\xfcr einfache oder logikbasierte Aufgaben ohne tiefes Fachwissen. Der urspr\xfcngliche Denkprozess ist zug\xe4nglich."},"xai/grok-3-mini-fast":{"description":"xAIs leichtgewichtiges Modell, das vor der Antwort nachdenkt. Ideal f\xfcr einfache oder logikbasierte Aufgaben ohne tiefes Fachwissen. Der urspr\xfcngliche Denkprozess ist zug\xe4nglich. Die schnelle Modellvariante wird auf schnellerer Infrastruktur bereitgestellt und bietet deutlich schnellere Antwortzeiten. Die erh\xf6hte Geschwindigkeit geht mit h\xf6heren Kosten pro ausgegebenem Token einher."},"xai/grok-4":{"description":"xAIs neuestes und bestes Flaggschiffmodell mit unvergleichlicher Leistung in nat\xfcrlicher Sprache, Mathematik und Inferenz – der perfekte Allrounder."},"yi-large":{"description":"Das brandneue Modell mit einer Billion Parametern bietet au\xdfergew\xf6hnliche Frage- und Textgenerierungsf\xe4higkeiten."},"yi-large-fc":{"description":"Basierend auf dem yi-large-Modell unterst\xfctzt und verst\xe4rkt es die F\xe4higkeit zu Werkzeugaufrufen und eignet sich f\xfcr verschiedene Gesch\xe4ftsszenarien, die den Aufbau von Agenten oder Workflows erfordern."},"yi-large-preview":{"description":"Fr\xfche Version, empfohlen wird die Verwendung von yi-large (neue Version)."},"yi-large-rag":{"description":"Ein fortgeschrittener Dienst, der auf dem leistungsstarken yi-large-Modell basiert und pr\xe4zise Antworten durch die Kombination von Abruf- und Generierungstechnologien bietet, sowie Echtzeit-Informationsdienste aus dem gesamten Web."},"yi-large-turbo":{"description":"Hervorragendes Preis-Leistungs-Verh\xe4ltnis und au\xdfergew\xf6hnliche Leistung. Hochpr\xe4zise Feinabstimmung basierend auf Leistung, Schlussfolgerungsgeschwindigkeit und Kosten."},"yi-lightning":{"description":"Das neueste Hochleistungsmodell, das hochwertige Ausgaben gew\xe4hrleistet und gleichzeitig die Schlussfolgerungsgeschwindigkeit erheblich verbessert."},"yi-lightning-lite":{"description":"Leichte Version, empfohlen wird die Verwendung von yi-lightning."},"yi-medium":{"description":"Mittelgro\xdfes Modell mit verbesserten Feinabstimmungen, ausgewogene F\xe4higkeiten und gutes Preis-Leistungs-Verh\xe4ltnis. Tiefgehende Optimierung der Anweisungsbefolgung."},"yi-medium-200k":{"description":"200K ultra-lange Kontextfenster bieten tiefes Verst\xe4ndnis und Generierungsf\xe4higkeiten f\xfcr lange Texte."},"yi-spark":{"description":"Klein und kompakt, ein leichtgewichtiges und schnelles Modell. Bietet verbesserte mathematische Berechnungs- und Programmierf\xe4higkeiten."},"yi-vision":{"description":"Modell f\xfcr komplexe visuelle Aufgaben, das hohe Leistungsf\xe4higkeit bei der Bildverarbeitung und -analyse bietet."},"yi-vision-v2":{"description":"Ein Modell f\xfcr komplexe visuelle Aufgaben, das leistungsstarke Verst\xe4ndnis- und Analysef\xe4higkeiten auf der Grundlage mehrerer Bilder bietet."},"z-ai/glm-4.6":{"description":"Das neueste Flaggschiffmodell von Zhipu, GLM-4.6, \xfcbertrifft seine Vorg\xe4nger deutlich in den Bereichen fortgeschrittenes Codieren, Verarbeitung langer Texte, logisches Schlie\xdfen und agentenbasierte F\xe4higkeiten."},"zai-org/GLM-4.5":{"description":"GLM-4.5 ist ein speziell f\xfcr Agentenanwendungen entwickeltes Basismodell mit Mixture-of-Experts-Architektur. Es ist tief optimiert f\xfcr Werkzeugaufrufe, Web-Browsing, Softwareentwicklung und Frontend-Programmierung und unterst\xfctzt nahtlos die Integration in Code-Agenten wie Claude Code und Roo Code. GLM-4.5 verwendet einen hybriden Inferenzmodus und ist f\xfcr komplexe Schlussfolgerungen sowie den Alltagsgebrauch geeignet."},"zai-org/GLM-4.5-Air":{"description":"GLM-4.5-Air ist ein speziell f\xfcr Agentenanwendungen entwickeltes Basismodell mit Mixture-of-Experts-Architektur. Es ist tief optimiert f\xfcr Werkzeugaufrufe, Web-Browsing, Softwareentwicklung und Frontend-Programmierung und unterst\xfctzt nahtlos die Integration in Code-Agenten wie Claude Code und Roo Code. GLM-4.5 verwendet einen hybriden Inferenzmodus und ist f\xfcr komplexe Schlussfolgerungen sowie den Alltagsgebrauch geeignet."},"zai-org/GLM-4.5V":{"description":"GLM-4.5V ist das neueste visuell-sprachliche Modell (VLM), das von Zhipu AI ver\xf6ffentlicht wurde. Das Modell basiert auf dem Flaggschiff-Textmodell GLM-4.5-Air mit insgesamt 106 Milliarden Parametern und 12 Milliarden Aktivierungsparametern und verwendet eine Mixture-of-Experts-(MoE)-Architektur. Es zielt darauf ab, bei geringeren Inferenzkosten herausragende Leistung zu erzielen. Technisch setzt es die Entwicklungslinie von GLM-4.1V-Thinking fort und f\xfchrt Innovationen wie die dreidimensionale Rotations-Positionskodierung (3D-RoPE) ein, wodurch die Wahrnehmung und das Schlie\xdfen \xfcber dreidimensionale Raumbeziehungen deutlich verbessert werden. Durch Optimierungen in den Phasen des Pre-Trainings, der \xfcberwachten Feinabstimmung und des Reinforcement Learnings ist das Modell in der Lage, verschiedene visuelle Inhalte wie Bilder, Videos und lange Dokumente zu verarbeiten; in 41 \xf6ffentlichen multimodalen Benchmarks erreichte es Spitzenwerte unter frei verf\xfcgbaren Modellen derselben Klasse. Zudem wurde ein \\"Denkmodus\\"-Schalter hinzugef\xfcgt, der es Nutzern erlaubt, flexibel zwischen schneller Reaktion und tiefgehendem Schlussfolgern zu w\xe4hlen, um Effizienz und Ergebnisqualit\xe4t auszubalancieren."},"zai-org/GLM-4.6":{"description":"Im Vergleich zu GLM-4.5 bringt GLM-4.6 mehrere wichtige Verbesserungen. Das Kontextfenster wurde von 128K auf 200K Tokens erweitert, wodurch das Modell komplexere Agentenaufgaben bew\xe4ltigen kann. Das Modell erzielte h\xf6here Werte in Code-Benchmark-Tests und zeigte in Anwendungen wie Claude Code, Cline, Roo Code und Kilo Code eine st\xe4rkere Leistung in realen Szenarien, einschlie\xdflich verbesserter Generierung visuell ansprechender Frontend-Seiten. GLM-4.6 zeigt eine deutliche Steigerung der Inferenzleistung und unterst\xfctzt die Nutzung von Werkzeugen w\xe4hrend der Inferenz, was zu einer st\xe4rkeren Gesamtkapazit\xe4t f\xfchrt. Es zeigt bessere Leistungen bei der Werkzeugnutzung und suchbasierten Agenten und l\xe4sst sich effektiver in Agentenframeworks integrieren. Im Bereich des Schreibens entspricht das Modell stilistisch und in der Lesbarkeit st\xe4rker menschlichen Pr\xe4ferenzen und verh\xe4lt sich in Rollenspielszenarien nat\xfcrlicher."},"zai/glm-4.5":{"description":"Die GLM-4.5 Modellreihe sind speziell f\xfcr Agenten entwickelte Basismodelle. Das Flaggschiff GLM-4.5 integriert 355 Milliarden Gesamtparameter (32 Milliarden aktiv) und vereint Inferenz-, Codierungs- und Agentenf\xe4higkeiten zur L\xf6sung komplexer Anwendungsanforderungen. Als hybrides Inferenzsystem bietet es zwei Betriebsmodi."},"zai/glm-4.5-air":{"description":"GLM-4.5 und GLM-4.5-Air sind unsere neuesten Flaggschiffmodelle, speziell als Basismodelle f\xfcr Agentenanwendungen entwickelt. Beide nutzen eine gemischte Expertenarchitektur (MoE). GLM-4.5 hat 355 Milliarden Gesamtparameter mit 32 Milliarden aktiven Parametern pro Vorw\xe4rtsdurchlauf, w\xe4hrend GLM-4.5-Air ein vereinfachtes Design mit 106 Milliarden Gesamtparametern und 12 Milliarden aktiven Parametern verwendet."},"zai/glm-4.5v":{"description":"GLM-4.5V basiert auf dem GLM-4.5-Air Basismodell, \xfcbernimmt bew\xe4hrte Techniken von GLM-4.1V-Thinking und skaliert effektiv mit einer leistungsstarken MoE-Architektur mit 106 Milliarden Parametern."}}')}}]);