"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[63665],{63665:e=>{e.exports=JSON.parse('{"01-ai/yi-1.5-34b-chat":{"description":"Zero One Everything, le dernier mod\xe8le de fine-tuning open source, avec 34 milliards de param\xe8tres, prend en charge divers sc\xe9narios de dialogue, avec des donn\xe9es d\'entra\xeenement de haute qualit\xe9, align\xe9es sur les pr\xe9f\xe9rences humaines."},"01-ai/yi-1.5-9b-chat":{"description":"Zero One Everything, le dernier mod\xe8le de fine-tuning open source, avec 9 milliards de param\xe8tres, prend en charge divers sc\xe9narios de dialogue, avec des donn\xe9es d\'entra\xeenement de haute qualit\xe9, align\xe9es sur les pr\xe9f\xe9rences humaines."},"360/deepseek-r1":{"description":"【Version d\xe9ploy\xe9e 360】DeepSeek-R1 utilise massivement des techniques d\'apprentissage par renforcement lors de la phase de post-formation, am\xe9liorant consid\xe9rablement la capacit\xe9 d\'inf\xe9rence du mod\xe8le avec tr\xe8s peu de donn\xe9es annot\xe9es. Ses performances dans des t\xe2ches telles que les math\xe9matiques, le code et le raisonnement en langage naturel rivalisent avec la version officielle d\'OpenAI o1."},"360gpt-pro":{"description":"360GPT Pro, en tant que membre important de la s\xe9rie de mod\xe8les AI de 360, r\xe9pond \xe0 des applications vari\xe9es de traitement de texte avec une efficacit\xe9 \xe9lev\xe9e, supportant la compr\xe9hension de longs textes et les dialogues multi-tours."},"360gpt-pro-trans":{"description":"Mod\xe8le d\xe9di\xe9 \xe0 la traduction, optimis\xe9 par un ajustement approfondi, offrant des r\xe9sultats de traduction de premier plan."},"360gpt-turbo":{"description":"360GPT Turbo offre de puissantes capacit\xe9s de calcul et de dialogue, avec une excellente compr\xe9hension s\xe9mantique et une efficacit\xe9 de g\xe9n\xe9ration, ce qui en fait une solution id\xe9ale pour les entreprises et les d\xe9veloppeurs."},"360gpt-turbo-responsibility-8k":{"description":"360GPT Turbo Responsibility 8K met l\'accent sur la s\xe9curit\xe9 s\xe9mantique et l\'orientation vers la responsabilit\xe9, con\xe7u pour des sc\xe9narios d\'application exigeant une s\xe9curit\xe9 de contenu \xe9lev\xe9e, garantissant l\'exactitude et la robustesse de l\'exp\xe9rience utilisateur."},"360gpt2-o1":{"description":"360gpt2-o1 utilise une recherche arborescente pour construire des cha\xeenes de pens\xe9e et introduit un m\xe9canisme de r\xe9flexion, entra\xeen\xe9 par apprentissage par renforcement, permettant au mod\xe8le d\'avoir des capacit\xe9s d\'auto-r\xe9flexion et de correction."},"360gpt2-pro":{"description":"360GPT2 Pro est un mod\xe8le avanc\xe9 de traitement du langage naturel lanc\xe9 par la soci\xe9t\xe9 360, offrant d\'excellentes capacit\xe9s de g\xe9n\xe9ration et de compr\xe9hension de texte, en particulier dans le domaine de la cr\xe9ation et de la g\xe9n\xe9ration."},"360zhinao2-o1":{"description":"Le mod\xe8le 360zhinao2-o1 utilise une recherche arborescente pour construire une cha\xeene de pens\xe9e et introduit un m\xe9canisme de r\xe9flexion, form\xe9 par apprentissage par renforcement, permettant au mod\xe8le d\'avoir la capacit\xe9 de r\xe9flexion et de correction autonome."},"4.0Ultra":{"description":"Spark4.0 Ultra est la version la plus puissante de la s\xe9rie de grands mod\xe8les Xinghuo, am\xe9liorant la compr\xe9hension et la capacit\xe9 de r\xe9sum\xe9 du contenu textuel tout en mettant \xe0 jour le lien de recherche en ligne. C\'est une solution compl\xe8te pour am\xe9liorer la productivit\xe9 au bureau et r\xe9pondre avec pr\xe9cision aux besoins, repr\xe9sentant un produit intelligent de premier plan dans l\'industrie."},"AnimeSharp":{"description":"AnimeSharp (\xe9galement connu sous le nom de \xab 4x‑AnimeSharp \xbb) est un mod\xe8le open source de super-r\xe9solution d\xe9velopp\xe9 par Kim2091, bas\xe9 sur l\'architecture ESRGAN, sp\xe9cialis\xe9 dans l\'agrandissement et l\'am\xe9lioration des images de style anime. Il a \xe9t\xe9 renomm\xe9 en f\xe9vrier 2022 \xe0 partir de \xab 4x-TextSharpV1 \xbb, initialement con\xe7u aussi pour les images de texte, mais ses performances ont \xe9t\xe9 largement optimis\xe9es pour le contenu anime."},"Baichuan2-Turbo":{"description":"Utilise une technologie d\'am\xe9lioration de recherche pour relier compl\xe8tement le grand mod\xe8le aux connaissances sectorielles et aux connaissances du web. Supporte le t\xe9l\xe9chargement de divers documents tels que PDF, Word, et l\'entr\xe9e d\'URL, permettant une acquisition d\'informations rapide et compl\xe8te, avec des r\xe9sultats pr\xe9cis et professionnels."},"Baichuan3-Turbo":{"description":"Optimis\xe9 pour des sc\xe9narios d\'entreprise \xe0 haute fr\xe9quence, avec des am\xe9liorations significatives et un excellent rapport qualit\xe9-prix. Par rapport au mod\xe8le Baichuan2, la cr\xe9ation de contenu a augment\xe9 de 20%, les questions-r\xe9ponses de 17%, et les capacit\xe9s de jeu de r\xf4le de 40%. Les performances globales surpassent celles de GPT-3.5."},"Baichuan3-Turbo-128k":{"description":"Dot\xe9 d\'une fen\xeatre de contexte ultra-longue de 128K, optimis\xe9 pour des sc\xe9narios d\'entreprise \xe0 haute fr\xe9quence, avec des am\xe9liorations significatives et un excellent rapport qualit\xe9-prix. Par rapport au mod\xe8le Baichuan2, la cr\xe9ation de contenu a augment\xe9 de 20%, les questions-r\xe9ponses de 17%, et les capacit\xe9s de jeu de r\xf4le de 40%. Les performances globales surpassent celles de GPT-3.5."},"Baichuan4":{"description":"Le mod\xe8le est le meilleur en Chine, surpassant les mod\xe8les \xe9trangers dans des t\xe2ches en chinois telles que l\'encyclop\xe9die, les longs textes et la cr\xe9ation. Il poss\xe8de \xe9galement des capacit\xe9s multimodales de pointe, avec d\'excellentes performances dans plusieurs \xe9valuations de r\xe9f\xe9rence."},"Baichuan4-Air":{"description":"Le mod\xe8le le plus performant en Chine, surpassant les mod\xe8les dominants \xe9trangers dans les t\xe2ches en chinois telles que les encyclop\xe9dies, les longs textes et la cr\xe9ation. Il poss\xe8de \xe9galement des capacit\xe9s multimodales de pointe, avec d\'excellentes performances dans plusieurs \xe9valuations de r\xe9f\xe9rence."},"Baichuan4-Turbo":{"description":"Le mod\xe8le le plus performant en Chine, surpassant les mod\xe8les dominants \xe9trangers dans les t\xe2ches en chinois telles que les encyclop\xe9dies, les longs textes et la cr\xe9ation. Il poss\xe8de \xe9galement des capacit\xe9s multimodales de pointe, avec d\'excellentes performances dans plusieurs \xe9valuations de r\xe9f\xe9rence."},"ByteDance-Seed/Seed-OSS-36B-Instruct":{"description":"Seed-OSS est une s\xe9rie de grands mod\xe8les de langage open source d\xe9velopp\xe9e par l\'\xe9quipe Seed de ByteDance, con\xe7ue pour une gestion puissante des contextes longs, le raisonnement, les agents intelligents et les capacit\xe9s g\xe9n\xe9rales. Le mod\xe8le Seed-OSS-36B-Instruct de cette s\xe9rie est un mod\xe8le affin\xe9 par instruction avec 36 milliards de param\xe8tres, prenant en charge nativement des contextes ultra-longs, ce qui lui permet de traiter en une seule fois d\'\xe9normes documents ou des bases de code complexes. Ce mod\xe8le est sp\xe9cialement optimis\xe9 pour le raisonnement, la g\xe9n\xe9ration de code et les t\xe2ches d\'agents (comme l\'utilisation d\'outils), tout en maintenant un \xe9quilibre et une excellente capacit\xe9 g\xe9n\xe9rale. Une caract\xe9ristique majeure de ce mod\xe8le est la fonction \xab budget de r\xe9flexion \xbb (Thinking Budget), qui permet aux utilisateurs d\'ajuster de mani\xe8re flexible la longueur du raisonnement selon leurs besoins, am\xe9liorant ainsi efficacement l\'efficacit\xe9 du raisonnement en application pratique."},"DeepSeek-R1":{"description":"LLM efficace \xe0 la pointe de la technologie, sp\xe9cialis\xe9 dans le raisonnement, les math\xe9matiques et la programmation."},"DeepSeek-R1-Distill-Llama-70B":{"description":"DeepSeek R1 - un mod\xe8le plus grand et plus intelligent dans la suite DeepSeek - a \xe9t\xe9 distill\xe9 dans l\'architecture Llama 70B. Bas\xe9 sur des tests de r\xe9f\xe9rence et des \xe9valuations humaines, ce mod\xe8le est plus intelligent que le Llama 70B d\'origine, en particulier dans les t\xe2ches n\xe9cessitant des math\xe9matiques et une pr\xe9cision factuelle."},"DeepSeek-R1-Distill-Qwen-1.5B":{"description":"Le mod\xe8le distill\xe9 DeepSeek-R1 bas\xe9 sur Qwen2.5-Math-1.5B optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source."},"DeepSeek-R1-Distill-Qwen-14B":{"description":"Le mod\xe8le distill\xe9 DeepSeek-R1 bas\xe9 sur Qwen2.5-14B optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source."},"DeepSeek-R1-Distill-Qwen-32B":{"description":"La s\xe9rie DeepSeek-R1 optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source, d\xe9passant le niveau d\'OpenAI-o1-mini."},"DeepSeek-R1-Distill-Qwen-7B":{"description":"Le mod\xe8le distill\xe9 DeepSeek-R1 bas\xe9 sur Qwen2.5-Math-7B optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source."},"DeepSeek-V3":{"description":"DeepSeek-V3 est un mod\xe8le MoE d\xe9velopp\xe9 en interne par la soci\xe9t\xe9 DeepSeek. Les performances de DeepSeek-V3 surpassent celles d\'autres mod\xe8les open source tels que Qwen2.5-72B et Llama-3.1-405B, et se mesurent \xe0 la performance des mod\xe8les ferm\xe9s de pointe au monde comme GPT-4o et Claude-3.5-Sonnet."},"DeepSeek-V3-1":{"description":"DeepSeek V3.1 : mod\xe8le d\'inf\xe9rence de nouvelle g\xe9n\xe9ration, am\xe9liorant les capacit\xe9s de raisonnement complexe et de r\xe9flexion en cha\xeene, adapt\xe9 aux t\xe2ches n\xe9cessitant une analyse approfondie."},"DeepSeek-V3-Fast":{"description":"Fournisseur du mod\xe8le : plateforme sophnet. DeepSeek V3 Fast est la version ultra-rapide \xe0 TPS \xe9lev\xe9 de DeepSeek V3 0324, enti\xe8rement non quantifi\xe9e, avec des capacit\xe9s de code et math\xe9matiques renforc\xe9es, offrant une r\xe9activit\xe9 accrue !"},"DeepSeek-V3.1":{"description":"DeepSeek-V3.1 - mode sans r\xe9flexion ; DeepSeek-V3.1 est un nouveau mod\xe8le de raisonnement hybride lanc\xe9 par DeepSeek, supportant deux modes de raisonnement : avec et sans r\xe9flexion, avec une efficacit\xe9 de r\xe9flexion sup\xe9rieure \xe0 celle de DeepSeek-R1-0528. Optimis\xe9 par post-entra\xeenement, l\'utilisation des outils Agent et les performances dans les t\xe2ches d\'agents ont \xe9t\xe9 grandement am\xe9lior\xe9es."},"DeepSeek-V3.1-Fast":{"description":"DeepSeek V3.1 Fast est la version ultra-rapide \xe0 TPS \xe9lev\xe9 de DeepSeek V3.1. Mode de r\xe9flexion hybride : en modifiant le mod\xe8le de conversation, un seul mod\xe8le peut supporter \xe0 la fois les modes avec et sans r\xe9flexion. Appel d\'outils plus intelligent : gr\xe2ce \xe0 l\'optimisation post-entra\xeenement, les performances du mod\xe8le dans l\'utilisation des outils et les t\xe2ches d\'agents sont significativement am\xe9lior\xe9es."},"DeepSeek-V3.1-Think":{"description":"DeepSeek-V3.1 - mode r\xe9flexion ; DeepSeek-V3.1 est un nouveau mod\xe8le de raisonnement hybride lanc\xe9 par DeepSeek, supportant deux modes de raisonnement : avec et sans r\xe9flexion, avec une efficacit\xe9 de r\xe9flexion sup\xe9rieure \xe0 celle de DeepSeek-R1-0528. Optimis\xe9 par post-entra\xeenement, l\'utilisation des outils Agent et les performances dans les t\xe2ches d\'agents ont \xe9t\xe9 grandement am\xe9lior\xe9es."},"DeepSeek-V3.2-Exp":{"description":"DeepSeek V3.2 est le dernier mod\xe8le universel publi\xe9 par DeepSeek, prenant en charge une architecture d\'inf\xe9rence hybride et offrant des capacit\xe9s d\'agent renforc\xe9es."},"DeepSeek-V3.2-Exp-Think":{"description":"Mode de r\xe9flexion DeepSeek V3.2. Avant de fournir la r\xe9ponse finale, le mod\xe8le g\xe9n\xe8re une cha\xeene de pens\xe9e pour am\xe9liorer la pr\xe9cision de la r\xe9ponse."},"Doubao-lite-128k":{"description":"Doubao-lite offre une vitesse de r\xe9ponse exceptionnelle et un excellent rapport qualit\xe9-prix, offrant aux clients une flexibilit\xe9 accrue pour diff\xe9rents sc\xe9narios. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 128k."},"Doubao-lite-32k":{"description":"Doubao-lite offre une vitesse de r\xe9ponse exceptionnelle et un excellent rapport qualit\xe9-prix, offrant aux clients une flexibilit\xe9 accrue pour diff\xe9rents sc\xe9narios. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 32k."},"Doubao-lite-4k":{"description":"Doubao-lite offre une vitesse de r\xe9ponse exceptionnelle et un excellent rapport qualit\xe9-prix, offrant aux clients une flexibilit\xe9 accrue pour diff\xe9rents sc\xe9narios. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 4k."},"Doubao-pro-128k":{"description":"Mod\xe8le principal le plus performant, adapt\xe9 aux t\xe2ches complexes, avec d\'excellents r\xe9sultats dans les domaines des questions-r\xe9ponses, r\xe9sum\xe9s, cr\xe9ation, classification de texte, jeu de r\xf4le, etc. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 128k."},"Doubao-pro-32k":{"description":"Mod\xe8le principal le plus performant, adapt\xe9 aux t\xe2ches complexes, avec d\'excellents r\xe9sultats dans les domaines des questions-r\xe9ponses, r\xe9sum\xe9s, cr\xe9ation, classification de texte, jeu de r\xf4le, etc. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 32k."},"Doubao-pro-4k":{"description":"Mod\xe8le principal le plus performant, adapt\xe9 aux t\xe2ches complexes, avec d\'excellents r\xe9sultats dans les domaines des questions-r\xe9ponses, r\xe9sum\xe9s, cr\xe9ation, classification de texte, jeu de r\xf4le, etc. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 4k."},"DreamO":{"description":"DreamO est un mod\xe8le open source de g\xe9n\xe9ration d\'images personnalis\xe9es d\xe9velopp\xe9 conjointement par ByteDance et l\'Universit\xe9 de P\xe9kin, visant \xe0 supporter la g\xe9n\xe9ration d\'images multit\xe2ches via une architecture unifi\xe9e. Il utilise une m\xe9thode de mod\xe9lisation combin\xe9e efficace, capable de g\xe9n\xe9rer des images hautement coh\xe9rentes et personnalis\xe9es selon plusieurs conditions sp\xe9cifi\xe9es par l\'utilisateur telles que l\'identit\xe9, le sujet, le style et l\'arri\xe8re-plan."},"ERNIE-3.5-128K":{"description":"Mod\xe8le de langage \xe0 grande \xe9chelle de pointe d\xe9velopp\xe9 par Baidu, couvrant une vaste quantit\xe9 de corpus en chinois et en anglais, avec de puissantes capacit\xe9s g\xe9n\xe9rales, capable de r\xe9pondre \xe0 la plupart des exigences en mati\xe8re de dialogue, de questions-r\xe9ponses, de cr\xe9ation de contenu et d\'applications de plugins ; prend en charge l\'int\xe9gration automatique avec le plugin de recherche Baidu, garantissant la pertinence des informations de r\xe9ponse."},"ERNIE-3.5-8K":{"description":"Mod\xe8le de langage \xe0 grande \xe9chelle de pointe d\xe9velopp\xe9 par Baidu, couvrant une vaste quantit\xe9 de corpus en chinois et en anglais, avec de puissantes capacit\xe9s g\xe9n\xe9rales, capable de r\xe9pondre \xe0 la plupart des exigences en mati\xe8re de dialogue, de questions-r\xe9ponses, de cr\xe9ation de contenu et d\'applications de plugins ; prend en charge l\'int\xe9gration automatique avec le plugin de recherche Baidu, garantissant la pertinence des informations de r\xe9ponse."},"ERNIE-3.5-8K-Preview":{"description":"Mod\xe8le de langage \xe0 grande \xe9chelle de pointe d\xe9velopp\xe9 par Baidu, couvrant une vaste quantit\xe9 de corpus en chinois et en anglais, avec de puissantes capacit\xe9s g\xe9n\xe9rales, capable de r\xe9pondre \xe0 la plupart des exigences en mati\xe8re de dialogue, de questions-r\xe9ponses, de cr\xe9ation de contenu et d\'applications de plugins ; prend en charge l\'int\xe9gration automatique avec le plugin de recherche Baidu, garantissant la pertinence des informations de r\xe9ponse."},"ERNIE-4.0-8K-Latest":{"description":"Mod\xe8le de langage ultra-large de premier plan d\xe9velopp\xe9 par Baidu, ayant r\xe9alis\xe9 une mise \xe0 niveau compl\xe8te des capacit\xe9s par rapport \xe0 ERNIE 3.5, largement applicable \xe0 des sc\xe9narios de t\xe2ches complexes dans divers domaines ; prend en charge l\'int\xe9gration automatique avec le plugin de recherche Baidu, garantissant l\'actualit\xe9 des informations de r\xe9ponse."},"ERNIE-4.0-8K-Preview":{"description":"Mod\xe8le de langage ultra-large de premier plan d\xe9velopp\xe9 par Baidu, ayant r\xe9alis\xe9 une mise \xe0 niveau compl\xe8te des capacit\xe9s par rapport \xe0 ERNIE 3.5, largement applicable \xe0 des sc\xe9narios de t\xe2ches complexes dans divers domaines ; prend en charge l\'int\xe9gration automatique avec le plugin de recherche Baidu, garantissant l\'actualit\xe9 des informations de r\xe9ponse."},"ERNIE-4.0-Turbo-8K-Latest":{"description":"Mod\xe8le linguistique ultramoderne et de grande taille auto-d\xe9velopp\xe9 par Baidu, avec d\'excellentes performances g\xe9n\xe9rales, largement applicable \xe0 divers sc\xe9narios de t\xe2ches complexes ; prend en charge la connexion automatique aux plugins de recherche Baidu pour assurer la pertinence des informations de r\xe9ponse. Par rapport \xe0 ERNIE 4.0, il affiche de meilleures performances."},"ERNIE-4.0-Turbo-8K-Preview":{"description":"Mod\xe8le de langage ultra-large de premier plan d\xe9velopp\xe9 par Baidu, offrant d\'excellentes performances globales, largement applicable \xe0 des sc\xe9narios de t\xe2ches complexes dans divers domaines ; prend en charge l\'int\xe9gration automatique avec le plugin de recherche Baidu, garantissant l\'actualit\xe9 des informations de r\xe9ponse. Par rapport \xe0 ERNIE 4.0, il offre de meilleures performances."},"ERNIE-Character-8K":{"description":"Mod\xe8le de langage pour sc\xe9narios verticaux d\xe9velopp\xe9 par Baidu, adapt\xe9 aux applications telles que les NPC de jeux, les dialogues de service client, et les jeux de r\xf4le, avec des styles de personnages plus distincts et coh\xe9rents, une meilleure capacit\xe9 \xe0 suivre les instructions et des performances d\'inf\xe9rence sup\xe9rieures."},"ERNIE-Lite-Pro-128K":{"description":"Mod\xe8le de langage l\xe9ger d\xe9velopp\xe9 par Baidu, alliant d\'excellentes performances du mod\xe8le et efficacit\xe9 d\'inf\xe9rence, offrant de meilleures performances que ERNIE Lite, adapt\xe9 \xe0 l\'inf\xe9rence sur des cartes d\'acc\xe9l\xe9ration AI \xe0 faible puissance de calcul."},"ERNIE-Speed-128K":{"description":"Mod\xe8le de langage haute performance d\xe9velopp\xe9 par Baidu, publi\xe9 en 2024, avec d\'excellentes capacit\xe9s g\xe9n\xe9rales, adapt\xe9 comme mod\xe8le de base pour un ajustement fin, permettant de mieux traiter les probl\xe8mes de sc\xe9narios sp\xe9cifiques, tout en offrant d\'excellentes performances d\'inf\xe9rence."},"ERNIE-Speed-Pro-128K":{"description":"Mod\xe8le de langage haute performance d\xe9velopp\xe9 par Baidu, publi\xe9 en 2024, avec d\'excellentes capacit\xe9s g\xe9n\xe9rales, offrant de meilleures performances que ERNIE Speed, adapt\xe9 comme mod\xe8le de base pour un ajustement fin, permettant de mieux traiter les probl\xe8mes de sc\xe9narios sp\xe9cifiques, tout en offrant d\'excellentes performances d\'inf\xe9rence."},"FLUX-1.1-pro":{"description":"FLUX.1.1 Pro"},"FLUX.1-Kontext-dev":{"description":"FLUX.1-Kontext-dev est un mod\xe8le multimodal de g\xe9n\xe9ration et d\'\xe9dition d\'images d\xe9velopp\xe9 par Black Forest Labs, bas\xe9 sur l\'architecture Rectified Flow Transformer, avec une \xe9chelle de 12 milliards de param\xe8tres. Il se concentre sur la g\xe9n\xe9ration, la reconstruction, l\'am\xe9lioration ou l\'\xe9dition d\'images sous conditions contextuelles donn\xe9es. Ce mod\xe8le combine les avantages de g\xe9n\xe9ration contr\xf4l\xe9e des mod\xe8les de diffusion et la capacit\xe9 de mod\xe9lisation contextuelle des Transformers, supportant une sortie d\'images de haute qualit\xe9, applicable \xe0 la restauration, au remplissage et \xe0 la reconstruction visuelle de sc\xe8nes."},"FLUX.1-Kontext-pro":{"description":"FLUX.1 Kontext [pro]"},"FLUX.1-dev":{"description":"FLUX.1-dev est un mod\xe8le open source multimodal de langage (Multimodal Language Model, MLLM) d\xe9velopp\xe9 par Black Forest Labs, optimis\xe9 pour les t\xe2ches texte-image, int\xe9grant la compr\xe9hension et la g\xe9n\xe9ration d\'images et de textes. Bas\xe9 sur des mod\xe8les de langage avanc\xe9s tels que Mistral-7B, il utilise un encodeur visuel soigneusement con\xe7u et un affinage par instructions en plusieurs \xe9tapes, permettant un traitement collaboratif texte-image et un raisonnement complexe."},"Gryphe/MythoMax-L2-13b":{"description":"MythoMax-L2 (13B) est un mod\xe8le innovant, adapt\xe9 \xe0 des applications dans plusieurs domaines et \xe0 des t\xe2ches complexes."},"HelloMeme":{"description":"HelloMeme est un outil d\'IA capable de g\xe9n\xe9rer automatiquement des m\xe8mes, GIFs ou courtes vid\xe9os \xe0 partir d\'images ou d\'actions fournies. Il ne n\xe9cessite aucune comp\xe9tence en dessin ou programmation, il suffit de fournir une image de r\xe9f\xe9rence pour cr\xe9er des contenus attrayants, amusants et coh\xe9rents en style."},"HiDream-I1-Full":{"description":"HiDream-E1-Full est un grand mod\xe8le open source d\'\xe9dition d\'images multimodales lanc\xe9 par HiDream.ai, bas\xe9 sur l\'architecture avanc\xe9e Diffusion Transformer et int\xe9grant une puissante capacit\xe9 de compr\xe9hension linguistique (int\xe9grant LLaMA 3.1-8B-Instruct). Il supporte la g\xe9n\xe9ration d\'images, le transfert de style, l\'\xe9dition locale et la red\xe9finition de contenu via des instructions en langage naturel, avec d\'excellentes capacit\xe9s de compr\xe9hension et d\'ex\xe9cution texte-image."},"HunyuanDiT-v1.2-Diffusers-Distilled":{"description":"hunyuandit-v1.2-distilled est un mod\xe8le l\xe9ger de g\xe9n\xe9ration d\'images \xe0 partir de texte, optimis\xe9 par distillation, capable de g\xe9n\xe9rer rapidement des images de haute qualit\xe9, particuli\xe8rement adapt\xe9 aux environnements \xe0 ressources limit\xe9es et aux t\xe2ches de g\xe9n\xe9ration en temps r\xe9el."},"InstantCharacter":{"description":"InstantCharacter est un mod\xe8le de g\xe9n\xe9ration de personnages personnalis\xe9s sans r\xe9glage (tuning-free) publi\xe9 par l\'\xe9quipe IA de Tencent en 2025, visant une g\xe9n\xe9ration coh\xe9rente et haute fid\xe9lit\xe9 de personnages \xe0 travers diff\xe9rents contextes. Ce mod\xe8le permet de mod\xe9liser un personnage \xe0 partir d\'une seule image de r\xe9f\xe9rence et de le transf\xe9rer de mani\xe8re flexible \xe0 divers styles, actions et arri\xe8re-plans."},"InternVL2-8B":{"description":"InternVL2-8B est un puissant mod\xe8le de langage visuel, prenant en charge le traitement multimodal d\'images et de textes, capable de reconna\xeetre avec pr\xe9cision le contenu des images et de g\xe9n\xe9rer des descriptions ou des r\xe9ponses pertinentes."},"InternVL2.5-26B":{"description":"InternVL2.5-26B est un puissant mod\xe8le de langage visuel, prenant en charge le traitement multimodal d\'images et de textes, capable de reconna\xeetre avec pr\xe9cision le contenu des images et de g\xe9n\xe9rer des descriptions ou des r\xe9ponses pertinentes."},"Kolors":{"description":"Kolors est un mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte d\xe9velopp\xe9 par l\'\xe9quipe Kolors de Kuaishou. Entra\xeen\xe9 sur des milliards de param\xe8tres, il excelle en qualit\xe9 visuelle, compr\xe9hension s\xe9mantique du chinois et rendu de texte."},"Kwai-Kolors/Kolors":{"description":"Kolors est un mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte \xe0 grande \xe9chelle bas\xe9 sur la diffusion latente, d\xe9velopp\xe9 par l\'\xe9quipe Kolors de Kuaishou. Entra\xeen\xe9 sur des milliards de paires texte-image, il pr\xe9sente des avantages significatifs en qualit\xe9 visuelle, pr\xe9cision s\xe9mantique complexe et rendu des caract\xe8res chinois et anglais. Il supporte les entr\xe9es en chinois et en anglais, avec une excellente compr\xe9hension et g\xe9n\xe9ration de contenus sp\xe9cifiques en chinois."},"Kwaipilot/KAT-Dev":{"description":"KAT-Dev (32B) est un mod\xe8le open source de 32 milliards de param\xe8tres sp\xe9cialement con\xe7u pour les t\xe2ches d\'ing\xe9nierie logicielle. Il a atteint un taux de r\xe9solution de 62,4 % sur le benchmark SWE-Bench Verified, se classant cinqui\xe8me parmi tous les mod\xe8les open source de diff\xe9rentes tailles. Ce mod\xe8le a \xe9t\xe9 optimis\xe9 \xe0 travers plusieurs \xe9tapes, notamment l\'entra\xeenement interm\xe9diaire, l\'ajustement supervis\xe9 (SFT) et l\'apprentissage par renforcement (RL), dans le but d\'offrir un soutien puissant pour des t\xe2ches de programmation complexes telles que la compl\xe9tion de code, la correction de bugs et la revue de code."},"Llama-3.2-11B-Vision-Instruct":{"description":"Excellentes capacit\xe9s de raisonnement d\'image sur des images haute r\xe9solution, adapt\xe9es aux applications de compr\xe9hension visuelle."},"Llama-3.2-90B-Vision-Instruct\\t":{"description":"Capacit\xe9s avanc\xe9es de raisonnement d\'image adapt\xe9es aux applications d\'agents de compr\xe9hension visuelle."},"Meta-Llama-3-3-70B-Instruct":{"description":"Llama 3.3 70B : mod\xe8le Transformer polyvalent, adapt\xe9 aux t\xe2ches de dialogue et de g\xe9n\xe9ration."},"Meta-Llama-3.1-405B-Instruct":{"description":"Mod\xe8le de texte optimis\xe9 pour les instructions de Llama 3.1, con\xe7u pour des cas d\'utilisation de dialogue multilingue, qui se distingue dans de nombreux mod\xe8les de chat open source et ferm\xe9s sur des benchmarks industriels courants."},"Meta-Llama-3.1-70B-Instruct":{"description":"Mod\xe8le de texte optimis\xe9 pour les instructions de Llama 3.1, con\xe7u pour des cas d\'utilisation de dialogue multilingue, qui se distingue dans de nombreux mod\xe8les de chat open source et ferm\xe9s sur des benchmarks industriels courants."},"Meta-Llama-3.1-8B-Instruct":{"description":"Mod\xe8le de texte optimis\xe9 pour les instructions de Llama 3.1, con\xe7u pour des cas d\'utilisation de dialogue multilingue, qui se distingue dans de nombreux mod\xe8les de chat open source et ferm\xe9s sur des benchmarks industriels courants."},"Meta-Llama-3.2-1B-Instruct":{"description":"Mod\xe8le de langage de petite taille \xe0 la pointe de la technologie, dot\xe9 de comp\xe9tences en compr\xe9hension linguistique, d\'excellentes capacit\xe9s de raisonnement et de g\xe9n\xe9ration de texte."},"Meta-Llama-3.2-3B-Instruct":{"description":"Mod\xe8le de langage de petite taille \xe0 la pointe de la technologie, dot\xe9 de comp\xe9tences en compr\xe9hension linguistique, d\'excellentes capacit\xe9s de raisonnement et de g\xe9n\xe9ration de texte."},"Meta-Llama-3.3-70B-Instruct":{"description":"Llama 3.3 est le mod\xe8le de langage open source multilingue le plus avanc\xe9 de la s\xe9rie Llama, offrant des performances comparables \xe0 celles d\'un mod\xe8le de 405B \xe0 un co\xfbt tr\xe8s faible. Bas\xe9 sur une architecture Transformer, il a \xe9t\xe9 am\xe9lior\xe9 en utilit\xe9 et en s\xe9curit\xe9 gr\xe2ce \xe0 un ajustement supervis\xe9 (SFT) et \xe0 un apprentissage par renforcement avec retour humain (RLHF). Sa version optimis\xe9e pour les instructions est sp\xe9cialement con\xe7ue pour les dialogues multilingues et surpasse de nombreux mod\xe8les de chat open source et ferm\xe9s sur plusieurs benchmarks industriels. La date limite des connaissances est d\xe9cembre 2023."},"Meta-Llama-4-Maverick-17B-128E-Instruct-FP8":{"description":"Llama 4 Maverick : mod\xe8le \xe0 grande \xe9chelle bas\xe9 sur le Mixture-of-Experts, offrant une strat\xe9gie d\'activation experte efficace pour des performances optimales en inf\xe9rence."},"MiniMax-M1":{"description":"Nouveau mod\xe8le d\'inf\xe9rence d\xe9velopp\xe9 en interne. Leader mondial : 80K cha\xeenes de raisonnement x 1M d\'entr\xe9es, des performances comparables aux meilleurs mod\xe8les internationaux."},"MiniMax-M2":{"description":"Con\xe7u sp\xe9cialement pour un codage efficace et les flux de travail des agents."},"MiniMax-Text-01":{"description":"Dans la s\xe9rie de mod\xe8les MiniMax-01, nous avons r\xe9alis\xe9 une innovation audacieuse : la premi\xe8re mise en œuvre \xe0 grande \xe9chelle d\'un m\xe9canisme d\'attention lin\xe9aire, rendant l\'architecture Transformer traditionnelle non plus le seul choix. Ce mod\xe8le poss\xe8de un nombre de param\xe8tres atteignant 456 milliards, avec 45,9 milliards d\'activations par instance. Les performances globales du mod\xe8le rivalisent avec celles des meilleurs mod\xe8les \xe9trangers, tout en \xe9tant capable de traiter efficacement un contexte mondial de 4 millions de tokens, soit 32 fois celui de GPT-4o et 20 fois celui de Claude-3.5-Sonnet."},"MiniMaxAI/MiniMax-M1-80k":{"description":"MiniMax-M1 est un mod\xe8le d\'inf\xe9rence \xe0 attention mixte \xe0 grande \xe9chelle avec poids open source, comptant 456 milliards de param\xe8tres, activant environ 45,9 milliards de param\xe8tres par token. Le mod\xe8le supporte nativement un contexte ultra-long de 1 million de tokens et, gr\xe2ce au m\xe9canisme d\'attention \xe9clair, r\xe9duit de 75 % les op\xe9rations en virgule flottante lors de t\xe2ches de g\xe9n\xe9ration de 100 000 tokens par rapport \xe0 DeepSeek R1. Par ailleurs, MiniMax-M1 utilise une architecture MoE (Experts Mixtes), combinant l\'algorithme CISPO et une conception d\'attention mixte pour un entra\xeenement efficace par apprentissage par renforcement, offrant des performances de pointe dans l\'inf\xe9rence sur longues entr\xe9es et les sc\xe9narios r\xe9els d\'ing\xe9nierie logicielle."},"MiniMaxAI/MiniMax-M2":{"description":"MiniMax-M2 red\xe9finit l\'efficacit\xe9 pour les agents intelligents. Il s\'agit d\'un mod\xe8le MoE compact, rapide et \xe9conomique, dot\xe9 de 230 milliards de param\xe8tres totaux et de 10 milliards de param\xe8tres actifs, con\xe7u pour offrir des performances de pointe dans les t\xe2ches de codage et d\'agents, tout en conservant une intelligence g\xe9n\xe9rale puissante. Avec seulement 10 milliards de param\xe8tres actifs, MiniMax-M2 offre des performances comparables \xe0 celles des mod\xe8les de grande taille, ce qui en fait un choix id\xe9al pour les applications \xe0 haute efficacit\xe9."},"Moonshot-Kimi-K2-Instruct":{"description":"Avec un total de 1 000 milliards de param\xe8tres et 32 milliards de param\xe8tres activ\xe9s, ce mod\xe8le non cognitif atteint un niveau de pointe en connaissances avanc\xe9es, math\xe9matiques et codage, excelling dans les t\xe2ches d\'agents g\xe9n\xe9raux. Optimis\xe9 pour les t\xe2ches d\'agents, il peut non seulement r\xe9pondre aux questions mais aussi agir. Id\xe9al pour les conversations improvis\xe9es, g\xe9n\xe9rales et les exp\xe9riences d\'agents, c\'est un mod\xe8le r\xe9flexe ne n\xe9cessitant pas de longues r\xe9flexions."},"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO":{"description":"Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) est un mod\xe8le d\'instructions de haute pr\xe9cision, adapt\xe9 aux calculs complexes."},"OmniConsistency":{"description":"OmniConsistency am\xe9liore la coh\xe9rence stylistique et la capacit\xe9 de g\xe9n\xe9ralisation dans les t\xe2ches image-\xe0-image en introduisant de grands Diffusion Transformers (DiTs) et des donn\xe9es stylis\xe9es appari\xe9es, \xe9vitant ainsi la d\xe9gradation du style."},"Phi-3-medium-128k-instruct":{"description":"M\xeame mod\xe8le Phi-3-medium, mais avec une taille de contexte plus grande pour RAG ou un prompt \xe0 quelques exemples."},"Phi-3-medium-4k-instruct":{"description":"Un mod\xe8le de 14 milliards de param\xe8tres, prouvant une meilleure qualit\xe9 que Phi-3-mini, avec un accent sur des donn\xe9es denses en raisonnement de haute qualit\xe9."},"Phi-3-mini-128k-instruct":{"description":"M\xeame mod\xe8le Phi-3-mini, mais avec une taille de contexte plus grande pour RAG ou un prompt \xe0 quelques exemples."},"Phi-3-mini-4k-instruct":{"description":"Le plus petit membre de la famille Phi-3. Optimis\xe9 pour la qualit\xe9 et la faible latence."},"Phi-3-small-128k-instruct":{"description":"M\xeame mod\xe8le Phi-3-small, mais avec une taille de contexte plus grande pour RAG ou un prompt \xe0 quelques exemples."},"Phi-3-small-8k-instruct":{"description":"Un mod\xe8le de 7 milliards de param\xe8tres, prouvant une meilleure qualit\xe9 que Phi-3-mini, avec un accent sur des donn\xe9es denses en raisonnement de haute qualit\xe9."},"Phi-3.5-mini-instruct":{"description":"Version am\xe9lior\xe9e du mod\xe8le Phi-3-mini."},"Phi-3.5-vision-instrust":{"description":"Version am\xe9lior\xe9e du mod\xe8le Phi-3-vision."},"Pro/Qwen/Qwen2-7B-Instruct":{"description":"Qwen2-7B-Instruct est un mod\xe8le de langage \xe0 grande \xe9chelle de la s\xe9rie Qwen2, avec une taille de param\xe8tre de 7B. Ce mod\xe8le est bas\xe9 sur l\'architecture Transformer, utilisant des fonctions d\'activation SwiGLU, des biais d\'attention QKV et des techniques d\'attention par groupe. Il est capable de traiter de grandes entr\xe9es. Ce mod\xe8le excelle dans la compr\xe9hension du langage, la g\xe9n\xe9ration, les capacit\xe9s multilingues, le codage, les math\xe9matiques et le raisonnement dans plusieurs tests de r\xe9f\xe9rence, surpassant la plupart des mod\xe8les open source et montrant une comp\xe9titivit\xe9 comparable \xe0 celle des mod\xe8les propri\xe9taires dans certaines t\xe2ches. Qwen2-7B-Instruct a montr\xe9 des performances significativement meilleures que Qwen1.5-7B-Chat dans plusieurs \xe9valuations."},"Pro/Qwen/Qwen2.5-7B-Instruct":{"description":"Qwen2.5-7B-Instruct est l\'un des derniers mod\xe8les de langage \xe0 grande \xe9chelle publi\xe9s par Alibaba Cloud. Ce mod\xe8le 7B pr\xe9sente des capacit\xe9s consid\xe9rablement am\xe9lior\xe9es dans des domaines tels que le codage et les math\xe9matiques. Le mod\xe8le offre \xe9galement un support multilingue, couvrant plus de 29 langues, y compris le chinois et l\'anglais. Il a montr\xe9 des am\xe9liorations significatives dans le suivi des instructions, la compr\xe9hension des donn\xe9es structur\xe9es et la g\xe9n\xe9ration de sorties structur\xe9es (en particulier JSON)."},"Pro/Qwen/Qwen2.5-Coder-7B-Instruct":{"description":"Qwen2.5-Coder-7B-Instruct est la derni\xe8re version de la s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle sp\xe9cifique au code publi\xe9e par Alibaba Cloud. Ce mod\xe8le, bas\xe9 sur Qwen2.5, a \xe9t\xe9 form\xe9 avec 55 trillions de tokens, am\xe9liorant consid\xe9rablement les capacit\xe9s de g\xe9n\xe9ration, de raisonnement et de correction de code. Il renforce non seulement les capacit\xe9s de codage, mais maintient \xe9galement des avantages en math\xe9matiques et en comp\xe9tences g\xe9n\xe9rales. Le mod\xe8le fournit une base plus compl\xe8te pour des applications pratiques telles que les agents de code."},"Pro/Qwen/Qwen2.5-VL-7B-Instruct":{"description":"Qwen2.5-VL est le nouveau membre de la s\xe9rie Qwen, dot\xe9 de puissantes capacit\xe9s de compr\xe9hension visuelle. Il peut analyser le texte, les graphiques et la mise en page dans les images, comprendre les vid\xe9os longues et capturer des \xe9v\xe9nements. Il est capable de raisonner, d\'utiliser des outils, de prendre en charge le positionnement d\'objets multiformats et de g\xe9n\xe9rer des sorties structur\xe9es. Il optimise la r\xe9solution dynamique et la fr\xe9quence d\'images pour la compr\xe9hension vid\xe9o, et am\xe9liore l\'efficacit\xe9 de l\'encodeur visuel."},"Pro/THUDM/GLM-4.1V-9B-Thinking":{"description":"GLM-4.1V-9B-Thinking est un mod\xe8le de langage visuel open source (VLM) publi\xe9 conjointement par Zhipu AI et le laboratoire KEG de l\'Universit\xe9 Tsinghua, con\xe7u pour traiter des t\xe2ches cognitives multimodales complexes. Ce mod\xe8le est bas\xe9 sur le mod\xe8le de base GLM-4-9B-0414 et int\xe8gre un m\xe9canisme de raisonnement \xab cha\xeene de pens\xe9e \xbb (Chain-of-Thought) ainsi qu\'une strat\xe9gie d\'apprentissage par renforcement, am\xe9liorant significativement ses capacit\xe9s de raisonnement intermodal et sa stabilit\xe9."},"Pro/THUDM/glm-4-9b-chat":{"description":"GLM-4-9B-Chat est la version open source de la s\xe9rie de mod\xe8les pr\xe9-entra\xeen\xe9s GLM-4 lanc\xe9e par Zhipu AI. Ce mod\xe8le excelle dans plusieurs domaines tels que la s\xe9mantique, les math\xe9matiques, le raisonnement, le code et les connaissances. En plus de prendre en charge des dialogues multi-tours, GLM-4-9B-Chat dispose \xe9galement de fonctionnalit\xe9s avanc\xe9es telles que la navigation sur le web, l\'ex\xe9cution de code, l\'appel d\'outils personnalis\xe9s (Function Call) et le raisonnement sur de longs textes. Le mod\xe8le prend en charge 26 langues, y compris le chinois, l\'anglais, le japonais, le cor\xe9en et l\'allemand. Dans plusieurs tests de r\xe9f\xe9rence, GLM-4-9B-Chat a montr\xe9 d\'excellentes performances, comme AlignBench-v2, MT-Bench, MMLU et C-Eval. Ce mod\xe8le prend en charge une longueur de contexte maximale de 128K, adapt\xe9 \xe0 la recherche acad\xe9mique et aux applications commerciales."},"Pro/deepseek-ai/DeepSeek-R1":{"description":"DeepSeek-R1 est un mod\xe8le d\'inf\xe9rence pilot\xe9 par l\'apprentissage par renforcement (RL), qui r\xe9sout les probl\xe8mes de r\xe9p\xe9tition et de lisibilit\xe9 dans le mod\xe8le. Avant le RL, DeepSeek-R1 a introduit des donn\xe9es de d\xe9marrage \xe0 froid, optimisant encore les performances d\'inf\xe9rence. Il se compare \xe0 OpenAI-o1 dans les t\xe2ches math\xe9matiques, de code et d\'inf\xe9rence, et am\xe9liore l\'ensemble des performances gr\xe2ce \xe0 des m\xe9thodes d\'entra\xeenement soigneusement con\xe7ues."},"Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B":{"description":"DeepSeek-R1-Distill-Qwen-7B est un mod\xe8le obtenu par distillation de connaissances \xe0 partir de Qwen2.5-Math-7B. Ce mod\xe8le a \xe9t\xe9 affin\xe9 \xe0 l\'aide de 800 000 \xe9chantillons s\xe9lectionn\xe9s g\xe9n\xe9r\xe9s par DeepSeek-R1, d\xe9montrant d\'excellentes capacit\xe9s de raisonnement. Il obtient des performances remarquables dans plusieurs benchmarks, atteignant une pr\xe9cision de 92,8 % sur MATH-500, un taux de r\xe9ussite de 55,5 % sur AIME 2024 et un score de 1189 sur CodeForces, montrant ainsi de solides comp\xe9tences en math\xe9matiques et en programmation pour un mod\xe8le de taille 7B."},"Pro/deepseek-ai/DeepSeek-V3":{"description":"DeepSeek-V3 est un mod\xe8le de langage \xe0 experts mixtes (MoE) avec 671 milliards de param\xe8tres, utilisant une attention potentielle multi-t\xeate (MLA) et une architecture DeepSeekMoE, combinant une strat\xe9gie d\'\xe9quilibrage de charge sans perte auxiliaire pour optimiser l\'efficacit\xe9 d\'inf\xe9rence et d\'entra\xeenement. Pr\xe9-entra\xeen\xe9 sur 14,8 billions de tokens de haute qualit\xe9, et affin\xe9 par supervision et apprentissage par renforcement, DeepSeek-V3 surpasse d\'autres mod\xe8les open source et se rapproche des mod\xe8les ferm\xe9s de premier plan."},"Pro/deepseek-ai/DeepSeek-V3.1-Terminus":{"description":"DeepSeek-V3.1-Terminus est une version mise \xe0 jour du mod\xe8le V3.1 publi\xe9e par DeepSeek, positionn\xe9e comme un grand mod\xe8le de langage hybride pour agents intelligents. Cette mise \xe0 jour conserve les capacit\xe9s originales du mod\xe8le tout en se concentrant sur la correction des probl\xe8mes signal\xe9s par les utilisateurs et l\'am\xe9lioration de la stabilit\xe9. Elle am\xe9liore significativement la coh\xe9rence linguistique, r\xe9duisant le m\xe9lange de chinois et d\'anglais ainsi que l\'apparition de caract\xe8res anormaux. Le mod\xe8le int\xe8gre un \xab mode r\xe9flexion \xbb (Thinking Mode) et un \xab mode non-r\xe9flexion \xbb (Non-thinking Mode), permettant aux utilisateurs de basculer facilement entre ces modes via des mod\xe8les de conversation adapt\xe9s \xe0 diff\xe9rentes t\xe2ches. En tant qu\'optimisation majeure, V3.1-Terminus renforce les performances des agents de code (Code Agent) et de recherche (Search Agent), rendant leur appel d\'outils et l\'ex\xe9cution de t\xe2ches complexes en plusieurs \xe9tapes plus fiables."},"Pro/deepseek-ai/DeepSeek-V3.2-Exp":{"description":"DeepSeek-V3.2-Exp est une version exp\xe9rimentale V3.2 publi\xe9e par DeepSeek, servant d\'\xe9tape interm\xe9diaire vers une architecture de nouvelle g\xe9n\xe9ration. Bas\xe9e sur la V3.1-Terminus, elle introduit le m\xe9canisme d\'attention clairsem\xe9e DeepSeek (DeepSeek Sparse Attention, DSA) afin d\'am\xe9liorer l\'efficacit\xe9 de l\'entra\xeenement et de l\'inf\xe9rence sur de longs contextes. Elle est sp\xe9cialement optimis\xe9e pour l\'appel d\'outils, la compr\xe9hension de longs documents et le raisonnement en plusieurs \xe9tapes. V3.2-Exp fait le lien entre la recherche et l\'industrialisation, id\xe9ale pour les utilisateurs souhaitant explorer une efficacit\xe9 de raisonnement accrue dans des sc\xe9narios \xe0 budget contextuel \xe9lev\xe9."},"Pro/moonshotai/Kimi-K2-Instruct-0905":{"description":"Kimi K2-Instruct-0905 est la version la plus r\xe9cente et la plus puissante de Kimi K2. Il s\'agit d\'un mod\xe8le linguistique de pointe \xe0 experts mixtes (MoE), avec un total de 1 000 milliards de param\xe8tres et 32 milliards de param\xe8tres activ\xe9s. Les principales caract\xe9ristiques de ce mod\xe8le incluent : une intelligence de codage d\'agents am\xe9lior\xe9e, d\xe9montrant des performances significatives dans les tests de r\xe9f\xe9rence publics et les t\xe2ches r\xe9elles d\'agents de codage ; une exp\xe9rience de codage frontale am\xe9lior\xe9e, avec des progr\xe8s tant en esth\xe9tique qu\'en praticit\xe9 pour la programmation frontale."},"QwQ-32B-Preview":{"description":"QwQ-32B-Preview est un mod\xe8le de traitement du langage naturel innovant, capable de g\xe9rer efficacement des t\xe2ches complexes de g\xe9n\xe9ration de dialogues et de compr\xe9hension contextuelle."},"Qwen/QVQ-72B-Preview":{"description":"QVQ-72B-Preview est un mod\xe8le de recherche d\xe9velopp\xe9 par l\'\xe9quipe Qwen, ax\xe9 sur les capacit\xe9s de raisonnement visuel, qui poss\xe8de des avantages uniques dans la compr\xe9hension de sc\xe8nes complexes et la r\xe9solution de probl\xe8mes math\xe9matiques li\xe9s \xe0 la vision."},"Qwen/QwQ-32B":{"description":"QwQ est le mod\xe8le d\'inf\xe9rence de la s\xe9rie Qwen. Compar\xe9 aux mod\xe8les d\'optimisation d\'instructions traditionnels, QwQ poss\xe8de des capacit\xe9s de r\xe9flexion et de raisonnement, permettant d\'obtenir des performances nettement am\xe9lior\xe9es dans les t\xe2ches en aval, en particulier pour r\xe9soudre des probl\xe8mes difficiles. QwQ-32B est un mod\xe8le d\'inf\xe9rence de taille moyenne, capable d\'obtenir des performances comp\xe9titives par rapport aux mod\xe8les d\'inf\xe9rence les plus avanc\xe9s (comme DeepSeek-R1, o1-mini). Ce mod\xe8le utilise des techniques telles que RoPE, SwiGLU, RMSNorm et Attention QKV bias, avec une architecture de r\xe9seau de 64 couches et 40 t\xeates d\'attention Q (dans l\'architecture GQA, KV est de 8)."},"Qwen/QwQ-32B-Preview":{"description":"QwQ-32B-Preview est le dernier mod\xe8le de recherche exp\xe9rimental de Qwen, ax\xe9 sur l\'am\xe9lioration des capacit\xe9s de raisonnement de l\'IA. En explorant des m\xe9canismes complexes tels que le m\xe9lange de langues et le raisonnement r\xe9cursif, ses principaux avantages incluent de puissantes capacit\xe9s d\'analyse de raisonnement, ainsi que des comp\xe9tences en math\xe9matiques et en programmation. Cependant, il existe \xe9galement des probl\xe8mes de changement de langue, des cycles de raisonnement, des consid\xe9rations de s\xe9curit\xe9 et des diff\xe9rences dans d\'autres capacit\xe9s."},"Qwen/Qwen-Image":{"description":"Qwen-Image est un mod\xe8le de base de g\xe9n\xe9ration d’images d\xe9velopp\xe9 par l’\xe9quipe Qwen d’Alibaba, dot\xe9 de 20 milliards de param\xe8tres. Ce mod\xe8le a r\xe9alis\xe9 des avanc\xe9es significatives dans le rendu complexe de texte et l’\xe9dition d’image de haute pr\xe9cision, avec une capacit\xe9 remarquable \xe0 g\xe9n\xe9rer des images contenant du texte en chinois et en anglais avec une grande fid\xe9lit\xe9. Qwen-Image g\xe8re non seulement la mise en page sur plusieurs lignes et les textes de niveau paragraphe, mais maintient \xe9galement la coh\xe9rence typographique et l’harmonie contextuelle lors de la g\xe9n\xe9ration d’images. En plus de ses performances exceptionnelles en rendu de texte, le mod\xe8le prend en charge une large gamme de styles artistiques, allant de la photographie r\xe9aliste \xe0 l’esth\xe9tique anime, s’adaptant avec souplesse \xe0 divers besoins cr\xe9atifs. Il dispose \xe9galement de puissantes capacit\xe9s d’\xe9dition et de compr\xe9hension d’images, permettant des op\xe9rations avanc\xe9es telles que le transfert de style, l’ajout ou la suppression d’objets, l’am\xe9lioration des d\xe9tails, l’\xe9dition de texte et m\xeame le contr\xf4le de la posture humaine. L’objectif est d’en faire un mod\xe8le de base intelligent et polyvalent pour la cr\xe9ation et le traitement visuel, int\xe9grant langage, mise en page et image."},"Qwen/Qwen-Image-Edit-2509":{"description":"Qwen-Image-Edit-2509 est la derni\xe8re version d’\xe9dition d’image du mod\xe8le Qwen-Image, d\xe9velopp\xe9e par l’\xe9quipe Qwen d’Alibaba. Ce mod\xe8le repose sur les 20 milliards de param\xe8tres de Qwen-Image et a \xe9t\xe9 entra\xeen\xe9 en profondeur pour \xe9tendre ses capacit\xe9s uniques de rendu de texte au domaine de l’\xe9dition d’image, permettant une modification pr\xe9cise du texte dans les images. Qwen-Image-Edit adopte une architecture innovante, envoyant l’image d’entr\xe9e simultan\xe9ment \xe0 Qwen2.5-VL (pour le contr\xf4le s\xe9mantique visuel) et \xe0 un encodeur VAE (pour le contr\xf4le de l’apparence visuelle), offrant ainsi une double capacit\xe9 d’\xe9dition s\xe9mantique et visuelle. Cela signifie qu’il prend en charge non seulement les modifications locales de l’apparence telles que l’ajout, la suppression ou la modification d’\xe9l\xe9ments, mais aussi des \xe9ditions s\xe9mantiques avanc\xe9es n\xe9cessitant une coh\xe9rence conceptuelle, comme la cr\xe9ation d’IP ou le transfert de style. Le mod\xe8le a d\xe9montr\xe9 des performances de pointe (SOTA) sur plusieurs benchmarks publics, en faisant un mod\xe8le de base puissant pour l’\xe9dition d’image."},"Qwen/Qwen2-72B-Instruct":{"description":"Qwen2 est un mod\xe8le de langage g\xe9n\xe9ral avanc\xe9, prenant en charge divers types d\'instructions."},"Qwen/Qwen2-7B-Instruct":{"description":"Qwen2-72B-Instruct est un mod\xe8le de langage \xe0 grande \xe9chelle de la s\xe9rie Qwen2, avec une taille de param\xe8tre de 72B. Ce mod\xe8le est bas\xe9 sur l\'architecture Transformer, utilisant des fonctions d\'activation SwiGLU, des biais d\'attention QKV et des techniques d\'attention par groupe. Il est capable de traiter de grandes entr\xe9es. Ce mod\xe8le excelle dans la compr\xe9hension du langage, la g\xe9n\xe9ration, les capacit\xe9s multilingues, le codage, les math\xe9matiques et le raisonnement dans plusieurs tests de r\xe9f\xe9rence, surpassant la plupart des mod\xe8les open source et montrant une comp\xe9titivit\xe9 comparable \xe0 celle des mod\xe8les propri\xe9taires dans certaines t\xe2ches."},"Qwen/Qwen2-VL-72B-Instruct":{"description":"Qwen2-VL est la derni\xe8re it\xe9ration du mod\xe8le Qwen-VL, atteignant des performances de pointe dans les tests de r\xe9f\xe9rence de compr\xe9hension visuelle."},"Qwen/Qwen2.5-14B-Instruct":{"description":"Qwen2.5 est une toute nouvelle s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle, con\xe7ue pour optimiser le traitement des t\xe2ches d\'instruction."},"Qwen/Qwen2.5-32B-Instruct":{"description":"Qwen2.5 est une toute nouvelle s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle, con\xe7ue pour optimiser le traitement des t\xe2ches d\'instruction."},"Qwen/Qwen2.5-72B-Instruct":{"description":"Un grand mod\xe8le de langage d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba Cloud"},"Qwen/Qwen2.5-72B-Instruct-128K":{"description":"Qwen2.5 est une toute nouvelle s\xe9rie de mod\xe8les de langage de grande taille avec des capacit\xe9s de compr\xe9hension et de g\xe9n\xe9ration am\xe9lior\xe9es."},"Qwen/Qwen2.5-72B-Instruct-Turbo":{"description":"Qwen2.5 est une toute nouvelle s\xe9rie de mod\xe8les de langage de grande taille, con\xe7ue pour optimiser le traitement des t\xe2ches d\'instruction."},"Qwen/Qwen2.5-7B-Instruct":{"description":"Qwen2.5 est une toute nouvelle s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle, con\xe7ue pour optimiser le traitement des t\xe2ches d\'instruction."},"Qwen/Qwen2.5-7B-Instruct-Turbo":{"description":"Qwen2.5 est une toute nouvelle s\xe9rie de mod\xe8les de langage de grande taille, con\xe7ue pour optimiser le traitement des t\xe2ches d\'instruction."},"Qwen/Qwen2.5-Coder-32B-Instruct":{"description":"Qwen2.5-Coder se concentre sur la r\xe9daction de code."},"Qwen/Qwen2.5-Coder-7B-Instruct":{"description":"Qwen2.5-Coder-7B-Instruct est la derni\xe8re version de la s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle sp\xe9cifique au code publi\xe9e par Alibaba Cloud. Ce mod\xe8le, bas\xe9 sur Qwen2.5, a \xe9t\xe9 form\xe9 avec 55 trillions de tokens, am\xe9liorant consid\xe9rablement les capacit\xe9s de g\xe9n\xe9ration, de raisonnement et de correction de code. Il renforce non seulement les capacit\xe9s de codage, mais maintient \xe9galement des avantages en math\xe9matiques et en comp\xe9tences g\xe9n\xe9rales. Le mod\xe8le fournit une base plus compl\xe8te pour des applications pratiques telles que les agents de code."},"Qwen/Qwen2.5-VL-32B-Instruct":{"description":"Qwen2.5-VL-32B-Instruct est un mod\xe8le multimodal avanc\xe9 d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen, faisant partie de la s\xe9rie Qwen2.5-VL. Ce mod\xe8le excelle non seulement dans la reconnaissance d\'objets courants, mais aussi dans l\'analyse de textes, diagrammes, ic\xf4nes, graphiques et mises en page contenus dans des images. Il peut fonctionner comme un agent visuel intelligent capable de raisonner et de manipuler dynamiquement des outils, avec des comp\xe9tences d\'utilisation d\'ordinateurs et de smartphones. De plus, ce mod\xe8le peut localiser avec pr\xe9cision des objets dans des images et produire des sorties structur\xe9es pour des documents tels que des factures ou des tableaux. Par rapport \xe0 son pr\xe9d\xe9cesseur Qwen2-VL, cette version pr\xe9sente des am\xe9liorations significatives en math\xe9matiques et en r\xe9solution de probl\xe8mes gr\xe2ce \xe0 l\'apprentissage par renforcement, tout en adoptant un style de r\xe9ponse plus conforme aux pr\xe9f\xe9rences humaines."},"Qwen/Qwen2.5-VL-72B-Instruct":{"description":"Qwen2.5-VL est le mod\xe8le de langage visuel de la s\xe9rie Qwen2.5. Ce mod\xe8le pr\xe9sente des am\xe9liorations significatives \xe0 plusieurs \xe9gards : il poss\xe8de une meilleure compr\xe9hension visuelle, capable de reconna\xeetre des objets courants, d\'analyser du texte, des graphiques et des mises en page ; en tant qu\'agent visuel, il peut raisonner et guider dynamiquement l\'utilisation d\'outils ; il prend en charge la compr\xe9hension de vid\xe9os longues de plus d\'une heure et capture les \xe9v\xe9nements cl\xe9s ; il peut localiser avec pr\xe9cision des objets dans une image en g\xe9n\xe9rant des cadres de d\xe9limitation ou des points ; il prend en charge la g\xe9n\xe9ration de sorties structur\xe9es, particuli\xe8rement adapt\xe9e aux donn\xe9es scann\xe9es comme les factures et les tableaux."},"Qwen/Qwen3-14B":{"description":"Qwen3 est un nouveau mod\xe8le de Tongyi Qianwen avec des capacit\xe9s consid\xe9rablement am\xe9lior\xe9es, atteignant des niveaux de pointe dans plusieurs comp\xe9tences cl\xe9s telles que le raisonnement, l\'agent et le multilingue, et prenant en charge le changement de mode de pens\xe9e."},"Qwen/Qwen3-235B-A22B":{"description":"Qwen3 est un nouveau mod\xe8le de Tongyi Qianwen avec des capacit\xe9s consid\xe9rablement am\xe9lior\xe9es, atteignant des niveaux de pointe dans plusieurs comp\xe9tences cl\xe9s telles que le raisonnement, l\'agent et le multilingue, et prenant en charge le changement de mode de pens\xe9e."},"Qwen/Qwen3-235B-A22B-Instruct-2507":{"description":"Qwen3-235B-A22B-Instruct-2507 est un mod\xe8le de langage \xe0 experts mixtes (MoE) phare de la s\xe9rie Qwen3 d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Aliyun. Avec 235 milliards de param\xe8tres totaux et 22 milliards activ\xe9s par inf\xe9rence, il est une version mise \xe0 jour du mode non cognitif Qwen3-235B-A22B, am\xe9liorant significativement l\'adh\xe9rence aux instructions, le raisonnement logique, la compr\xe9hension textuelle, les math\xe9matiques, les sciences, la programmation et l\'utilisation d\'outils. Le mod\xe8le \xe9tend aussi la couverture des connaissances multilingues rares et s\'aligne mieux sur les pr\xe9f\xe9rences utilisateur pour des t\xe2ches subjectives et ouvertes, g\xe9n\xe9rant des textes plus utiles et de meilleure qualit\xe9."},"Qwen/Qwen3-235B-A22B-Thinking-2507":{"description":"Qwen3-235B-A22B-Thinking-2507 est un mod\xe8le de langage volumineux de la s\xe9rie Qwen3 d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba, sp\xe9cialis\xe9 dans les t\xe2ches complexes de raisonnement avanc\xe9. Bas\xe9 sur une architecture MoE, il compte 235 milliards de param\xe8tres totaux avec environ 22 milliards activ\xe9s par token, optimisant ainsi l\'efficacit\xe9 de calcul tout en maintenant une puissance \xe9lev\xe9e. En tant que mod\xe8le \xab de r\xe9flexion \xbb, il excelle dans le raisonnement logique, les math\xe9matiques, les sciences, la programmation et les tests acad\xe9miques n\xe9cessitant une expertise humaine, atteignant un niveau de pointe parmi les mod\xe8les open source de r\xe9flexion. Il am\xe9liore \xe9galement les capacit\xe9s g\xe9n\xe9rales telles que l\'adh\xe9rence aux instructions, l\'utilisation d\'outils et la g\xe9n\xe9ration de texte, avec un support natif pour une compr\xe9hension de contexte longue de 256K tokens, id\xe9al pour les sc\xe9narios n\xe9cessitant un raisonnement profond et le traitement de longs documents."},"Qwen/Qwen3-30B-A3B":{"description":"Qwen3 est un nouveau mod\xe8le de Tongyi Qianwen avec des capacit\xe9s consid\xe9rablement am\xe9lior\xe9es, atteignant des niveaux de pointe dans plusieurs comp\xe9tences cl\xe9s telles que le raisonnement, l\'agent et le multilingue, et prenant en charge le changement de mode de pens\xe9e."},"Qwen/Qwen3-30B-A3B-Instruct-2507":{"description":"Qwen3-30B-A3B-Instruct-2507 est une version mise \xe0 jour du mod\xe8le non r\xe9flexif Qwen3-30B-A3B. Il s\'agit d\'un mod\xe8le d\'experts mixtes (MoE) avec un total de 30,5 milliards de param\xe8tres et 3,3 milliards de param\xe8tres activ\xe9s. Ce mod\xe8le pr\xe9sente des am\xe9liorations cl\xe9s dans plusieurs domaines, notamment une am\xe9lioration significative de la conformit\xe9 aux instructions, du raisonnement logique, de la compr\xe9hension du texte, des math\xe9matiques, des sciences, du codage et de l\'utilisation des outils. Par ailleurs, il r\xe9alise des progr\xe8s substantiels dans la couverture des connaissances multilingues \xe0 longue tra\xeene et s\'aligne mieux avec les pr\xe9f\xe9rences des utilisateurs dans les t\xe2ches subjectives et ouvertes, ce qui lui permet de g\xe9n\xe9rer des r\xe9ponses plus utiles et des textes de meilleure qualit\xe9. De plus, sa capacit\xe9 de compr\xe9hension des textes longs a \xe9t\xe9 \xe9tendue \xe0 256K. Ce mod\xe8le ne prend en charge que le mode non r\xe9flexif et ne g\xe9n\xe8re pas de balises `<think></think>` dans ses sorties."},"Qwen/Qwen3-30B-A3B-Thinking-2507":{"description":"Qwen3-30B-A3B-Thinking-2507 est le dernier mod\xe8le de \xab r\xe9flexion \xbb de la s\xe9rie Qwen3 publi\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba. En tant que mod\xe8le Mixture-of-Experts (MoE) comptant 30,5 milliards de param\xe8tres au total et 3,3 milliards de param\xe8tres d\'activation, il est ax\xe9 sur l\'am\xe9lioration des capacit\xe9s de traitement des t\xe2ches complexes. Le mod\xe8le pr\xe9sente des gains de performance significatifs sur des benchmarks acad\xe9miques en raisonnement logique, math\xe9matiques, sciences, programmation et autres t\xe2ches requ\xe9rant une expertise humaine. Parall\xe8lement, ses capacit\xe9s g\xe9n\xe9rales — respect des instructions, utilisation d\'outils, g\xe9n\xe9ration de texte et alignement sur les pr\xe9f\xe9rences humaines — ont \xe9t\xe9 nettement renforc\xe9es. Il prend nativement en charge une compr\xe9hension de contextes longs de 256K tokens, extensible jusqu\'\xe0 1 million de tokens. Cette version, con\xe7ue pour le \xab mode r\xe9flexion \xbb, vise \xe0 r\xe9soudre des t\xe2ches hautement complexes via un raisonnement d\xe9taill\xe9 pas \xe0 pas ; ses capacit\xe9s d\'agent sont \xe9galement remarquables."},"Qwen/Qwen3-32B":{"description":"Qwen3 est un nouveau mod\xe8le de Tongyi Qianwen avec des capacit\xe9s consid\xe9rablement am\xe9lior\xe9es, atteignant des niveaux de pointe dans plusieurs comp\xe9tences cl\xe9s telles que le raisonnement, l\'agent et le multilingue, et prenant en charge le changement de mode de pens\xe9e."},"Qwen/Qwen3-8B":{"description":"Qwen3 est un nouveau mod\xe8le de Tongyi Qianwen avec des capacit\xe9s consid\xe9rablement am\xe9lior\xe9es, atteignant des niveaux de pointe dans plusieurs comp\xe9tences cl\xe9s telles que le raisonnement, l\'agent et le multilingue, et prenant en charge le changement de mode de pens\xe9e."},"Qwen/Qwen3-Coder-30B-A3B-Instruct":{"description":"Qwen3-Coder-30B-A3B-Instruct est un mod\xe8le de code de la s\xe9rie Qwen3 d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba. En tant que mod\xe8le \xe9pur\xe9 et optimis\xe9, il se concentre sur l\'am\xe9lioration des capacit\xe9s de traitement du code tout en conservant des performances et une grande efficacit\xe9. Ce mod\xe8le affiche un avantage de performance notable parmi les mod\xe8les open source pour des t\xe2ches complexes telles que la programmation agentique (Agentic Coding), l\'automatisation de navigateurs et l\'appel d\'outils. Il prend en charge nativement un contexte long de 256K tokens et peut \xeatre \xe9tendu jusqu\'\xe0 1M tokens, permettant une meilleure compr\xe9hension et gestion des bases de code \xe0 l\'\xe9chelle du d\xe9p\xf4t. De plus, ce mod\xe8le fournit un solide support d\'encodage par agents pour des plateformes comme Qwen Code et CLINE, et int\xe8gre un format d\xe9di\xe9 d\'appel de fonctions."},"Qwen/Qwen3-Coder-480B-A35B-Instruct":{"description":"Qwen3-Coder-480B-A35B-Instruct est un mod\xe8le de code publi\xe9 par Alibaba, et \xe0 ce jour le plus avanc\xe9 en termes de capacit\xe9s d\'agent (agentic). Il s\'agit d\'un mod\xe8le MoE (Mixture-of-Experts) disposant de 480 milliards de param\xe8tres au total et de 35 milliards de param\xe8tres activ\xe9s, offrant un \xe9quilibre entre efficacit\xe9 et performance. Le mod\xe8le prend en charge nativement une longueur de contexte de 256K (environ 260 000) tokens et peut \xeatre \xe9tendu jusqu\'\xe0 1 million de tokens via des m\xe9thodes d\'extrapolation telles que YaRN, ce qui lui permet de traiter de vastes bases de code et des t\xe2ches de programmation complexes. Qwen3-Coder a \xe9t\xe9 con\xe7u pour des flux de travail de codage pilot\xe9s par des agents : il ne se contente pas de g\xe9n\xe9rer du code, il peut aussi interagir de mani\xe8re autonome avec les outils et environnements de d\xe9veloppement pour r\xe9soudre des probl\xe8mes de programmation complexes. Sur plusieurs benchmarks de codage et de t\xe2ches agent, ce mod\xe8le atteint un niveau de premier plan parmi les mod\xe8les open source, ses performances rivalisant avec celles de mod\xe8les de pointe comme Claude Sonnet 4."},"Qwen/Qwen3-Next-80B-A3B-Instruct":{"description":"Qwen3-Next-80B-A3B-Instruct est un mod\xe8le de base de nouvelle g\xe9n\xe9ration publi\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba. Il est bas\xe9 sur la toute nouvelle architecture Qwen3-Next, visant \xe0 atteindre une efficacit\xe9 extr\xeame en entra\xeenement et en inf\xe9rence. Ce mod\xe8le utilise un m\xe9canisme d\'attention hybride innovant (Gated DeltaNet et Gated Attention), une structure d\'experts mixtes \xe0 haute sparsit\xe9 (MoE) ainsi que plusieurs optimisations pour la stabilit\xe9 de l\'entra\xeenement. En tant que mod\xe8le sparse avec un total de 80 milliards de param\xe8tres, il n\'active qu\'environ 3 milliards de param\xe8tres lors de l\'inf\xe9rence, r\xe9duisant ainsi consid\xe9rablement les co\xfbts de calcul. Lors de t\xe2ches avec un contexte long d\xe9passant 32K tokens, son d\xe9bit d\'inf\xe9rence est plus de 10 fois sup\xe9rieur \xe0 celui du mod\xe8le Qwen3-32B. Ce mod\xe8le est une version fine-tun\xe9e pour les instructions, con\xe7ue pour des t\xe2ches g\xe9n\xe9rales, et ne supporte pas le mode cha\xeene de pens\xe9e (Thinking). En termes de performance, il est comparable au mod\xe8le phare Tongyi Qianwen Qwen3-235B sur certains benchmarks, montrant un avantage marqu\xe9 sur les t\xe2ches \xe0 contexte tr\xe8s long."},"Qwen/Qwen3-Next-80B-A3B-Thinking":{"description":"Qwen3-Next-80B-A3B-Thinking est un mod\xe8le de base de nouvelle g\xe9n\xe9ration publi\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba, sp\xe9cialement con\xe7u pour les t\xe2ches de raisonnement complexes. Il repose sur l\'architecture innovante Qwen3-Next, qui int\xe8gre un m\xe9canisme d\'attention hybride (Gated DeltaNet et Gated Attention) et une structure d\'experts mixtes \xe0 haute sparsit\xe9 (MoE), visant une efficacit\xe9 extr\xeame en entra\xeenement et inf\xe9rence. En tant que mod\xe8le sparse totalisant 80 milliards de param\xe8tres, il n\'active qu\'environ 3 milliards de param\xe8tres lors de l\'inf\xe9rence, r\xe9duisant significativement les co\xfbts de calcul. Pour les t\xe2ches \xe0 contexte long d\xe9passant 32K tokens, son d\xe9bit est plus de 10 fois sup\xe9rieur \xe0 celui du mod\xe8le Qwen3-32B. Cette version \xab Thinking \xbb est optimis\xe9e pour ex\xe9cuter des t\xe2ches complexes \xe0 \xe9tapes multiples telles que preuves math\xe9matiques, synth\xe8se de code, analyse logique et planification, et produit par d\xe9faut le processus de raisonnement sous forme structur\xe9e de \xab cha\xeene de pens\xe9e \xbb. En termes de performance, il d\xe9passe non seulement des mod\xe8les plus co\xfbteux comme Qwen3-32B-Thinking, mais surpasse \xe9galement Gemini-2.5-Flash-Thinking sur plusieurs benchmarks."},"Qwen/Qwen3-Omni-30B-A3B-Captioner":{"description":"Qwen3-Omni-30B-A3B-Captioner est un mod\xe8le de langage visuel (VLM) de la s\xe9rie Qwen3 d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba. Il est sp\xe9cialement con\xe7u pour g\xe9n\xe9rer des descriptions d\'images de haute qualit\xe9, d\xe9taill\xe9es et pr\xe9cises. Bas\xe9 sur une architecture d\'experts mixtes (MoE) avec un total de 30 milliards de param\xe8tres, ce mod\xe8le comprend en profondeur le contenu visuel et le traduit en descriptions textuelles naturelles et fluides. Il excelle dans la capture des d\xe9tails visuels, la compr\xe9hension des sc\xe8nes, la reconnaissance d\'objets et le raisonnement relationnel, ce qui le rend id\xe9al pour les applications n\xe9cessitant une compr\xe9hension et une description pr\xe9cises des images."},"Qwen/Qwen3-Omni-30B-A3B-Instruct":{"description":"Qwen3-Omni-30B-A3B-Instruct est un mod\xe8le de la derni\xe8re s\xe9rie Qwen3 d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba. Il s\'agit d\'un mod\xe8le d\'experts mixtes (MoE) avec 30 milliards de param\xe8tres totaux et 3 milliards de param\xe8tres activ\xe9s, offrant de hautes performances tout en r\xe9duisant les co\xfbts d\'inf\xe9rence. Entra\xeen\xe9 sur des donn\xe9es de haute qualit\xe9, multilingues et provenant de sources vari\xe9es, il poss\xe8de de puissantes capacit\xe9s g\xe9n\xe9rales et prend en charge les entr\xe9es multimodales, y compris le texte, l\'image, l\'audio et la vid\xe9o, permettant la compr\xe9hension et la g\xe9n\xe9ration de contenus intermodaux."},"Qwen/Qwen3-Omni-30B-A3B-Thinking":{"description":"Qwen3-Omni-30B-A3B-Thinking est le composant central \\"penseur\\" (Thinker) du mod\xe8le multimodal Qwen3-Omni. Il est con\xe7u pour traiter des entr\xe9es multimodales telles que le texte, l\'audio, l\'image et la vid\xe9o, et pour ex\xe9cuter des cha\xeenes de raisonnement complexes. En tant que cerveau de l\'inf\xe9rence, ce mod\xe8le unifie toutes les entr\xe9es dans un espace de repr\xe9sentation commun, permettant une compr\xe9hension approfondie et un raisonnement complexe intermodal. Bas\xe9 sur une architecture d\'experts mixtes (MoE) avec 30 milliards de param\xe8tres totaux et 3 milliards de param\xe8tres activ\xe9s, il maintient une forte capacit\xe9 de raisonnement tout en optimisant l\'efficacit\xe9 du calcul."},"Qwen/Qwen3-VL-235B-A22B-Instruct":{"description":"Qwen3-VL-235B-A22B-Instruct est un grand mod\xe8le de la s\xe9rie Qwen3-VL, affin\xe9 par instructions, bas\xe9 sur une architecture \xe0 experts mixtes (MoE). Il offre d\'excellentes capacit\xe9s de compr\xe9hension et de g\xe9n\xe9ration multimodales, prend en charge nativement un contexte de 256K, et convient aux services multimodaux de production \xe0 haute concurrence."},"Qwen/Qwen3-VL-235B-A22B-Thinking":{"description":"Qwen3-VL-235B-A22B-Thinking est la version phare de raisonnement de la s\xe9rie Qwen3-VL, sp\xe9cialement optimis\xe9e pour le raisonnement multimodal complexe, le raisonnement sur de longs contextes et l\'interaction avec des agents intelligents. Elle est adapt\xe9e aux sc\xe9narios d\'entreprise n\xe9cessitant une r\xe9flexion approfondie et un raisonnement visuel avanc\xe9."},"Qwen/Qwen3-VL-30B-A3B-Instruct":{"description":"Qwen3-VL-30B-A3B-Instruct est une version de la s\xe9rie Qwen3-VL affin\xe9e par instructions, dot\xe9e de puissantes capacit\xe9s de compr\xe9hension et de g\xe9n\xe9ration visuo-langagi\xe8res. Elle prend en charge nativement une longueur de contexte de 256K, id\xe9ale pour les dialogues multimodaux et les t\xe2ches de g\xe9n\xe9ration conditionn\xe9e par image."},"Qwen/Qwen3-VL-30B-A3B-Thinking":{"description":"Qwen3-VL-30B-A3B-Thinking est une version renforc\xe9e pour le raisonnement (Thinking) de Qwen3-VL, optimis\xe9e pour le raisonnement multimodal, la g\xe9n\xe9ration de code \xe0 partir d\'images et les t\xe2ches complexes de compr\xe9hension visuelle. Elle prend en charge un contexte de 256K et dispose de capacit\xe9s accrues de raisonnement en cha\xeene."},"Qwen/Qwen3-VL-32B-Instruct":{"description":"Qwen3-VL-32B-Instruct est un mod\xe8le de langage visuel d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba, ayant atteint des performances SOTA sur plusieurs benchmarks de langage visuel. Il prend en charge des images haute r\xe9solution de niveau m\xe9gapixel et poss\xe8de de puissantes capacit\xe9s de compr\xe9hension visuelle g\xe9n\xe9rale, de reconnaissance optique multilingue (OCR), de localisation visuelle fine et de dialogue visuel. En tant que mod\xe8le de langage visuel de la s\xe9rie Qwen3, il est capable de g\xe9rer des t\xe2ches multimodales complexes et prend en charge des fonctions avanc\xe9es telles que l\'appel d\'outils et la g\xe9n\xe9ration conditionnelle par pr\xe9fixe."},"Qwen/Qwen3-VL-32B-Thinking":{"description":"Qwen3-VL-32B-Thinking est une version optimis\xe9e du mod\xe8le de langage visuel d\xe9velopp\xe9 par l\'\xe9quipe Tongyi Qianwen d\'Alibaba, sp\xe9cialement con\xe7ue pour les t\xe2ches de raisonnement visuel complexe. Ce mod\xe8le int\xe8gre un \\"mode de r\xe9flexion\\" qui lui permet de g\xe9n\xe9rer des \xe9tapes de raisonnement interm\xe9diaires d\xe9taill\xe9es avant de r\xe9pondre, am\xe9liorant ainsi consid\xe9rablement ses performances dans les t\xe2ches n\xe9cessitant une logique multi-\xe9tapes, de la planification et un raisonnement complexe. Il prend en charge des images haute r\xe9solution de niveau m\xe9gapixel, avec de solides capacit\xe9s de compr\xe9hension visuelle g\xe9n\xe9rale, d\'OCR multilingue, de localisation visuelle fine et de dialogue visuel, tout en prenant en charge l\'appel d\'outils et la g\xe9n\xe9ration conditionnelle par pr\xe9fixe."},"Qwen/Qwen3-VL-8B-Instruct":{"description":"Qwen3-VL-8B-Instruct est un mod\xe8le de langage visuel de la s\xe9rie Qwen3, d\xe9velopp\xe9 \xe0 partir de Qwen3-8B-Instruct et entra\xeen\xe9 sur un grand volume de donn\xe9es image-texte. Il excelle dans la compr\xe9hension visuelle g\xe9n\xe9rale, les dialogues centr\xe9s sur l’image et la reconnaissance multilingue de texte dans les images. Il est adapt\xe9 aux cas d’usage tels que les questions-r\xe9ponses visuelles, la description d’images, le suivi d’instructions multimodales et l’appel d’outils."},"Qwen/Qwen3-VL-8B-Thinking":{"description":"Qwen3-VL-8B-Thinking est la version orient\xe9e raisonnement visuel de la s\xe9rie Qwen3, optimis\xe9e pour les t\xe2ches complexes de raisonnement en plusieurs \xe9tapes. Par d\xe9faut, il g\xe9n\xe8re une cha\xeene de r\xe9flexion (thinking chain) avant de r\xe9pondre, afin d’am\xe9liorer la pr\xe9cision du raisonnement. Il est particuli\xe8rement adapt\xe9 aux sc\xe9narios n\xe9cessitant une analyse approfondie, comme les questions-r\xe9ponses visuelles complexes ou l’examen d\xe9taill\xe9 du contenu d’une image."},"Qwen2-72B-Instruct":{"description":"Qwen2 est la derni\xe8re s\xe9rie du mod\xe8le Qwen, prenant en charge un contexte de 128k. Compar\xe9 aux meilleurs mod\xe8les open source actuels, Qwen2-72B surpasse de mani\xe8re significative les mod\xe8les leaders dans des domaines tels que la compr\xe9hension du langage naturel, les connaissances, le code, les math\xe9matiques et le multilinguisme."},"Qwen2-7B-Instruct":{"description":"Qwen2 est la derni\xe8re s\xe9rie du mod\xe8le Qwen, capable de surpasser les meilleurs mod\xe8les open source de taille \xe9quivalente, voire de plus grande taille. Qwen2 7B a obtenu des r\xe9sultats significatifs dans plusieurs \xe9valuations, en particulier en ce qui concerne la compr\xe9hension du code et du chinois."},"Qwen2-VL-72B":{"description":"Qwen2-VL-72B est un puissant mod\xe8le de langage visuel, prenant en charge le traitement multimodal d\'images et de textes, capable de reconna\xeetre avec pr\xe9cision le contenu des images et de g\xe9n\xe9rer des descriptions ou des r\xe9ponses pertinentes."},"Qwen2.5-14B-Instruct":{"description":"Qwen2.5-14B-Instruct est un grand mod\xe8le de langage de 14 milliards de param\xe8tres, offrant d\'excellentes performances, optimis\xe9 pour les sc\xe9narios en chinois et multilingues, prenant en charge des applications telles que les questions-r\xe9ponses intelligentes et la g\xe9n\xe9ration de contenu."},"Qwen2.5-32B-Instruct":{"description":"Qwen2.5-32B-Instruct est un grand mod\xe8le de langage de 32 milliards de param\xe8tres, offrant des performances \xe9quilibr\xe9es, optimis\xe9 pour les sc\xe9narios en chinois et multilingues, prenant en charge des applications telles que les questions-r\xe9ponses intelligentes et la g\xe9n\xe9ration de contenu."},"Qwen2.5-72B-Instruct":{"description":"Qwen2.5-72B-Instruct prend en charge un contexte de 16k, g\xe9n\xe9rant des textes longs de plus de 8K. Il permet des appels de fonction et une interaction transparente avec des syst\xe8mes externes, augmentant consid\xe9rablement la flexibilit\xe9 et l\'\xe9volutivit\xe9. Les connaissances du mod\xe8le ont consid\xe9rablement augment\xe9, et ses capacit\xe9s en codage et en math\xe9matiques ont \xe9t\xe9 grandement am\xe9lior\xe9es, avec un support multilingue d\xe9passant 29 langues."},"Qwen2.5-7B-Instruct":{"description":"Qwen2.5-7B-Instruct est un grand mod\xe8le de langage de 7 milliards de param\xe8tres, prenant en charge les appels de fonction et l\'interaction transparente avec des syst\xe8mes externes, am\xe9liorant consid\xe9rablement la flexibilit\xe9 et l\'\xe9volutivit\xe9. Optimis\xe9 pour les sc\xe9narios en chinois et multilingues, il prend en charge des applications telles que les questions-r\xe9ponses intelligentes et la g\xe9n\xe9ration de contenu."},"Qwen2.5-Coder-14B-Instruct":{"description":"Qwen2.5-Coder-14B-Instruct est un mod\xe8le d\'instructions de programmation bas\xe9 sur un pr\xe9-entra\xeenement \xe0 grande \xe9chelle, dot\xe9 d\'une puissante capacit\xe9 de compr\xe9hension et de g\xe9n\xe9ration de code, capable de traiter efficacement diverses t\xe2ches de programmation, particuli\xe8rement adapt\xe9 \xe0 la r\xe9daction de code intelligent, \xe0 la g\xe9n\xe9ration de scripts automatis\xe9s et \xe0 la r\xe9solution de probl\xe8mes de programmation."},"Qwen2.5-Coder-32B-Instruct":{"description":"Qwen2.5-Coder-32B-Instruct est un grand mod\xe8le de langage con\xe7u pour la g\xe9n\xe9ration de code, la compr\xe9hension de code et les sc\xe9narios de d\xe9veloppement efficaces, avec une \xe9chelle de 32 milliards de param\xe8tres, r\xe9pondant \xe0 des besoins de programmation vari\xe9s."},"Qwen3-235B":{"description":"Qwen3-235B-A22B est un mod\xe8le MoE (mod\xe8le d\'experts mixtes) qui introduit un \xab mode de raisonnement hybride \xbb, permettant aux utilisateurs de basculer sans interruption entre le \xab mode r\xe9flexif \xbb et le \xab mode non r\xe9flexif \xbb. Il prend en charge la compr\xe9hension et le raisonnement dans 119 langues et dialectes, et dispose de puissantes capacit\xe9s d\'appel d\'outils. Sur plusieurs benchmarks, notamment en capacit\xe9s globales, codage et math\xe9matiques, multilinguisme, connaissances et raisonnement, il rivalise avec les principaux grands mod\xe8les du march\xe9 tels que DeepSeek R1, OpenAI o1, o3-mini, Grok 3 et Google Gemini 2.5 Pro."},"Qwen3-235B-A22B-Instruct-2507-FP8":{"description":"Qwen3 235B A22B Instruct 2507 : mod\xe8le optimis\xe9 pour le raisonnement avanc\xe9 et les instructions de dialogue, avec une architecture \xe0 experts mixtes pour maintenir l\'efficacit\xe9 de l\'inf\xe9rence \xe0 grande \xe9chelle."},"Qwen3-32B":{"description":"Qwen3-32B est un mod\xe8le dense (Dense Model) qui introduit un \xab mode de raisonnement hybride \xbb, permettant aux utilisateurs de basculer sans interruption entre le \xab mode r\xe9flexif \xbb et le \xab mode non r\xe9flexif \xbb. Gr\xe2ce \xe0 des am\xe9liorations de l\'architecture du mod\xe8le, \xe0 l\'augmentation des donn\xe9es d\'entra\xeenement et \xe0 des m\xe9thodes d\'entra\xeenement plus efficaces, ses performances globales sont comparables \xe0 celles de Qwen2.5-72B."},"SenseChat":{"description":"Mod\xe8le de version de base (V4), longueur de contexte de 4K, avec de puissantes capacit\xe9s g\xe9n\xe9rales."},"SenseChat-128K":{"description":"Mod\xe8le de version de base (V4), longueur de contexte de 128K, excellent dans les t\xe2ches de compr\xe9hension et de g\xe9n\xe9ration de longs textes."},"SenseChat-32K":{"description":"Mod\xe8le de version de base (V4), longueur de contexte de 32K, appliqu\xe9 de mani\xe8re flexible \xe0 divers sc\xe9narios."},"SenseChat-5":{"description":"Mod\xe8le de derni\xe8re version (V5.5), longueur de contexte de 128K, avec des capacit\xe9s significativement am\xe9lior\xe9es dans le raisonnement math\xe9matique, les dialogues en anglais, le suivi d\'instructions et la compr\xe9hension de longs textes, rivalisant avec GPT-4o."},"SenseChat-5-1202":{"description":"Bas\xe9 sur la version V5.5 la plus r\xe9cente, avec des am\xe9liorations significatives par rapport \xe0 la version pr\xe9c\xe9dente dans plusieurs dimensions telles que les capacit\xe9s de base en chinois et en anglais, le dialogue, les connaissances scientifiques, les connaissances litt\xe9raires, la r\xe9daction, la logique math\xe9matique et le contr\xf4le du nombre de mots."},"SenseChat-5-Cantonese":{"description":"Longueur de contexte de 32K, surpassant GPT-4 dans la compr\xe9hension des dialogues en cantonais, rivalisant avec GPT-4 Turbo dans plusieurs domaines tels que les connaissances, le raisonnement, les math\xe9matiques et la r\xe9daction de code."},"SenseChat-5-beta":{"description":"Certaines performances surpassent celles de SenseCat-5-1202"},"SenseChat-Character":{"description":"Mod\xe8le standard, longueur de contexte de 8K, avec une grande rapidit\xe9 de r\xe9ponse."},"SenseChat-Character-Pro":{"description":"Mod\xe8le avanc\xe9, longueur de contexte de 32K, avec des capacit\xe9s globalement am\xe9lior\xe9es, prenant en charge les dialogues en chinois et en anglais."},"SenseChat-Turbo":{"description":"Con\xe7u pour des questions-r\xe9ponses rapides et des sc\xe9narios de micro-ajustement du mod\xe8le."},"SenseChat-Turbo-1202":{"description":"C\'est le dernier mod\xe8le l\xe9ger, atteignant plus de 90 % des capacit\xe9s du mod\xe8le complet, tout en r\xe9duisant consid\xe9rablement le co\xfbt d\'inf\xe9rence."},"SenseChat-Vision":{"description":"Le dernier mod\xe8le (V5.5) prend en charge l\'entr\xe9e de plusieurs images, optimisant les capacit\xe9s de base du mod\xe8le, avec des am\xe9liorations significatives dans la reconnaissance des attributs d\'objets, les relations spatiales, la reconnaissance d\'\xe9v\xe9nements d\'action, la compr\xe9hension de sc\xe8nes, la reconnaissance des \xe9motions, le raisonnement de bon sens logique et la compr\xe9hension et g\xe9n\xe9ration de texte."},"SenseNova-V6-5-Pro":{"description":"Gr\xe2ce \xe0 une mise \xe0 jour compl\xe8te des donn\xe9es multimodales, linguistiques et de raisonnement ainsi qu\'\xe0 l\'optimisation des strat\xe9gies d\'entra\xeenement, le nouveau mod\xe8le r\xe9alise des progr\xe8s significatifs en mati\xe8re de raisonnement multimodal et de suivi g\xe9n\xe9ralis\xe9 des instructions. Il prend en charge une fen\xeatre contextuelle allant jusqu\'\xe0 128k et excelle dans des t\xe2ches sp\xe9cialis\xe9es telles que la reconnaissance OCR et l\'identification des propri\xe9t\xe9s intellectuelles dans le secteur du tourisme culturel."},"SenseNova-V6-5-Turbo":{"description":"Gr\xe2ce \xe0 une mise \xe0 jour compl\xe8te des donn\xe9es multimodales, linguistiques et de raisonnement ainsi qu\'\xe0 l\'optimisation des strat\xe9gies d\'entra\xeenement, le nouveau mod\xe8le r\xe9alise des progr\xe8s significatifs en mati\xe8re de raisonnement multimodal et de suivi g\xe9n\xe9ralis\xe9 des instructions. Il prend en charge une fen\xeatre contextuelle allant jusqu\'\xe0 128k et excelle dans des t\xe2ches sp\xe9cialis\xe9es telles que la reconnaissance OCR et l\'identification des propri\xe9t\xe9s intellectuelles dans le secteur du tourisme culturel."},"SenseNova-V6-Pro":{"description":"R\xe9aliser une unification native des capacit\xe9s d\'image, de texte et de vid\xe9o, briser les limitations traditionnelles de la multimodalit\xe9 discr\xe8te, remportant le double championnat dans les \xe9valuations OpenCompass et SuperCLUE."},"SenseNova-V6-Reasoner":{"description":"Allier raisonnement visuel et linguistique en profondeur, r\xe9aliser une r\xe9flexion lente et un raisonnement approfondi, pr\xe9sentant un processus de cha\xeene de pens\xe9e complet."},"SenseNova-V6-Turbo":{"description":"R\xe9aliser une unification native des capacit\xe9s d\'image, de texte et de vid\xe9o, briser les limitations traditionnelles de la multimodalit\xe9 discr\xe8te, \xeatre en t\xeate dans des dimensions cl\xe9s telles que les capacit\xe9s multimodales et linguistiques, alliant rigueur et cr\xe9ativit\xe9, se classant \xe0 plusieurs reprises parmi les meilleurs niveaux nationaux et internationaux dans divers \xe9valuations."},"Skylark2-lite-8k":{"description":"Le mod\xe8le de deuxi\xe8me g\xe9n\xe9ration Skylark (Skylark2-lite) pr\xe9sente une grande rapidit\xe9 de r\xe9ponse, adapt\xe9 \xe0 des sc\xe9narios n\xe9cessitant une r\xe9activit\xe9 \xe9lev\xe9e, sensible aux co\xfbts, avec des exigences de pr\xe9cision de mod\xe8le moins \xe9lev\xe9es, avec une longueur de fen\xeatre de contexte de 8k."},"Skylark2-pro-32k":{"description":"Le mod\xe8le de deuxi\xe8me g\xe9n\xe9ration Skylark (Skylark2-pro) offre une pr\xe9cision \xe9lev\xe9e, adapt\xe9 \xe0 des sc\xe9narios de g\xe9n\xe9ration de texte plus complexes tels que la cr\xe9ation de contenu dans des domaines professionnels, la r\xe9daction de romans et les traductions de haute qualit\xe9, avec une longueur de fen\xeatre de contexte de 32k."},"Skylark2-pro-4k":{"description":"Le mod\xe8le de deuxi\xe8me g\xe9n\xe9ration Skylark (Skylark2-pro) offre une pr\xe9cision \xe9lev\xe9e, adapt\xe9 \xe0 des sc\xe9narios de g\xe9n\xe9ration de texte plus complexes tels que la cr\xe9ation de contenu dans des domaines professionnels, la r\xe9daction de romans et les traductions de haute qualit\xe9, avec une longueur de fen\xeatre de contexte de 4k."},"Skylark2-pro-character-4k":{"description":"Le mod\xe8le de deuxi\xe8me g\xe9n\xe9ration Skylark (Skylark2-pro-character) poss\xe8de d\'excellentes capacit\xe9s de jeu de r\xf4le et de chat, capable d\'interagir suivant les instructions des utilisateurs, avec un style de personnage distinct et un contenu de dialogue fluide. Il est appropri\xe9 pour construire des chatbots, des assistants virtuels et des services clients en ligne, avec une grande rapidit\xe9 de r\xe9ponse."},"Skylark2-pro-turbo-8k":{"description":"Le mod\xe8le de deuxi\xe8me g\xe9n\xe9ration Skylark (Skylark2-pro-turbo-8k) offre un raisonnement plus rapide et un co\xfbt r\xe9duit, avec une longueur de fen\xeatre de contexte de 8k."},"THUDM/GLM-4-32B-0414":{"description":"GLM-4-32B-0414 est le nouveau mod\xe8le open source de la s\xe9rie GLM, avec 32 milliards de param\xe8tres. Ce mod\xe8le rivalise avec les performances des s\xe9ries GPT d\'OpenAI et V3/R1 de DeepSeek."},"THUDM/GLM-4-9B-0414":{"description":"GLM-4-9B-0414 est un mod\xe8le de petite taille de la s\xe9rie GLM, avec 9 milliards de param\xe8tres. Ce mod\xe8le h\xe9rite des caract\xe9ristiques techniques de la s\xe9rie GLM-4-32B, tout en offrant une option de d\xe9ploiement plus l\xe9g\xe8re. Bien que de taille r\xe9duite, GLM-4-9B-0414 excelle toujours dans des t\xe2ches telles que la g\xe9n\xe9ration de code, la conception de sites web, la g\xe9n\xe9ration de graphiques SVG et l\'\xe9criture bas\xe9e sur la recherche."},"THUDM/GLM-4.1V-9B-Thinking":{"description":"GLM-4.1V-9B-Thinking est un mod\xe8le de langage visuel open source (VLM) publi\xe9 conjointement par Zhipu AI et le laboratoire KEG de l\'Universit\xe9 Tsinghua, con\xe7u pour traiter des t\xe2ches cognitives multimodales complexes. Ce mod\xe8le est bas\xe9 sur le mod\xe8le de base GLM-4-9B-0414 et int\xe8gre un m\xe9canisme de raisonnement \xab cha\xeene de pens\xe9e \xbb (Chain-of-Thought) ainsi qu\'une strat\xe9gie d\'apprentissage par renforcement, am\xe9liorant significativement ses capacit\xe9s de raisonnement intermodal et sa stabilit\xe9."},"THUDM/GLM-Z1-32B-0414":{"description":"GLM-Z1-32B-0414 est un mod\xe8le de raisonnement avec des capacit\xe9s de r\xe9flexion profonde. Ce mod\xe8le est bas\xe9 sur GLM-4-32B-0414, d\xe9velopp\xe9 par un d\xe9marrage \xe0 froid et un apprentissage par renforcement \xe9tendu, et a \xe9t\xe9 form\xe9 davantage sur des t\xe2ches de math\xe9matiques, de code et de logique. Par rapport au mod\xe8le de base, GLM-Z1-32B-0414 am\xe9liore consid\xe9rablement les capacit\xe9s math\xe9matiques et la r\xe9solution de t\xe2ches complexes."},"THUDM/GLM-Z1-9B-0414":{"description":"GLM-Z1-9B-0414 est un mod\xe8le de petite taille de la s\xe9rie GLM, avec seulement 9 milliards de param\xe8tres, mais montrant des capacit\xe9s \xe9tonnantes tout en maintenant la tradition open source. Bien que de taille r\xe9duite, ce mod\xe8le excelle dans le raisonnement math\xe9matique et les t\xe2ches g\xe9n\xe9rales, avec des performances globales parmi les meilleures de sa cat\xe9gorie dans les mod\xe8les open source."},"THUDM/GLM-Z1-Rumination-32B-0414":{"description":"GLM-Z1-Rumination-32B-0414 est un mod\xe8le de raisonnement profond avec des capacit\xe9s de r\xe9flexion (compar\xe9 \xe0 la recherche approfondie d\'OpenAI). Contrairement aux mod\xe8les de r\xe9flexion typiques, le mod\xe8le de r\xe9flexion utilise des p\xe9riodes de r\xe9flexion plus longues pour r\xe9soudre des probl\xe8mes plus ouverts et complexes."},"THUDM/glm-4-9b-chat":{"description":"GLM-4 9B est une version open source, offrant une exp\xe9rience de dialogue optimis\xe9e pour les applications de conversation."},"Tongyi-Zhiwen/QwenLong-L1-32B":{"description":"QwenLong-L1-32B est le premier grand mod\xe8le de raisonnement \xe0 long contexte (LRM) entra\xeen\xe9 par renforcement, optimis\xe9 pour les t\xe2ches de raisonnement sur de longs textes. Ce mod\xe8le utilise un cadre d’apprentissage par renforcement \xe0 extension progressive du contexte, assurant une transition stable du court au long contexte. Sur sept benchmarks de questions-r\xe9ponses \xe0 long contexte, QwenLong-L1-32B d\xe9passe les mod\xe8les phares tels que OpenAI-o3-mini et Qwen3-235B-A22B, avec des performances comparables \xe0 Claude-3.7-Sonnet-Thinking. Il excelle particuli\xe8rement dans les t\xe2ches complexes de raisonnement math\xe9matique, logique et multi-sauts."},"Yi-34B-Chat":{"description":"Yi-1.5-34B, tout en maintenant les excellentes capacit\xe9s linguistiques g\xe9n\xe9rales de la s\xe9rie originale, a consid\xe9rablement am\xe9lior\xe9 ses comp\xe9tences en logique math\xe9matique et en codage gr\xe2ce \xe0 un entra\xeenement incr\xe9mental sur 500 milliards de tokens de haute qualit\xe9."},"abab5.5-chat":{"description":"Orient\xe9 vers des sc\xe9narios de productivit\xe9, prenant en charge le traitement de t\xe2ches complexes et la g\xe9n\xe9ration de texte efficace, adapt\xe9 aux applications professionnelles."},"abab5.5s-chat":{"description":"Con\xe7u pour des sc\xe9narios de dialogue en chinois, offrant une capacit\xe9 de g\xe9n\xe9ration de dialogues en chinois de haute qualit\xe9, adapt\xe9e \xe0 divers sc\xe9narios d\'application."},"abab6.5g-chat":{"description":"Con\xe7u pour des dialogues de personnages multilingues, prenant en charge la g\xe9n\xe9ration de dialogues de haute qualit\xe9 en anglais et dans d\'autres langues."},"abab6.5s-chat":{"description":"Adapt\xe9 \xe0 une large gamme de t\xe2ches de traitement du langage naturel, y compris la g\xe9n\xe9ration de texte, les syst\xe8mes de dialogue, etc."},"abab6.5t-chat":{"description":"Optimis\xe9 pour des sc\xe9narios de dialogue en chinois, offrant une capacit\xe9 de g\xe9n\xe9ration de dialogues fluide et conforme aux habitudes d\'expression en chinois."},"accounts/fireworks/models/deepseek-r1":{"description":"DeepSeek-R1 est un mod\xe8le de langage de grande taille \xe0 la pointe de la technologie, optimis\xe9 par apprentissage renforc\xe9 et donn\xe9es de d\xe9marrage \xe0 froid, offrant d\'excellentes performances en raisonnement, math\xe9matiques et programmation."},"accounts/fireworks/models/deepseek-v3":{"description":"Mod\xe8le de langage puissant de Deepseek bas\xe9 sur un m\xe9lange d\'experts (MoE), avec un total de 671B de param\xe8tres, activant 37B de param\xe8tres par jeton."},"accounts/fireworks/models/llama-v3-70b-instruct":{"description":"Le mod\xe8le d\'instructions Llama 3 70B est optimis\xe9 pour les dialogues multilingues et la compr\xe9hension du langage naturel, surpassant la plupart des mod\xe8les concurrents."},"accounts/fireworks/models/llama-v3-8b-instruct":{"description":"Le mod\xe8le d\'instructions Llama 3 8B est optimis\xe9 pour les dialogues et les t\xe2ches multilingues, offrant des performances exceptionnelles et efficaces."},"accounts/fireworks/models/llama-v3-8b-instruct-hf":{"description":"Le mod\xe8le d\'instructions Llama 3 8B (version HF) est conforme aux r\xe9sultats de l\'impl\xe9mentation officielle, offrant une grande coh\xe9rence et une compatibilit\xe9 multiplateforme."},"accounts/fireworks/models/llama-v3p1-405b-instruct":{"description":"Le mod\xe8le d\'instructions Llama 3.1 405B, avec des param\xe8tres de tr\xe8s grande \xe9chelle, est adapt\xe9 aux t\xe2ches complexes et au suivi d\'instructions dans des sc\xe9narios \xe0 forte charge."},"accounts/fireworks/models/llama-v3p1-70b-instruct":{"description":"Le mod\xe8le d\'instructions Llama 3.1 70B offre une compr\xe9hension et une g\xe9n\xe9ration de langage exceptionnelles, id\xe9al pour les t\xe2ches de dialogue et d\'analyse."},"accounts/fireworks/models/llama-v3p1-8b-instruct":{"description":"Le mod\xe8le d\'instructions Llama 3.1 8B est optimis\xe9 pour les dialogues multilingues, capable de surpasser la plupart des mod\xe8les open source et ferm\xe9s sur des benchmarks industriels courants."},"accounts/fireworks/models/llama-v3p2-11b-vision-instruct":{"description":"Mod\xe8le d\'inf\xe9rence d\'image ajust\xe9 par instructions de Meta avec 11B param\xe8tres. Ce mod\xe8le est optimis\xe9 pour la reconnaissance visuelle, l\'inf\xe9rence d\'image, la description d\'image et pour r\xe9pondre \xe0 des questions g\xe9n\xe9rales sur l\'image. Il est capable de comprendre des donn\xe9es visuelles, comme des graphiques et des diagrammes, et de combler le foss\xe9 entre la vision et le langage en g\xe9n\xe9rant des descriptions textuelles des d\xe9tails de l\'image."},"accounts/fireworks/models/llama-v3p2-3b-instruct":{"description":"Le mod\xe8le d\'instructions Llama 3.2 3B est un mod\xe8le multilingue l\xe9ger lanc\xe9 par Meta. Ce mod\xe8le vise \xe0 am\xe9liorer l\'efficacit\xe9, offrant des am\xe9liorations significatives en mati\xe8re de latence et de co\xfbt par rapport aux mod\xe8les plus grands. Les cas d\'utilisation incluent les requ\xeates, la r\xe9\xe9criture de prompts et l\'assistance \xe0 l\'\xe9criture."},"accounts/fireworks/models/llama-v3p2-90b-vision-instruct":{"description":"Mod\xe8le d\'inf\xe9rence d\'image ajust\xe9 par instructions de Meta avec 90B param\xe8tres. Ce mod\xe8le est optimis\xe9 pour la reconnaissance visuelle, l\'inf\xe9rence d\'image, la description d\'image et pour r\xe9pondre \xe0 des questions g\xe9n\xe9rales sur l\'image. Il est capable de comprendre des donn\xe9es visuelles, comme des graphiques et des diagrammes, et de combler le foss\xe9 entre la vision et le langage en g\xe9n\xe9rant des descriptions textuelles des d\xe9tails de l\'image."},"accounts/fireworks/models/llama-v3p3-70b-instruct":{"description":"Llama 3.3 70B Instruct est la version mise \xe0 jour de Llama 3.1 70B de d\xe9cembre. Ce mod\xe8le a \xe9t\xe9 am\xe9lior\xe9 par rapport \xe0 Llama 3.1 70B (publi\xe9 en juillet 2024), renfor\xe7ant les appels d\'outils, le support multilingue, ainsi que les capacit\xe9s en math\xe9matiques et en programmation. Ce mod\xe8le atteint des niveaux de performance de pointe dans le raisonnement, les math\xe9matiques et le respect des instructions, tout en offrant des performances similaires \xe0 celles du 3.1 405B, avec des avantages significatifs en termes de vitesse et de co\xfbt."},"accounts/fireworks/models/mistral-small-24b-instruct-2501":{"description":"Mod\xe8le de 24B param\xe8tres, dot\xe9 de capacit\xe9s de pointe comparables \xe0 celles de mod\xe8les plus grands."},"accounts/fireworks/models/mixtral-8x22b-instruct":{"description":"Le mod\xe8le d\'instructions Mixtral MoE 8x22B, avec des param\xe8tres \xe0 grande \xe9chelle et une architecture multi-experts, prend en charge efficacement le traitement de t\xe2ches complexes."},"accounts/fireworks/models/mixtral-8x7b-instruct":{"description":"Le mod\xe8le d\'instructions Mixtral MoE 8x7B, avec une architecture multi-experts, offre un suivi et une ex\xe9cution d\'instructions efficaces."},"accounts/fireworks/models/mythomax-l2-13b":{"description":"Le mod\xe8le MythoMax L2 13B, combinant des techniques de fusion novatrices, excelle dans la narration et le jeu de r\xf4le."},"accounts/fireworks/models/phi-3-vision-128k-instruct":{"description":"Le mod\xe8le d\'instructions Phi 3 Vision est un mod\xe8le multimodal l\xe9ger, capable de traiter des informations visuelles et textuelles complexes, avec une forte capacit\xe9 de raisonnement."},"accounts/fireworks/models/qwen-qwq-32b-preview":{"description":"Le mod\xe8le QwQ est un mod\xe8le de recherche exp\xe9rimental d\xe9velopp\xe9 par l\'\xe9quipe Qwen, ax\xe9 sur l\'am\xe9lioration des capacit\xe9s de raisonnement de l\'IA."},"accounts/fireworks/models/qwen2-vl-72b-instruct":{"description":"La version 72B du mod\xe8le Qwen-VL est le fruit de la derni\xe8re it\xe9ration d\'Alibaba, repr\xe9sentant pr\xe8s d\'un an d\'innovation."},"accounts/fireworks/models/qwen2p5-72b-instruct":{"description":"Qwen2.5 est une s\xe9rie de mod\xe8les de langage \xe0 d\xe9codage uniquement d\xe9velopp\xe9e par l\'\xe9quipe Qwen d\'Alibaba Cloud. Ces mod\xe8les sont offerts en diff\xe9rentes tailles, y compris 0.5B, 1.5B, 3B, 7B, 14B, 32B et 72B, avec des variantes de base (base) et d\'instruction (instruct)."},"accounts/fireworks/models/qwen2p5-coder-32b-instruct":{"description":"Qwen2.5 Coder 32B Instruct est la derni\xe8re version de la s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle sp\xe9cifique au code publi\xe9e par Alibaba Cloud. Ce mod\xe8le, bas\xe9 sur Qwen2.5, a \xe9t\xe9 form\xe9 avec 55 trillions de tokens, am\xe9liorant consid\xe9rablement les capacit\xe9s de g\xe9n\xe9ration, de raisonnement et de correction de code. Il renforce non seulement les capacit\xe9s de codage, mais maintient \xe9galement des avantages en math\xe9matiques et en comp\xe9tences g\xe9n\xe9rales. Le mod\xe8le fournit une base plus compl\xe8te pour des applications pratiques telles que les agents de code."},"accounts/yi-01-ai/models/yi-large":{"description":"Le mod\xe8le Yi-Large offre d\'excellentes capacit\xe9s de traitement multilingue, adapt\xe9 \xe0 diverses t\xe2ches de g\xe9n\xe9ration et de compr\xe9hension de langage."},"ai21-jamba-1.5-large":{"description":"Un mod\xe8le multilingue de 398 milliards de param\xe8tres (94 milliards actifs), offrant une fen\xeatre de contexte longue de 256K, des appels de fonction, une sortie structur\xe9e et une g\xe9n\xe9ration ancr\xe9e."},"ai21-jamba-1.5-mini":{"description":"Un mod\xe8le multilingue de 52 milliards de param\xe8tres (12 milliards actifs), offrant une fen\xeatre de contexte longue de 256K, des appels de fonction, une sortie structur\xe9e et une g\xe9n\xe9ration ancr\xe9e."},"ai21-labs/AI21-Jamba-1.5-Large":{"description":"Un mod\xe8le multilingue de 398 milliards de param\xe8tres (94 milliards actifs), offrant une fen\xeatre contextuelle longue de 256K, des appels de fonctions, une sortie structur\xe9e et une g\xe9n\xe9ration factuelle."},"ai21-labs/AI21-Jamba-1.5-Mini":{"description":"Un mod\xe8le multilingue de 52 milliards de param\xe8tres (12 milliards actifs), offrant une fen\xeatre contextuelle longue de 256K, des appels de fonctions, une sortie structur\xe9e et une g\xe9n\xe9ration factuelle."},"alibaba/qwen-3-14b":{"description":"Qwen3 est la derni\xe8re g\xe9n\xe9ration de grands mod\xe8les de langage de la s\xe9rie Qwen, offrant un ensemble complet de mod\xe8les experts denses et hybrides (MoE). Bas\xe9 sur un entra\xeenement \xe9tendu, Qwen3 r\xe9alise des avanc\xe9es majeures en mati\xe8re de raisonnement, de suivi des instructions, de capacit\xe9s d\'agent et de support multilingue."},"alibaba/qwen-3-235b":{"description":"Qwen3 est la derni\xe8re g\xe9n\xe9ration de grands mod\xe8les de langage de la s\xe9rie Qwen, offrant un ensemble complet de mod\xe8les experts denses et hybrides (MoE). Bas\xe9 sur un entra\xeenement \xe9tendu, Qwen3 r\xe9alise des avanc\xe9es majeures en mati\xe8re de raisonnement, de suivi des instructions, de capacit\xe9s d\'agent et de support multilingue."},"alibaba/qwen-3-30b":{"description":"Qwen3 est la derni\xe8re g\xe9n\xe9ration de grands mod\xe8les de langage de la s\xe9rie Qwen, offrant un ensemble complet de mod\xe8les experts denses et hybrides (MoE). Bas\xe9 sur un entra\xeenement \xe9tendu, Qwen3 r\xe9alise des avanc\xe9es majeures en mati\xe8re de raisonnement, de suivi des instructions, de capacit\xe9s d\'agent et de support multilingue."},"alibaba/qwen-3-32b":{"description":"Qwen3 est la derni\xe8re g\xe9n\xe9ration de grands mod\xe8les de langage de la s\xe9rie Qwen, offrant un ensemble complet de mod\xe8les experts denses et hybrides (MoE). Bas\xe9 sur un entra\xeenement \xe9tendu, Qwen3 r\xe9alise des avanc\xe9es majeures en mati\xe8re de raisonnement, de suivi des instructions, de capacit\xe9s d\'agent et de support multilingue."},"alibaba/qwen3-coder":{"description":"Qwen3-Coder-480B-A35B-Instruct est le mod\xe8le de code le plus agentif de Qwen, avec des performances remarquables en codage agent, utilisation d\'agents navigateurs et autres t\xe2ches de codage fondamentales, atteignant des r\xe9sultats comparables \xe0 Claude Sonnet."},"amazon/nova-lite":{"description":"Un mod\xe8le multimodal \xe0 tr\xe8s faible co\xfbt, traitant les entr\xe9es d\'images, vid\xe9os et textes \xe0 une vitesse extr\xeamement rapide."},"amazon/nova-micro":{"description":"Un mod\xe8le uniquement textuel offrant des r\xe9ponses \xe0 latence minimale \xe0 tr\xe8s faible co\xfbt."},"amazon/nova-pro":{"description":"Un mod\xe8le multimodal tr\xe8s performant, offrant le meilleur compromis entre pr\xe9cision, vitesse et co\xfbt, adapt\xe9 \xe0 une large gamme de t\xe2ches."},"amazon/titan-embed-text-v2":{"description":"Amazon Titan Text Embeddings V2 est un mod\xe8le d\'embedding multilingue l\xe9ger et efficace, supportant des dimensions de 1024, 512 et 256."},"anthropic.claude-3-5-sonnet-20240620-v1:0":{"description":"Claude 3.5 Sonnet \xe9l\xe8ve les normes de l\'industrie, surpassant les mod\xe8les concurrents et Claude 3 Opus, avec d\'excellentes performances dans une large gamme d\'\xe9valuations, tout en offrant la vitesse et le co\xfbt de nos mod\xe8les de niveau interm\xe9diaire."},"anthropic.claude-3-5-sonnet-20241022-v2:0":{"description":"Claude 3.5 Sonnet a \xe9lev\xe9 les normes de l\'industrie, surpassant les mod\xe8les concurrents et Claude 3 Opus, tout en affichant d\'excellentes performances dans une large gamme d\'\xe9valuations, tout en conservant la vitesse et le co\xfbt de nos mod\xe8les de niveau interm\xe9diaire."},"anthropic.claude-3-haiku-20240307-v1:0":{"description":"Claude 3 Haiku est le mod\xe8le le plus rapide et le plus compact d\'Anthropic, offrant une vitesse de r\xe9ponse quasi instantan\xe9e. Il peut r\xe9pondre rapidement \xe0 des requ\xeates et demandes simples. Les clients pourront construire une exp\xe9rience AI transparente imitant l\'interaction humaine. Claude 3 Haiku peut traiter des images et retourner des sorties textuelles, avec une fen\xeatre contextuelle de 200K."},"anthropic.claude-3-opus-20240229-v1:0":{"description":"Claude 3 Opus est le mod\xe8le AI le plus puissant d\'Anthropic, avec des performances de pointe sur des t\xe2ches hautement complexes. Il peut traiter des invites ouvertes et des sc\xe9narios non vus, avec une fluidit\xe9 et une compr\xe9hension humaine exceptionnelles. Claude 3 Opus d\xe9montre les possibilit\xe9s de g\xe9n\xe9ration AI \xe0 la pointe. Claude 3 Opus peut traiter des images et retourner des sorties textuelles, avec une fen\xeatre contextuelle de 200K."},"anthropic.claude-3-sonnet-20240229-v1:0":{"description":"Claude 3 Sonnet d\'Anthropic atteint un \xe9quilibre id\xe9al entre intelligence et vitesse, particuli\xe8rement adapt\xe9 aux charges de travail d\'entreprise. Il offre une utilit\xe9 maximale \xe0 un prix inf\xe9rieur \xe0 celui des concurrents, con\xe7u pour \xeatre un mod\xe8le fiable et durable, adapt\xe9 aux d\xe9ploiements AI \xe0 grande \xe9chelle. Claude 3 Sonnet peut traiter des images et retourner des sorties textuelles, avec une fen\xeatre contextuelle de 200K."},"anthropic.claude-instant-v1":{"description":"Un mod\xe8le rapide, \xe9conomique et toujours tr\xe8s capable, capable de traiter une s\xe9rie de t\xe2ches, y compris des conversations quotidiennes, l\'analyse de texte, le r\xe9sum\xe9 et les questions-r\xe9ponses sur des documents."},"anthropic.claude-v2":{"description":"Anthropic a d\xe9montr\xe9 une grande capacit\xe9 dans une large gamme de t\xe2ches, allant des dialogues complexes \xe0 la g\xe9n\xe9ration de contenu cr\xe9atif, en passant par le suivi d\xe9taill\xe9 des instructions."},"anthropic.claude-v2:1":{"description":"Version mise \xe0 jour de Claude 2, avec une fen\xeatre contextuelle doubl\xe9e, ainsi que des am\xe9liorations en fiabilit\xe9, taux d\'hallucination et pr\xe9cision bas\xe9e sur des preuves dans des documents longs et des contextes RAG."},"anthropic/claude-3-haiku":{"description":"Claude 3 Haiku est le mod\xe8le le plus rapide d\'Anthropic \xe0 ce jour, con\xe7u pour les charges de travail d\'entreprise impliquant g\xe9n\xe9ralement des invites longues. Haiku peut analyser rapidement de nombreux documents, tels que rapports trimestriels, contrats ou dossiers juridiques, \xe0 un co\xfbt moiti\xe9 moindre que d\'autres mod\xe8les de sa cat\xe9gorie."},"anthropic/claude-3-opus":{"description":"Claude 3 Opus est le mod\xe8le le plus intelligent d\'Anthropic, offrant des performances de pointe sur des t\xe2ches tr\xe8s complexes. Il ma\xeetrise avec fluidit\xe9 et compr\xe9hension humaine les invites ouvertes et les sc\xe9narios in\xe9dits."},"anthropic/claude-3.5-haiku":{"description":"Claude 3.5 Haiku est la nouvelle g\xe9n\xe9ration de notre mod\xe8le le plus rapide. Avec une vitesse comparable \xe0 Claude 3 Haiku, il am\xe9liore chaque comp\xe9tence et d\xe9passe dans de nombreux benchmarks intelligents notre plus grand mod\xe8le pr\xe9c\xe9dent, Claude 3 Opus."},"anthropic/claude-3.5-sonnet":{"description":"Claude 3.5 Sonnet atteint un \xe9quilibre id\xe9al entre intelligence et vitesse, particuli\xe8rement adapt\xe9 aux charges de travail d\'entreprise. Par rapport \xe0 ses pairs, il offre des performances puissantes \xe0 moindre co\xfbt, con\xe7u pour une haute durabilit\xe9 dans les d\xe9ploiements d\'IA \xe0 grande \xe9chelle."},"anthropic/claude-3.7-sonnet":{"description":"Claude 3.7 Sonnet est le premier mod\xe8le hybride de raisonnement et le plus intelligent d\'Anthropic \xe0 ce jour. Il offre des performances de pointe en codage, g\xe9n\xe9ration de contenu, analyse de donn\xe9es et planification, s\'appuyant sur les capacit\xe9s en ing\xe9nierie logicielle et informatique de son pr\xe9d\xe9cesseur Claude 3.5 Sonnet."},"anthropic/claude-opus-4":{"description":"Claude Opus 4 est le mod\xe8le le plus puissant d\'Anthropic et le meilleur mod\xe8le de codage au monde, en t\xeate sur SWE-bench (72,5 %) et Terminal-bench (43,2 %). Il assure des performances durables pour des t\xe2ches longues n\xe9cessitant concentration et milliers d\'\xe9tapes, capable de fonctionner plusieurs heures d\'affil\xe9e, \xe9tendant significativement les capacit\xe9s des agents IA."},"anthropic/claude-opus-4.1":{"description":"Claude Opus 4.1 est une alternative plug-and-play \xe0 Opus 4, offrant des performances et une pr\xe9cision exceptionnelles pour les t\xe2ches de codage et d\'agent. Il porte la performance de codage \xe0 74,5 % sur SWE-bench Verified, traitant les probl\xe8mes complexes \xe0 plusieurs \xe9tapes avec rigueur et souci du d\xe9tail accrus."},"anthropic/claude-sonnet-4":{"description":"Claude Sonnet 4 am\xe9liore significativement les capacit\xe9s de Sonnet 3.7, excelle en codage avec un score de pointe de 72,7 % sur SWE-bench. Ce mod\xe8le \xe9quilibre performance et efficacit\xe9, adapt\xe9 aux cas d\'usage internes et externes, avec un contr\xf4le accru gr\xe2ce \xe0 une meilleure contr\xf4labilit\xe9."},"anthropic/claude-sonnet-4.5":{"description":"Claude Sonnet 4.5 est le mod\xe8le le plus intelligent d\'Anthropic \xe0 ce jour."},"ascend-tribe/pangu-pro-moe":{"description":"Pangu-Pro-MoE 72B-A16B est un grand mod\xe8le de langage sparse \xe0 72 milliards de param\xe8tres, avec 16 milliards de param\xe8tres activ\xe9s. Il repose sur une architecture Mixture of Experts group\xe9e (MoGE), qui regroupe les experts lors de la s\xe9lection et contraint chaque token \xe0 activer un nombre \xe9gal d\'experts dans chaque groupe, assurant ainsi un \xe9quilibre de charge entre les experts et am\xe9liorant consid\xe9rablement l\'efficacit\xe9 de d\xe9ploiement sur la plateforme Ascend."},"aya":{"description":"Aya 23 est un mod\xe8le multilingue lanc\xe9 par Cohere, prenant en charge 23 langues, facilitant les applications linguistiques diversifi\xe9es."},"aya:35b":{"description":"Aya 23 est un mod\xe8le multilingue lanc\xe9 par Cohere, prenant en charge 23 langues, facilitant les applications linguistiques diversifi\xe9es."},"azure-DeepSeek-R1-0528":{"description":"D\xe9ploy\xe9 et fourni par Microsoft ; le mod\xe8le DeepSeek R1 a b\xe9n\xe9fici\xe9 d\'une mise \xe0 jour mineure, la version actuelle \xe9tant DeepSeek-R1-0528. Dans la derni\xe8re mise \xe0 jour, DeepSeek R1 a consid\xe9rablement am\xe9lior\xe9 sa profondeur d\'inf\xe9rence et ses capacit\xe9s de raisonnement gr\xe2ce \xe0 l\'augmentation des ressources de calcul et \xe0 l\'introduction d\'un m\xe9canisme d\'optimisation algorithmique en phase post-entra\xeenement. Ce mod\xe8le excelle dans plusieurs benchmarks, notamment en math\xe9matiques, programmation et logique g\xe9n\xe9rale, avec des performances globales proches des mod\xe8les de pointe tels que O3 et Gemini 2.5 Pro."},"baichuan-m2-32b":{"description":"Baichuan M2 32B est un mod\xe8le \xe0 experts mixtes d\xe9velopp\xe9 par Baichuan Intelligence, dot\xe9 de puissantes capacit\xe9s de raisonnement."},"baichuan/baichuan2-13b-chat":{"description":"Baichuan-13B est un mod\xe8le de langage open source et commercialisable d\xe9velopp\xe9 par Baichuan Intelligence, contenant 13 milliards de param\xe8tres, qui a obtenu les meilleurs r\xe9sultats dans des benchmarks chinois et anglais de r\xe9f\xe9rence."},"baidu/ERNIE-4.5-300B-A47B":{"description":"ERNIE-4.5-300B-A47B est un grand mod\xe8le de langage d\xe9velopp\xe9 par Baidu, bas\xe9 sur une architecture Mixture of Experts (MoE). Avec un total de 300 milliards de param\xe8tres, il n\'active que 47 milliards de param\xe8tres par token lors de l\'inf\xe9rence, garantissant ainsi une performance puissante tout en optimisant l\'efficacit\xe9 de calcul. En tant que mod\xe8le central de la s\xe9rie ERNIE 4.5, il excelle dans la compr\xe9hension, la g\xe9n\xe9ration, le raisonnement textuel et la programmation. Ce mod\xe8le utilise une m\xe9thode innovante de pr\xe9-entra\xeenement multimodal h\xe9t\xe9rog\xe8ne MoE, combinant entra\xeenement sur texte et vision, ce qui am\xe9liore ses capacit\xe9s globales, notamment dans le suivi des instructions et la m\xe9moire des connaissances mondiales."},"c4ai-aya-expanse-32b":{"description":"Aya Expanse est un mod\xe8le multilingue haute performance de 32B, con\xe7u pour d\xe9fier les performances des mod\xe8les monolingues gr\xe2ce \xe0 des innovations en mati\xe8re d\'optimisation par instructions, d\'arbitrage de donn\xe9es, d\'entra\xeenement de pr\xe9f\xe9rences et de fusion de mod\xe8les. Il prend en charge 23 langues."},"c4ai-aya-expanse-8b":{"description":"Aya Expanse est un mod\xe8le multilingue haute performance de 8B, con\xe7u pour d\xe9fier les performances des mod\xe8les monolingues gr\xe2ce \xe0 des innovations en mati\xe8re d\'optimisation par instructions, d\'arbitrage de donn\xe9es, d\'entra\xeenement de pr\xe9f\xe9rences et de fusion de mod\xe8les. Il prend en charge 23 langues."},"c4ai-aya-vision-32b":{"description":"Aya Vision est un mod\xe8le multimodal de pointe, offrant d\'excellentes performances sur plusieurs benchmarks cl\xe9s en mati\xe8re de langage, de texte et d\'image. Cette version de 32 milliards de param\xe8tres se concentre sur des performances multilingues de pointe."},"c4ai-aya-vision-8b":{"description":"Aya Vision est un mod\xe8le multimodal de pointe, offrant d\'excellentes performances sur plusieurs benchmarks cl\xe9s en mati\xe8re de langage, de texte et d\'image. Cette version de 8 milliards de param\xe8tres se concentre sur une faible latence et des performances optimales."},"charglm-3":{"description":"CharGLM-3 est con\xe7u pour le jeu de r\xf4le et l\'accompagnement \xe9motionnel, prenant en charge une m\xe9moire multi-tours ultra-longue et des dialogues personnalis\xe9s, avec des applications vari\xe9es."},"charglm-4":{"description":"CharGLM-4 est con\xe7u pour le jeu de r\xf4le et l\'accompagnement \xe9motionnel, prenant en charge une m\xe9moire multi-tours ultra-longue et des dialogues personnalis\xe9s, avec une large gamme d\'applications."},"chatgpt-4o-latest":{"description":"ChatGPT-4o est un mod\xe8le dynamique, mis \xe0 jour en temps r\xe9el pour rester \xe0 jour avec la derni\xe8re version. Il combine une compr\xe9hension et une g\xe9n\xe9ration de langage puissantes, adapt\xe9 \xe0 des sc\xe9narios d\'application \xe0 grande \xe9chelle, y compris le service client, l\'\xe9ducation et le support technique."},"claude-2.0":{"description":"Claude 2 offre des avanc\xe9es cl\xe9s pour les entreprises, y compris un contexte de 200K jetons, une r\xe9duction significative du taux d\'illusion du mod\xe8le, des invites syst\xe8me et une nouvelle fonctionnalit\xe9 de test : l\'appel d\'outils."},"claude-2.1":{"description":"Claude 2 offre des avanc\xe9es cl\xe9s pour les entreprises, y compris un contexte de 200K jetons, une r\xe9duction significative du taux d\'illusion du mod\xe8le, des invites syst\xe8me et une nouvelle fonctionnalit\xe9 de test : l\'appel d\'outils."},"claude-3-5-haiku-20241022":{"description":"Claude 3.5 Haiku est le mod\xe8le de prochaine g\xe9n\xe9ration le plus rapide d\'Anthropic. Par rapport \xe0 Claude 3 Haiku, Claude 3.5 Haiku a am\xe9lior\xe9 ses comp\xe9tences dans tous les domaines et a surpass\xe9 le plus grand mod\xe8le de la g\xe9n\xe9ration pr\xe9c\xe9dente, Claude 3 Opus, dans de nombreux tests de r\xe9f\xe9rence intellectuels."},"claude-3-5-haiku-latest":{"description":"Claude 3.5 Haiku offre des r\xe9ponses rapides, id\xe9al pour les t\xe2ches l\xe9g\xe8res."},"claude-3-7-sonnet-20250219":{"description":"Claude 3.7 Sonnet \xe9l\xe8ve les normes de l\'industrie, surpassant les mod\xe8les concurrents et Claude 3 Opus, avec d\'excellentes performances dans une large gamme d\'\xe9valuations, tout en offrant la vitesse et le co\xfbt de nos mod\xe8les de niveau interm\xe9diaire."},"claude-3-7-sonnet-latest":{"description":"Claude 3.7 Sonnet est le mod\xe8le le plus puissant d\'Anthropic pour traiter des t\xe2ches tr\xe8s complexes. Il excelle en performance, intelligence, fluidit\xe9 et compr\xe9hension."},"claude-3-haiku-20240307":{"description":"Claude 3 Haiku est le mod\xe8le le plus rapide et le plus compact d\'Anthropic, con\xe7u pour des r\xe9ponses quasi instantan\xe9es. Il pr\xe9sente des performances directionnelles rapides et pr\xe9cises."},"claude-3-opus-20240229":{"description":"Claude 3 Opus est le mod\xe8le le plus puissant d\'Anthropic pour traiter des t\xe2ches hautement complexes. Il excelle en performance, intelligence, fluidit\xe9 et compr\xe9hension."},"claude-3-sonnet-20240229":{"description":"Claude 3 Sonnet offre un \xe9quilibre id\xe9al entre intelligence et vitesse pour les charges de travail d\'entreprise. Il fournit une utilit\xe9 maximale \xe0 un co\xfbt inf\xe9rieur, fiable et adapt\xe9 \xe0 un d\xe9ploiement \xe0 grande \xe9chelle."},"claude-haiku-4-5-20251001":{"description":"Claude Haiku 4.5 est le mod\xe8le Haiku le plus rapide et le plus intelligent d\'Anthropic, offrant une vitesse fulgurante et des capacit\xe9s de raisonnement \xe9tendues."},"claude-opus-4-1-20250805":{"description":"Claude Opus 4.1 est le mod\xe8le le plus puissant d\'Anthropic pour traiter des t\xe2ches hautement complexes. Il se distingue par ses performances, son intelligence, sa fluidit\xe9 et sa capacit\xe9 de compr\xe9hension exceptionnelles."},"claude-opus-4-1-20250805-thinking":{"description":"Claude Opus 4.1 est un mod\xe8le de r\xe9flexion capable de d\xe9montrer un raisonnement avanc\xe9."},"claude-opus-4-20250514":{"description":"Claude Opus 4 est le mod\xe8le le plus puissant d\'Anthropic pour traiter des t\xe2ches hautement complexes. Il excelle en performance, intelligence, fluidit\xe9 et compr\xe9hension."},"claude-sonnet-4-20250514":{"description":"Claude Sonnet 4 peut g\xe9n\xe9rer des r\xe9ponses quasi instantan\xe9es ou des r\xe9flexions prolong\xe9es \xe9tape par \xe9tape, visibles clairement par l\'utilisateur."},"claude-sonnet-4-20250514-thinking":{"description":"Le mod\xe8le de r\xe9flexion Claude Sonnet 4 produit des r\xe9ponses quasi instantan\xe9es ou des raisonnements prolong\xe9s \xe9tape par \xe9tape, clairement visibles par l\'utilisateur."},"claude-sonnet-4-5-20250929":{"description":"Claude Sonnet 4.5 est le mod\xe8le le plus intelligent d\'Anthropic \xe0 ce jour."},"codegeex-4":{"description":"CodeGeeX-4 est un puissant assistant de programmation AI, prenant en charge des questions intelligentes et l\'ach\xe8vement de code dans divers langages de programmation, am\xe9liorant l\'efficacit\xe9 du d\xe9veloppement."},"codegeex4-all-9b":{"description":"CodeGeeX4-ALL-9B est un mod\xe8le de g\xe9n\xe9ration de code multilingue, offrant des fonctionnalit\xe9s compl\xe8tes, y compris la compl\xe9tion et la g\xe9n\xe9ration de code, un interpr\xe9teur de code, une recherche sur le web, des appels de fonction et des questions-r\xe9ponses sur le code au niveau des d\xe9p\xf4ts, couvrant divers sc\xe9narios de d\xe9veloppement logiciel. C\'est un mod\xe8le de g\xe9n\xe9ration de code de premier plan avec moins de 10B de param\xe8tres."},"codegemma":{"description":"CodeGemma est un mod\xe8le de langage l\xe9ger d\xe9di\xe9 \xe0 diff\xe9rentes t\xe2ches de programmation, prenant en charge une it\xe9ration et une int\xe9gration rapides."},"codegemma:2b":{"description":"CodeGemma est un mod\xe8le de langage l\xe9ger d\xe9di\xe9 \xe0 diff\xe9rentes t\xe2ches de programmation, prenant en charge une it\xe9ration et une int\xe9gration rapides."},"codellama":{"description":"Code Llama est un LLM ax\xe9 sur la g\xe9n\xe9ration et la discussion de code, combinant un large support de langages de programmation, adapt\xe9 aux environnements de d\xe9veloppement."},"codellama/CodeLlama-34b-Instruct-hf":{"description":"Code Llama est un LLM ax\xe9 sur la g\xe9n\xe9ration et la discussion de code, combinant un large support de langages de programmation, adapt\xe9 aux environnements de d\xe9veloppement."},"codellama:13b":{"description":"Code Llama est un LLM ax\xe9 sur la g\xe9n\xe9ration et la discussion de code, combinant un large support de langages de programmation, adapt\xe9 aux environnements de d\xe9veloppement."},"codellama:34b":{"description":"Code Llama est un LLM ax\xe9 sur la g\xe9n\xe9ration et la discussion de code, combinant un large support de langages de programmation, adapt\xe9 aux environnements de d\xe9veloppement."},"codellama:70b":{"description":"Code Llama est un LLM ax\xe9 sur la g\xe9n\xe9ration et la discussion de code, combinant un large support de langages de programmation, adapt\xe9 aux environnements de d\xe9veloppement."},"codeqwen":{"description":"CodeQwen1.5 est un mod\xe8le de langage \xe0 grande \xe9chelle entra\xeen\xe9 sur une grande quantit\xe9 de donn\xe9es de code, con\xe7u pour r\xe9soudre des t\xe2ches de programmation complexes."},"codestral":{"description":"Codestral est le premier mod\xe8le de code de Mistral AI, offrant un excellent soutien pour les t\xe2ches de g\xe9n\xe9ration de code."},"codestral-latest":{"description":"Codestral est un mod\xe8le de g\xe9n\xe9ration de pointe ax\xe9 sur la g\xe9n\xe9ration de code, optimis\xe9 pour les t\xe2ches de remplissage interm\xe9diaire et de compl\xe9tion de code."},"codex-mini-latest":{"description":"codex-mini-latest est une version affin\xe9e de o4-mini, sp\xe9cialement con\xe7ue pour Codex CLI. Pour une utilisation directe via l\'API, nous recommandons de commencer par gpt-4.1."},"cogview-4":{"description":"CogView-4 est le premier mod\xe8le open source de g\xe9n\xe9ration d\'images \xe0 partir de texte de Zhizhu, prenant en charge la g\xe9n\xe9ration de caract\xe8res chinois. Il offre une am\xe9lioration globale en compr\xe9hension s\xe9mantique, qualit\xe9 de g\xe9n\xe9ration d\'images, et capacit\xe9 de g\xe9n\xe9ration de textes en chinois et en anglais. Il supporte une entr\xe9e bilingue chinois-anglais de longueur arbitraire et peut g\xe9n\xe9rer des images \xe0 n\'importe quelle r\xe9solution dans une plage donn\xe9e."},"cohere-command-r":{"description":"Command R est un mod\xe8le g\xe9n\xe9ratif \xe9volutif ciblant RAG et l\'utilisation d\'outils pour permettre une IA \xe0 l\'\xe9chelle de la production pour les entreprises."},"cohere-command-r-plus":{"description":"Command R+ est un mod\xe8le optimis\xe9 RAG de pointe con\xe7u pour traiter des charges de travail de niveau entreprise."},"cohere/Cohere-command-r":{"description":"Command R est un mod\xe8le g\xe9n\xe9ratif \xe9volutif con\xe7u pour l\'utilisation avec RAG et les outils, permettant aux entreprises de d\xe9ployer une IA de niveau production."},"cohere/Cohere-command-r-plus":{"description":"Command R+ est un mod\xe8le optimis\xe9 RAG de pointe, con\xe7u pour g\xe9rer des charges de travail d\'entreprise."},"cohere/command-a":{"description":"Command A est le mod\xe8le le plus performant de Cohere \xe0 ce jour, excellent dans l\'utilisation d\'outils, les agents, la g\xe9n\xe9ration augment\xe9e par r\xe9cup\xe9ration (RAG) et les cas multilingues. Avec une longueur de contexte de 256K, il fonctionne sur seulement deux GPU, offrant un d\xe9bit 150 % sup\xe9rieur \xe0 Command R+ 08-2024."},"cohere/command-r":{"description":"Command R est un grand mod\xe8le de langage optimis\xe9 pour les interactions conversationnelles et les t\xe2ches \xe0 long contexte. Il se positionne dans la cat\xe9gorie \\"scalable\\", \xe9quilibrant haute performance et forte pr\xe9cision, permettant aux entreprises de d\xe9passer la preuve de concept pour la production."},"cohere/command-r-plus":{"description":"Command R+ est le dernier grand mod\xe8le de langage de Cohere, optimis\xe9 pour les interactions conversationnelles et les t\xe2ches \xe0 long contexte. Il vise une performance exceptionnelle, permettant aux entreprises de passer de la preuve de concept \xe0 la production."},"cohere/embed-v4.0":{"description":"Un mod\xe8le permettant de classifier ou de transformer en embeddings des textes, images ou contenus mixtes."},"comfyui/flux-dev":{"description":"FLUX.1 Dev - Mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte de haute qualit\xe9, 10 \xe0 50 \xe9tapes de g\xe9n\xe9ration, id\xe9al pour la cr\xe9ation artistique et les œuvres visuelles de qualit\xe9."},"comfyui/flux-kontext-dev":{"description":"FLUX.1 Kontext-dev - Mod\xe8le d\'\xe9dition d\'image, permet de modifier des images existantes \xe0 l\'aide d\'instructions textuelles, avec prise en charge des modifications locales et du transfert de style."},"comfyui/flux-krea-dev":{"description":"FLUX.1 Krea-dev - Mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte avec s\xe9curit\xe9 renforc\xe9e, d\xe9velopp\xe9 en collaboration avec Krea, int\xe8gre un filtrage de s\xe9curit\xe9."},"comfyui/flux-schnell":{"description":"FLUX.1 Schnell - Mod\xe8le ultra-rapide de g\xe9n\xe9ration d\'images \xe0 partir de texte, produit des images de haute qualit\xe9 en 1 \xe0 4 \xe9tapes, id\xe9al pour les applications en temps r\xe9el et le prototypage rapide."},"comfyui/stable-diffusion-15":{"description":"Stable Diffusion 1.5 - Mod\xe8le classique de g\xe9n\xe9ration d\'images \xe0 partir de texte en r\xe9solution 512x512, adapt\xe9 au prototypage rapide et aux exp\xe9rimentations cr\xe9atives."},"comfyui/stable-diffusion-35":{"description":"Stable Diffusion 3.5 - Mod\xe8le de nouvelle g\xe9n\xe9ration pour la g\xe9n\xe9ration d\'images \xe0 partir de texte, disponible en versions Large et Medium, n\xe9cessite un fichier d\'encodeur CLIP externe, offre une qualit\xe9 d\'image exceptionnelle et une grande fid\xe9lit\xe9 aux invites textuelles."},"comfyui/stable-diffusion-35-inclclip":{"description":"Stable Diffusion 3.5 avec encodeur CLIP/T5 int\xe9gr\xe9 - Ne n\xe9cessite pas de fichier d\'encodeur externe, compatible avec des mod\xe8les comme sd3.5_medium_incl_clips, avec une consommation de ressources r\xe9duite."},"comfyui/stable-diffusion-custom":{"description":"Mod\xe8le personnalis\xe9 SD pour la g\xe9n\xe9ration d\'images \xe0 partir de texte. Le fichier du mod\xe8le doit \xeatre nomm\xe9 custom_sd_lobe.safetensors. Si un VAE est utilis\xe9, nommez-le custom_sd_vae_lobe.safetensors. Les fichiers doivent \xeatre plac\xe9s dans les dossiers requis selon les sp\xe9cifications de Comfy."},"comfyui/stable-diffusion-custom-refiner":{"description":"Mod\xe8le personnalis\xe9 SDXL pour la transformation d\'image \xe0 image. Le fichier du mod\xe8le doit \xeatre nomm\xe9 custom_sd_lobe.safetensors. Si un VAE est utilis\xe9, nommez-le custom_sd_vae_lobe.safetensors. Les fichiers doivent \xeatre plac\xe9s dans les dossiers requis selon les sp\xe9cifications de Comfy."},"comfyui/stable-diffusion-refiner":{"description":"Mod\xe8le SDXL de transformation d\'image \xe0 image, permet une conversion d\'image de haute qualit\xe9 \xe0 partir d\'une image source, avec prise en charge du transfert de style, de la restauration d\'image et des transformations cr\xe9atives."},"comfyui/stable-diffusion-xl":{"description":"Mod\xe8le SDXL de g\xe9n\xe9ration d\'images \xe0 partir de texte, prend en charge une r\xe9solution \xe9lev\xe9e de 1024x1024, offrant une qualit\xe9 d\'image sup\xe9rieure et un meilleur rendu des d\xe9tails."},"command":{"description":"Un mod\xe8le de dialogue qui suit des instructions, offrant une haute qualit\xe9 et une fiabilit\xe9 accrue dans les t\xe2ches linguistiques, avec une longueur de contexte plus longue que notre mod\xe8le de g\xe9n\xe9ration de base."},"command-a-03-2025":{"description":"Command A est notre mod\xe8le le plus performant \xe0 ce jour, offrant d\'excellentes performances dans l\'utilisation d\'outils, l\'agent, la g\xe9n\xe9ration augment\xe9e par r\xe9cup\xe9ration (RAG) et les applications multilingues. Command A a une longueur de contexte de 256K, n\xe9cessite seulement deux GPU pour fonctionner, et a am\xe9lior\xe9 le d\xe9bit de 150 % par rapport \xe0 Command R+ 08-2024."},"command-light":{"description":"Une version plus petite et plus rapide de Command, presque aussi puissante, mais plus rapide."},"command-light-nightly":{"description":"Pour r\xe9duire l\'intervalle de temps entre les versions majeures, nous avons lanc\xe9 une version nocturne du mod\xe8le Command. Pour la s\xe9rie command-light, cette version est appel\xe9e command-light-nightly. Veuillez noter que command-light-nightly est la version la plus r\xe9cente, la plus exp\xe9rimentale et (potentiellement) instable. Les versions nocturnes sont mises \xe0 jour r\xe9guli\xe8rement sans pr\xe9avis, il n\'est donc pas recommand\xe9 de les utiliser en production."},"command-nightly":{"description":"Pour r\xe9duire l\'intervalle de temps entre les versions majeures, nous avons lanc\xe9 une version nocturne du mod\xe8le Command. Pour la s\xe9rie Command, cette version est appel\xe9e command-cightly. Veuillez noter que command-nightly est la version la plus r\xe9cente, la plus exp\xe9rimentale et (potentiellement) instable. Les versions nocturnes sont mises \xe0 jour r\xe9guli\xe8rement sans pr\xe9avis, il n\'est donc pas recommand\xe9 de les utiliser en production."},"command-r":{"description":"Command R est un LLM optimis\xe9 pour les t\xe2ches de dialogue et de long contexte, particuli\xe8rement adapt\xe9 \xe0 l\'interaction dynamique et \xe0 la gestion des connaissances."},"command-r-03-2024":{"description":"Command R est un mod\xe8le de dialogue qui suit des instructions, offrant une qualit\xe9 sup\xe9rieure et une fiabilit\xe9 accrue dans les t\xe2ches linguistiques, avec une longueur de contexte plus longue que les mod\xe8les pr\xe9c\xe9dents. Il peut \xeatre utilis\xe9 pour des flux de travail complexes tels que la g\xe9n\xe9ration de code, la g\xe9n\xe9ration augment\xe9e par r\xe9cup\xe9ration (RAG), l\'utilisation d\'outils et l\'agent."},"command-r-08-2024":{"description":"command-r-08-2024 est une version mise \xe0 jour du mod\xe8le Command R, publi\xe9e en ao\xfbt 2024."},"command-r-plus":{"description":"Command R+ est un mod\xe8le de langage de grande taille \xe0 haute performance, con\xe7u pour des sc\xe9narios d\'entreprise r\xe9els et des applications complexes."},"command-r-plus-04-2024":{"description":"Command R+ est un mod\xe8le de dialogue qui suit des instructions, offrant une qualit\xe9 sup\xe9rieure et une fiabilit\xe9 accrue dans les t\xe2ches linguistiques, avec une longueur de contexte plus longue que les mod\xe8les pr\xe9c\xe9dents. Il est particuli\xe8rement adapt\xe9 aux flux de travail RAG complexes et \xe0 l\'utilisation d\'outils en plusieurs \xe9tapes."},"command-r-plus-08-2024":{"description":"Command R+ est un mod\xe8le de dialogue qui suit les instructions, offrant une qualit\xe9 sup\xe9rieure et une fiabilit\xe9 accrue dans les t\xe2ches linguistiques, avec une longueur de contexte plus longue par rapport aux mod\xe8les pr\xe9c\xe9dents. Il est particuli\xe8rement adapt\xe9 aux flux de travail RAG complexes et \xe0 l\'utilisation d\'outils en plusieurs \xe9tapes."},"command-r7b-12-2024":{"description":"command-r7b-12-2024 est une version mise \xe0 jour, petite et efficace, publi\xe9e en d\xe9cembre 2024. Il excelle dans les t\xe2ches n\xe9cessitant un raisonnement complexe et un traitement en plusieurs \xe9tapes, comme RAG, l\'utilisation d\'outils et l\'agent."},"computer-use-preview":{"description":"Le mod\xe8le computer-use-preview est un mod\xe8le d\xe9di\xe9 con\xe7u pour les \xab outils d\'utilisation informatique \xbb, entra\xeen\xe9 pour comprendre et ex\xe9cuter des t\xe2ches li\xe9es \xe0 l\'informatique."},"dall-e-2":{"description":"Le deuxi\xe8me mod\xe8le DALL\xb7E, prenant en charge la g\xe9n\xe9ration d\'images plus r\xe9alistes et pr\xe9cises, avec une r\xe9solution quatre fois sup\xe9rieure \xe0 celle de la premi\xe8re g\xe9n\xe9ration."},"dall-e-3":{"description":"Le dernier mod\xe8le DALL\xb7E, publi\xe9 en novembre 2023. Prend en charge la g\xe9n\xe9ration d\'images plus r\xe9alistes et pr\xe9cises, avec une meilleure expressivit\xe9 des d\xe9tails."},"databricks/dbrx-instruct":{"description":"DBRX Instruct offre des capacit\xe9s de traitement d\'instructions hautement fiables, prenant en charge des applications dans divers secteurs."},"deepseek-ai/DeepSeek-OCR":{"description":"DeepSeek-OCR est un mod\xe8le de langage visuel d\xe9velopp\xe9 par DeepSeek AI, sp\xe9cialis\xe9 dans la reconnaissance optique de caract\xe8res (OCR) et la \\"compression optique contextuelle\\". Ce mod\xe8le explore les limites de la compression d\'informations contextuelles \xe0 partir d\'images, permettant un traitement efficace des documents et leur conversion en formats de texte structur\xe9s tels que Markdown. Il est capable de reconna\xeetre avec pr\xe9cision le contenu textuel des images, ce qui le rend particuli\xe8rement adapt\xe9 \xe0 la num\xe9risation de documents, \xe0 l\'extraction de texte et au traitement structur\xe9."},"deepseek-ai/DeepSeek-R1":{"description":"DeepSeek-R1 est un mod\xe8le d\'inf\xe9rence aliment\xe9 par l\'apprentissage par renforcement (RL), qui r\xe9sout les probl\xe8mes de r\xe9p\xe9titivit\xe9 et de lisibilit\xe9 dans le mod\xe8le. Avant le RL, DeepSeek-R1 a introduit des donn\xe9es de d\xe9marrage \xe0 froid, optimisant ainsi les performances d\'inf\xe9rence. Il se compare \xe0 OpenAI-o1 en mati\xe8re de t\xe2ches math\xe9matiques, de code et d\'inf\xe9rence, et am\xe9liore l\'efficacit\xe9 globale gr\xe2ce \xe0 des m\xe9thodes d\'entra\xeenement soigneusement con\xe7ues."},"deepseek-ai/DeepSeek-R1-0528":{"description":"DeepSeek R1 am\xe9liore significativement la profondeur de ses capacit\xe9s de raisonnement et d’inf\xe9rence gr\xe2ce \xe0 l’utilisation accrue des ressources de calcul et \xe0 l’introduction de m\xe9canismes d’optimisation algorithmique durant la phase post-entra\xeenement. Ce mod\xe8le excelle dans divers benchmarks, notamment en math\xe9matiques, programmation et logique g\xe9n\xe9rale. Ses performances globales se rapprochent d\xe9sormais des mod\xe8les de pointe tels que O3 et Gemini 2.5 Pro."},"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B":{"description":"DeepSeek-R1-0528-Qwen3-8B est un mod\xe8le obtenu par distillation de la cha\xeene de pens\xe9e du mod\xe8le DeepSeek-R1-0528 vers Qwen3 8B Base. Ce mod\xe8le atteint des performances de pointe (SOTA) parmi les mod\xe8les open source, surpassant Qwen3 8B de 10 % lors du test AIME 2024 et atteignant le niveau de performance de Qwen3-235B-thinking. Il excelle dans les benchmarks de raisonnement math\xe9matique, programmation et logique g\xe9n\xe9rale, partageant la m\xeame architecture que Qwen3-8B mais utilisant la configuration de tokenizer de DeepSeek-R1-0528."},"deepseek-ai/DeepSeek-R1-Distill-Llama-70B":{"description":"Le mod\xe8le distill\xe9 DeepSeek-R1 optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B":{"description":"Le mod\xe8le distill\xe9 DeepSeek-R1 optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B":{"description":"Le mod\xe8le distill\xe9 DeepSeek-R1 optimise les performances d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement et aux donn\xe9es de d\xe9marrage \xe0 froid, rafra\xeechissant les r\xe9f\xe9rences multi-t\xe2ches des mod\xe8les open source."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":{"description":"DeepSeek-R1-Distill-Qwen-32B est un mod\xe8le obtenu par distillation de Qwen2.5-32B. Ce mod\xe8le a \xe9t\xe9 affin\xe9 avec 800 000 \xe9chantillons s\xe9lectionn\xe9s g\xe9n\xe9r\xe9s par DeepSeek-R1, montrant des performances exceptionnelles dans plusieurs domaines tels que les math\xe9matiques, la programmation et le raisonnement. Il a obtenu d\'excellents r\xe9sultats dans plusieurs tests de r\xe9f\xe9rence, atteignant 94,3 % de pr\xe9cision dans MATH-500, d\xe9montrant une forte capacit\xe9 de raisonnement math\xe9matique."},"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B":{"description":"DeepSeek-R1-Distill-Qwen-7B est un mod\xe8le obtenu par distillation de Qwen2.5-Math-7B. Ce mod\xe8le a \xe9t\xe9 affin\xe9 avec 800 000 \xe9chantillons s\xe9lectionn\xe9s g\xe9n\xe9r\xe9s par DeepSeek-R1, montrant d\'excellentes capacit\xe9s d\'inf\xe9rence. Il a obtenu d\'excellents r\xe9sultats dans plusieurs tests de r\xe9f\xe9rence, atteignant 92,8 % de pr\xe9cision dans MATH-500, 55,5 % de taux de r\xe9ussite dans AIME 2024, et un score de 1189 sur CodeForces, d\xe9montrant de fortes capacit\xe9s en math\xe9matiques et en programmation pour un mod\xe8le de 7B."},"deepseek-ai/DeepSeek-V2.5":{"description":"DeepSeek V2.5 int\xe8gre les excellentes caract\xe9ristiques des versions pr\xe9c\xe9dentes, renfor\xe7ant les capacit\xe9s g\xe9n\xe9rales et de codage."},"deepseek-ai/DeepSeek-V3":{"description":"DeepSeek-V3 est un mod\xe8le de langage \xe0 experts mixtes (MoE) avec 6710 milliards de param\xe8tres, utilisant une attention potentielle multi-t\xeate (MLA) et l\'architecture DeepSeekMoE, combin\xe9e \xe0 une strat\xe9gie d\'\xe9quilibrage de charge sans perte auxiliaire, optimisant ainsi l\'efficacit\xe9 d\'inf\xe9rence et d\'entra\xeenement. En pr\xe9-entra\xeenant sur 14,8 billions de tokens de haute qualit\xe9, suivi d\'un ajustement supervis\xe9 et d\'apprentissage par renforcement, DeepSeek-V3 surpasse les autres mod\xe8les open source en termes de performance, se rapprochant des mod\xe8les ferm\xe9s de premier plan."},"deepseek-ai/DeepSeek-V3.1":{"description":"Le mod\xe8le DeepSeek V3.1 est bas\xe9 sur une architecture de raisonnement hybride, prenant en charge \xe0 la fois les modes de r\xe9flexion et non r\xe9flexifs."},"deepseek-ai/DeepSeek-V3.1-Terminus":{"description":"DeepSeek-V3.1-Terminus est une version mise \xe0 jour du mod\xe8le V3.1 publi\xe9e par DeepSeek, positionn\xe9e comme un grand mod\xe8le de langage hybride pour agents intelligents. Cette mise \xe0 jour conserve les capacit\xe9s originales du mod\xe8le tout en se concentrant sur la correction des probl\xe8mes signal\xe9s par les utilisateurs et l\'am\xe9lioration de la stabilit\xe9. Elle am\xe9liore significativement la coh\xe9rence linguistique, r\xe9duisant le m\xe9lange de chinois et d\'anglais ainsi que l\'apparition de caract\xe8res anormaux. Le mod\xe8le int\xe8gre un \xab mode r\xe9flexion \xbb (Thinking Mode) et un \xab mode non-r\xe9flexion \xbb (Non-thinking Mode), permettant aux utilisateurs de basculer facilement entre ces modes via des mod\xe8les de conversation adapt\xe9s \xe0 diff\xe9rentes t\xe2ches. En tant qu\'optimisation majeure, V3.1-Terminus renforce les performances des agents de code (Code Agent) et de recherche (Search Agent), rendant leur appel d\'outils et l\'ex\xe9cution de t\xe2ches complexes en plusieurs \xe9tapes plus fiables."},"deepseek-ai/DeepSeek-V3.2-Exp":{"description":"DeepSeek-V3.2-Exp est une version exp\xe9rimentale V3.2 publi\xe9e par DeepSeek, servant d\'\xe9tape interm\xe9diaire vers une architecture de nouvelle g\xe9n\xe9ration. Bas\xe9e sur la V3.1-Terminus, elle introduit le m\xe9canisme d\'attention clairsem\xe9e DeepSeek (DeepSeek Sparse Attention, DSA) afin d\'am\xe9liorer l\'efficacit\xe9 de l\'entra\xeenement et de l\'inf\xe9rence sur de longs contextes. Elle est sp\xe9cialement optimis\xe9e pour l\'appel d\'outils, la compr\xe9hension de longs documents et le raisonnement en plusieurs \xe9tapes. V3.2-Exp fait le lien entre la recherche et l\'industrialisation, id\xe9ale pour les utilisateurs souhaitant explorer une efficacit\xe9 de raisonnement accrue dans des sc\xe9narios \xe0 budget contextuel \xe9lev\xe9."},"deepseek-ai/deepseek-llm-67b-chat":{"description":"DeepSeek 67B est un mod\xe8le avanc\xe9 form\xe9 pour des dialogues de haute complexit\xe9."},"deepseek-ai/deepseek-r1":{"description":"LLM avanc\xe9 et efficace, sp\xe9cialis\xe9 dans le raisonnement, les math\xe9matiques et la programmation."},"deepseek-ai/deepseek-v3.1":{"description":"DeepSeek V3.1 : mod\xe8le de raisonnement de nouvelle g\xe9n\xe9ration, am\xe9liorant les capacit\xe9s de raisonnement complexe et de r\xe9flexion en cha\xeene, adapt\xe9 aux t\xe2ches n\xe9cessitant une analyse approfondie."},"deepseek-ai/deepseek-v3.1-terminus":{"description":"DeepSeek V3.1 : un mod\xe8le de raisonnement de nouvelle g\xe9n\xe9ration, am\xe9liorant les capacit\xe9s de raisonnement complexe et de pens\xe9e en cha\xeene, id\xe9al pour les t\xe2ches n\xe9cessitant une analyse approfondie."},"deepseek-ai/deepseek-vl2":{"description":"DeepSeek-VL2 est un mod\xe8le de langage visuel \xe0 experts mixtes (MoE) d\xe9velopp\xe9 sur la base de DeepSeekMoE-27B, utilisant une architecture MoE \xe0 activation sparse, r\xe9alisant des performances exceptionnelles tout en n\'activant que 4,5 milliards de param\xe8tres. Ce mod\xe8le excelle dans plusieurs t\xe2ches telles que la question-r\xe9ponse visuelle, la reconnaissance optique de caract\xe8res, la compr\xe9hension de documents/tableaux/graphes et le positionnement visuel."},"deepseek-chat":{"description":"Un nouveau mod\xe8le open source qui fusionne des capacit\xe9s g\xe9n\xe9rales et de code, conservant non seulement la capacit\xe9 de dialogue g\xe9n\xe9ral du mod\xe8le Chat d\'origine et la puissante capacit\xe9 de traitement de code du mod\xe8le Coder, mais s\'alignant \xe9galement mieux sur les pr\xe9f\xe9rences humaines. De plus, DeepSeek-V2.5 a r\xe9alis\xe9 des am\xe9liorations significatives dans plusieurs domaines tels que les t\xe2ches d\'\xe9criture et le suivi des instructions."},"deepseek-coder-33B-instruct":{"description":"DeepSeek Coder 33B est un mod\xe8le de langage de code, entra\xeen\xe9 sur 20 trillions de donn\xe9es, dont 87 % sont du code et 13 % des langues chinoise et anglaise. Le mod\xe8le introduit une taille de fen\xeatre de 16K et des t\xe2ches de remplissage, offrant des fonctionnalit\xe9s de compl\xe9tion de code et de remplissage de fragments au niveau des projets."},"deepseek-coder-v2":{"description":"DeepSeek Coder V2 est un mod\xe8le de code open source de type expert mixte, performant dans les t\xe2ches de code, rivalisant avec GPT4-Turbo."},"deepseek-coder-v2:236b":{"description":"DeepSeek Coder V2 est un mod\xe8le de code open source de type expert mixte, performant dans les t\xe2ches de code, rivalisant avec GPT4-Turbo."},"deepseek-r1":{"description":"DeepSeek-R1 est un mod\xe8le d\'inf\xe9rence aliment\xe9 par l\'apprentissage par renforcement (RL), qui r\xe9sout les probl\xe8mes de r\xe9p\xe9titivit\xe9 et de lisibilit\xe9 dans le mod\xe8le. Avant le RL, DeepSeek-R1 a introduit des donn\xe9es de d\xe9marrage \xe0 froid, optimisant ainsi les performances d\'inf\xe9rence. Il se compare \xe0 OpenAI-o1 en mati\xe8re de t\xe2ches math\xe9matiques, de code et d\'inf\xe9rence, et am\xe9liore l\'efficacit\xe9 globale gr\xe2ce \xe0 des m\xe9thodes d\'entra\xeenement soigneusement con\xe7ues."},"deepseek-r1-0528":{"description":"Mod\xe8le complet de 685 milliards de param\xe8tres, publi\xe9 le 28 mai 2025. DeepSeek-R1 utilise massivement l\'apprentissage par renforcement en phase post-entra\xeenement, am\xe9liorant consid\xe9rablement les capacit\xe9s de raisonnement du mod\xe8le avec tr\xe8s peu de donn\xe9es annot\xe9es. Il excelle en math\xe9matiques, codage, raisonnement en langage naturel et autres t\xe2ches complexes."},"deepseek-r1-250528":{"description":"DeepSeek R1 250528, version compl\xe8te du mod\xe8le d\'inf\xe9rence DeepSeek-R1, id\xe9al pour les t\xe2ches complexes en math\xe9matiques et en logique."},"deepseek-r1-70b-fast-online":{"description":"DeepSeek R1 70B version rapide, prenant en charge la recherche en ligne en temps r\xe9el, offrant une vitesse de r\xe9ponse plus rapide tout en maintenant les performances du mod\xe8le."},"deepseek-r1-70b-online":{"description":"DeepSeek R1 70B version standard, prenant en charge la recherche en ligne en temps r\xe9el, adapt\xe9e aux t\xe2ches de dialogue et de traitement de texte n\xe9cessitant des informations \xe0 jour."},"deepseek-r1-distill-llama":{"description":"deepseek-r1-distill-llama est un mod\xe8le d\xe9riv\xe9 par distillation de DeepSeek-R1 \xe0 partir de Llama."},"deepseek-r1-distill-llama-70b":{"description":"DeepSeek R1 Distill Llama 70B, mod\xe8le distill\xe9 combinant les capacit\xe9s d\'inf\xe9rence R1 avec l\'\xe9cosyst\xe8me Llama."},"deepseek-r1-distill-llama-8b":{"description":"DeepSeek-R1-Distill-Llama-8B est un grand mod\xe8le de langage distill\xe9 bas\xe9 sur Llama-3.1-8B, utilisant les sorties de DeepSeek R1."},"deepseek-r1-distill-qianfan-70b":{"description":"DeepSeek R1 Distill Qianfan 70B, mod\xe8le distill\xe9 R1 bas\xe9 sur Qianfan-70B, offrant un excellent rapport qualit\xe9-prix."},"deepseek-r1-distill-qianfan-8b":{"description":"DeepSeek R1 Distill Qianfan 8B, mod\xe8le distill\xe9 R1 bas\xe9 sur Qianfan-8B, adapt\xe9 aux applications de taille moyenne \xe0 petite."},"deepseek-r1-distill-qianfan-llama-70b":{"description":"DeepSeek R1 Distill Qianfan Llama 70B, mod\xe8le distill\xe9 R1 bas\xe9 sur Llama-70B."},"deepseek-r1-distill-qwen":{"description":"deepseek-r1-distill-qwen est un mod\xe8le d\xe9riv\xe9 par distillation de Qwen \xe0 partir de DeepSeek-R1."},"deepseek-r1-distill-qwen-1.5b":{"description":"DeepSeek R1 Distill Qwen 1.5B, mod\xe8le distill\xe9 R1 ultra-l\xe9ger, con\xe7u pour les environnements \xe0 tr\xe8s faibles ressources."},"deepseek-r1-distill-qwen-14b":{"description":"DeepSeek R1 Distill Qwen 14B, mod\xe8le distill\xe9 R1 de taille moyenne, adapt\xe9 \xe0 un d\xe9ploiement multi-sc\xe9narios."},"deepseek-r1-distill-qwen-32b":{"description":"DeepSeek R1 Distill Qwen 32B, mod\xe8le distill\xe9 R1 bas\xe9 sur Qwen-32B, \xe9quilibrant performance et co\xfbt."},"deepseek-r1-distill-qwen-7b":{"description":"DeepSeek R1 Distill Qwen 7B, mod\xe8le distill\xe9 R1 l\xe9ger, id\xe9al pour les environnements en p\xe9riph\xe9rie ou priv\xe9s d\'entreprise."},"deepseek-r1-fast-online":{"description":"DeepSeek R1 version rapide compl\xe8te, prenant en charge la recherche en ligne en temps r\xe9el, combinant la puissance des 671B de param\xe8tres avec une vitesse de r\xe9ponse plus rapide."},"deepseek-r1-online":{"description":"DeepSeek R1 version compl\xe8te, avec 671B de param\xe8tres, prenant en charge la recherche en ligne en temps r\xe9el, offrant des capacit\xe9s de compr\xe9hension et de g\xe9n\xe9ration plus puissantes."},"deepseek-reasoner":{"description":"Mode de r\xe9flexion DeepSeek V3.2. Avant de fournir la r\xe9ponse finale, le mod\xe8le g\xe9n\xe8re une cha\xeene de pens\xe9e pour am\xe9liorer la pr\xe9cision de la r\xe9ponse."},"deepseek-v2":{"description":"DeepSeek V2 est un mod\xe8le de langage Mixture-of-Experts efficace, adapt\xe9 aux besoins de traitement \xe9conomique."},"deepseek-v2:236b":{"description":"DeepSeek V2 236B est le mod\xe8le de code de conception de DeepSeek, offrant de puissantes capacit\xe9s de g\xe9n\xe9ration de code."},"deepseek-v3":{"description":"DeepSeek-V3 est un mod\xe8le MoE d\xe9velopp\xe9 par la soci\xe9t\xe9 Hangzhou DeepSeek AI Technology Research Co., Ltd., avec des performances exceptionnelles dans plusieurs \xe9valuations, se classant au premier rang des mod\xe8les open source dans les classements principaux. Par rapport au mod\xe8le V2.5, la vitesse de g\xe9n\xe9ration a \xe9t\xe9 multipli\xe9e par 3, offrant aux utilisateurs une exp\xe9rience d\'utilisation plus rapide et fluide."},"deepseek-v3-0324":{"description":"DeepSeek-V3-0324 est un mod\xe8le MoE de 671 milliards de param\xe8tres, se distinguant par ses capacit\xe9s en programmation et en technique, ainsi que par sa compr\xe9hension du contexte et son traitement de longs textes."},"deepseek-v3.1":{"description":"DeepSeek-V3.1 est un nouveau mod\xe8le d\'inf\xe9rence hybride lanc\xe9 par DeepSeek, prenant en charge deux modes d\'inf\xe9rence : r\xe9fl\xe9chi et non r\xe9fl\xe9chi, avec une efficacit\xe9 de r\xe9flexion sup\xe9rieure \xe0 celle de DeepSeek-R1-0528. Optimis\xe9 par post-entra\xeenement, l\'utilisation des outils Agent et les performances des t\xe2ches des agents ont \xe9t\xe9 grandement am\xe9lior\xe9es. Supporte une fen\xeatre contextuelle de 128k et une longueur de sortie maximale de 64k tokens."},"deepseek-v3.1-terminus":{"description":"DeepSeek-V3.1-Terminus est une version optimis\xe9e pour terminaux du grand mod\xe8le linguistique lanc\xe9 par DeepSeek, sp\xe9cialement con\xe7u pour les appareils terminaux."},"deepseek-v3.1-think-250821":{"description":"DeepSeek V3.1 Think 250821, mod\xe8le de r\xe9flexion avanc\xe9e correspondant \xe0 la version Terminus, con\xe7u pour des sc\xe9narios d\'inf\xe9rence haute performance."},"deepseek-v3.1:671b":{"description":"DeepSeek V3.1 : mod\xe8le de raisonnement de nouvelle g\xe9n\xe9ration, am\xe9liorant les capacit\xe9s de raisonnement complexe et de r\xe9flexion en cha\xeene, adapt\xe9 aux t\xe2ches n\xe9cessitant une analyse approfondie."},"deepseek-v3.2-exp":{"description":"deepseek-v3.2-exp introduit un m\xe9canisme d\'attention parcimonieuse, visant \xe0 am\xe9liorer l\'efficacit\xe9 de l\'entra\xeenement et de l\'inf\xe9rence lors du traitement de longs textes, \xe0 un prix inf\xe9rieur \xe0 celui de deepseek-v3.1."},"deepseek-v3.2-think":{"description":"DeepSeek V3.2 Think, version compl\xe8te du mod\xe8le de r\xe9flexion avanc\xe9e, renfor\xe7ant les capacit\xe9s de raisonnement \xe0 long terme."},"deepseek-vl2":{"description":"DeepSeek VL2, mod\xe8le multimodal prenant en charge la compr\xe9hension image-texte et les questions-r\xe9ponses visuelles fines."},"deepseek-vl2-small":{"description":"DeepSeek VL2 Small, version multimodale l\xe9g\xe8re, adapt\xe9e aux environnements \xe0 ressources limit\xe9es et aux sc\xe9narios \xe0 forte concurrence."},"deepseek/deepseek-chat-v3-0324":{"description":"DeepSeek V3 est un mod\xe8le hybride d\'experts avec 685B de param\xe8tres, repr\xe9sentant la derni\xe8re it\xe9ration de la s\xe9rie de mod\xe8les de chat phare de l\'\xe9quipe DeepSeek.\\n\\nIl h\xe9rite du mod\xe8le [DeepSeek V3](/deepseek/deepseek-chat-v3) et excelle dans diverses t\xe2ches."},"deepseek/deepseek-chat-v3-0324:free":{"description":"DeepSeek V3 est un mod\xe8le hybride d\'experts avec 685B de param\xe8tres, repr\xe9sentant la derni\xe8re it\xe9ration de la s\xe9rie de mod\xe8les de chat phare de l\'\xe9quipe DeepSeek.\\n\\nIl h\xe9rite du mod\xe8le [DeepSeek V3](/deepseek/deepseek-chat-v3) et excelle dans diverses t\xe2ches."},"deepseek/deepseek-chat-v3.1":{"description":"DeepSeek-V3.1 est un grand mod\xe8le d\'inf\xe9rence hybride supportant un contexte long de 128K et un changement de mode efficace, offrant des performances et une rapidit\xe9 exceptionnelles dans l\'appel d\'outils, la g\xe9n\xe9ration de code et les t\xe2ches de raisonnement complexes."},"deepseek/deepseek-r1":{"description":"Le mod\xe8le DeepSeek R1 a b\xe9n\xe9fici\xe9 d\'une mise \xe0 jour mineure, version actuelle DeepSeek-R1-0528. Cette mise \xe0 jour am\xe9liore significativement la profondeur et la capacit\xe9 de raisonnement gr\xe2ce \xe0 des ressources de calcul accrues et des optimisations algorithmiques post-entra\xeenement. Il excelle dans plusieurs benchmarks en math\xe9matiques, programmation et logique g\xe9n\xe9rale, approchant les performances des mod\xe8les leaders comme O3 et Gemini 2.5 Pro."},"deepseek/deepseek-r1-0528":{"description":"DeepSeek-R1 am\xe9liore consid\xe9rablement les capacit\xe9s de raisonnement du mod\xe8le avec tr\xe8s peu de donn\xe9es annot\xe9es. Avant de fournir la r\xe9ponse finale, le mod\xe8le g\xe9n\xe8re une cha\xeene de pens\xe9e pour am\xe9liorer la pr\xe9cision de la r\xe9ponse."},"deepseek/deepseek-r1-0528:free":{"description":"DeepSeek-R1 am\xe9liore consid\xe9rablement les capacit\xe9s de raisonnement du mod\xe8le avec tr\xe8s peu de donn\xe9es annot\xe9es. Avant de fournir la r\xe9ponse finale, le mod\xe8le g\xe9n\xe8re une cha\xeene de pens\xe9e pour am\xe9liorer la pr\xe9cision de la r\xe9ponse."},"deepseek/deepseek-r1-distill-llama-70b":{"description":"DeepSeek R1 Distill Llama 70B est un grand mod\xe8le de langage bas\xe9 sur Llama3.3 70B. Gr\xe2ce au fine-tuning r\xe9alis\xe9 \xe0 partir des sorties de DeepSeek R1, il atteint des performances comp\xe9titives comparables \xe0 celles des mod\xe8les de pointe de grande envergure."},"deepseek/deepseek-r1-distill-llama-8b":{"description":"DeepSeek R1 Distill Llama 8B est un mod\xe8le de langage distill\xe9 bas\xe9 sur Llama-3.1-8B-Instruct, entra\xeen\xe9 en utilisant les sorties de DeepSeek R1."},"deepseek/deepseek-r1-distill-qwen-14b":{"description":"DeepSeek R1 Distill Qwen 14B est un mod\xe8le de langage distill\xe9 bas\xe9 sur Qwen 2.5 14B, entra\xeen\xe9 en utilisant les sorties de DeepSeek R1. Ce mod\xe8le a surpass\xe9 l\'o1-mini d\'OpenAI dans plusieurs benchmarks, atteignant des r\xe9sultats de pointe pour les mod\xe8les denses. Voici quelques r\xe9sultats de benchmarks :\\nAIME 2024 pass@1 : 69.7\\nMATH-500 pass@1 : 93.9\\nCodeForces Rating : 1481\\nCe mod\xe8le, affin\xe9 \xe0 partir des sorties de DeepSeek R1, d\xe9montre des performances comp\xe9titives comparables \xe0 celles de mod\xe8les de pointe de plus grande taille."},"deepseek/deepseek-r1-distill-qwen-32b":{"description":"DeepSeek R1 Distill Qwen 32B est un mod\xe8le de langage distill\xe9 bas\xe9 sur Qwen 2.5 32B, entra\xeen\xe9 en utilisant les sorties de DeepSeek R1. Ce mod\xe8le a surpass\xe9 l\'o1-mini d\'OpenAI dans plusieurs benchmarks, atteignant des r\xe9sultats de pointe pour les mod\xe8les denses. Voici quelques r\xe9sultats de benchmarks :\\nAIME 2024 pass@1 : 72.6\\nMATH-500 pass@1 : 94.3\\nCodeForces Rating : 1691\\nCe mod\xe8le, affin\xe9 \xe0 partir des sorties de DeepSeek R1, d\xe9montre des performances comp\xe9titives comparables \xe0 celles de mod\xe8les de pointe de plus grande taille."},"deepseek/deepseek-r1/community":{"description":"DeepSeek R1 est le dernier mod\xe8le open source publi\xe9 par l\'\xe9quipe DeepSeek, offrant des performances d\'inf\xe9rence tr\xe8s puissantes, atteignant des niveaux comparables \xe0 ceux du mod\xe8le o1 d\'OpenAI, en particulier dans les t\xe2ches de math\xe9matiques, de programmation et de raisonnement."},"deepseek/deepseek-r1:free":{"description":"DeepSeek-R1 am\xe9liore consid\xe9rablement les capacit\xe9s de raisonnement du mod\xe8le avec tr\xe8s peu de donn\xe9es annot\xe9es. Avant de fournir la r\xe9ponse finale, le mod\xe8le g\xe9n\xe8re d\'abord une cha\xeene de pens\xe9e pour am\xe9liorer l\'exactitude de la r\xe9ponse finale."},"deepseek/deepseek-v3":{"description":"Un grand mod\xe8le de langage universel rapide avec des capacit\xe9s de raisonnement am\xe9lior\xe9es."},"deepseek/deepseek-v3.1-base":{"description":"DeepSeek V3.1 Base est une version am\xe9lior\xe9e du mod\xe8le DeepSeek V3."},"deepseek/deepseek-v3/community":{"description":"DeepSeek-V3 a r\xe9alis\xe9 une perc\xe9e majeure en termes de vitesse d\'inf\xe9rence par rapport aux mod\xe8les pr\xe9c\xe9dents. Il se classe au premier rang des mod\xe8les open source et peut rivaliser avec les mod\xe8les ferm\xe9s les plus avanc\xe9s au monde. DeepSeek-V3 utilise une architecture d\'attention multi-t\xeate (MLA) et DeepSeekMoE, qui ont \xe9t\xe9 enti\xe8rement valid\xe9es dans DeepSeek-V2. De plus, DeepSeek-V3 a introduit une strat\xe9gie auxiliaire sans perte pour l\'\xe9quilibrage de charge et a \xe9tabli des objectifs d\'entra\xeenement de pr\xe9diction multi-\xe9tiquettes pour obtenir de meilleures performances."},"deepseek_r1":{"description":"DeepSeek-R1 est un mod\xe8le de raisonnement aliment\xe9 par l\'apprentissage par renforcement (RL), r\xe9solvant les probl\xe8mes de r\xe9p\xe9tition et de lisibilit\xe9 dans le mod\xe8le. Avant le RL, DeepSeek-R1 a introduit des donn\xe9es de d\xe9marrage \xe0 froid, optimisant encore les performances d\'inf\xe9rence. Il rivalise avec OpenAI-o1 dans les t\xe2ches de math\xe9matiques, de code et de raisonnement, et am\xe9liore l\'ensemble des performances gr\xe2ce \xe0 des m\xe9thodes d\'entra\xeenement soigneusement con\xe7ues."},"deepseek_r1_distill_llama_70b":{"description":"DeepSeek-R1-Distill-Llama-70B est un mod\xe8le obtenu par distillation de Llama-3.3-70B-Instruct. Ce mod\xe8le fait partie de la s\xe9rie DeepSeek-R1, montrant d\'excellentes performances dans les domaines des math\xe9matiques, de la programmation et du raisonnement, affin\xe9 \xe0 l\'aide d\'\xe9chantillons g\xe9n\xe9r\xe9s par DeepSeek-R1."},"deepseek_r1_distill_qwen_14b":{"description":"DeepSeek-R1-Distill-Qwen-14B est un mod\xe8le obtenu par distillation de connaissances bas\xe9 sur Qwen2.5-14B. Ce mod\xe8le a \xe9t\xe9 affin\xe9 \xe0 l\'aide de 800 000 \xe9chantillons s\xe9lectionn\xe9s g\xe9n\xe9r\xe9s par DeepSeek-R1, montrant d\'excellentes capacit\xe9s de raisonnement."},"deepseek_r1_distill_qwen_32b":{"description":"DeepSeek-R1-Distill-Qwen-32B est un mod\xe8le obtenu par distillation de connaissances bas\xe9 sur Qwen2.5-32B. Ce mod\xe8le a \xe9t\xe9 affin\xe9 \xe0 l\'aide de 800 000 \xe9chantillons s\xe9lectionn\xe9s g\xe9n\xe9r\xe9s par DeepSeek-R1, montrant des performances exceptionnelles dans plusieurs domaines tels que les math\xe9matiques, la programmation et le raisonnement."},"doubao-1.5-lite-32k":{"description":"Doubao-1.5-lite est un mod\xe8le l\xe9ger de nouvelle g\xe9n\xe9ration, offrant une vitesse de r\xe9ponse extr\xeame, avec des performances et des d\xe9lais atteignant des niveaux de classe mondiale."},"doubao-1.5-pro-256k":{"description":"Doubao-1.5-pro-256k est une version am\xe9lior\xe9e de Doubao-1.5-Pro, offrant une am\xe9lioration globale de 10%. Il prend en charge le raisonnement avec une fen\xeatre contextuelle de 256k et une longueur de sortie maximale de 12k tokens. Performances sup\xe9rieures, plus grande fen\xeatre, rapport qualit\xe9-prix exceptionnel, adapt\xe9 \xe0 un large \xe9ventail de sc\xe9narios d\'application."},"doubao-1.5-pro-32k":{"description":"Doubao-1.5-pro est un mod\xe8le phare de nouvelle g\xe9n\xe9ration, avec des performances enti\xe8rement am\xe9lior\xe9es, se distinguant dans les domaines de la connaissance, du code, du raisonnement, etc."},"doubao-1.5-thinking-pro":{"description":"Le mod\xe8le de r\xe9flexion approfondie Doubao-1.5, enti\xe8rement nouveau, se distingue dans des domaines sp\xe9cialis\xe9s tels que les math\xe9matiques, la programmation, le raisonnement scientifique, ainsi que dans des t\xe2ches g\xe9n\xe9rales comme l\'\xe9criture cr\xe9ative. Il atteint ou se rapproche du niveau de premier plan de l\'industrie sur plusieurs r\xe9f\xe9rences de renom telles que AIME 2024, Codeforces, GPQA. Il prend en charge une fen\xeatre de contexte de 128k et une sortie de 16k."},"doubao-1.5-thinking-pro-m":{"description":"Nouveau mod\xe8le de r\xe9flexion profonde Doubao-1.5 (version m avec capacit\xe9s natives d\'inf\xe9rence multimodale profonde), excellent dans les domaines sp\xe9cialis\xe9s tels que math\xe9matiques, programmation, raisonnement scientifique, ainsi que dans les t\xe2ches g\xe9n\xe9rales comme l\'\xe9criture cr\xe9ative. Atteint ou approche le niveau de pointe dans plusieurs benchmarks prestigieux tels que AIME 2024, Codeforces, GPQA. Prend en charge une fen\xeatre contextuelle de 128k et une sortie de 16k."},"doubao-1.5-thinking-vision-pro":{"description":"Nouveau mod\xe8le de r\xe9flexion visuelle profonde, dot\xe9 d\'une compr\xe9hension et d\'un raisonnement multimodaux g\xe9n\xe9raux renforc\xe9s, avec des performances SOTA sur 37 des 59 benchmarks publics."},"doubao-1.5-ui-tars":{"description":"Doubao-1.5-UI-TARS est un mod\xe8le Agent natif con\xe7u pour l\'interaction avec les interfaces graphiques (GUI). Il interagit de mani\xe8re fluide avec les GUI gr\xe2ce \xe0 des capacit\xe9s humaines de perception, raisonnement et action."},"doubao-1.5-vision-lite":{"description":"Doubao-1.5-vision-lite est un mod\xe8le multimodal de nouvelle g\xe9n\xe9ration, prenant en charge la reconnaissance d\'images \xe0 n\'importe quelle r\xe9solution et rapport d\'aspect extr\xeame, am\xe9liorant les capacit\xe9s de raisonnement visuel, de reconnaissance de documents, de compr\xe9hension des informations d\xe9taill\xe9es et de respect des instructions. Il prend en charge une fen\xeatre de contexte de 128k, avec une longueur de sortie maximale de 16k tokens."},"doubao-1.5-vision-pro":{"description":"Doubao-1.5-vision-pro est un mod\xe8le multimodal de nouvelle g\xe9n\xe9ration, prenant en charge la reconnaissance d\'images \xe0 r\xe9solution arbitraire et aux rapports d\'aspect extr\xeames, am\xe9liorant le raisonnement visuel, la reconnaissance documentaire, la compr\xe9hension des d\xe9tails et le respect des instructions."},"doubao-1.5-vision-pro-32k":{"description":"Doubao-1.5-vision-pro est un mod\xe8le multimodal de nouvelle g\xe9n\xe9ration, prenant en charge la reconnaissance d\'images \xe0 r\xe9solution arbitraire et aux rapports d\'aspect extr\xeames, am\xe9liorant le raisonnement visuel, la reconnaissance documentaire, la compr\xe9hension des d\xe9tails et le respect des instructions."},"doubao-lite-128k":{"description":"Offre une vitesse de r\xe9ponse exceptionnelle et un excellent rapport qualit\xe9-prix, offrant aux clients une flexibilit\xe9 accrue pour diff\xe9rents sc\xe9narios. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 128k."},"doubao-lite-32k":{"description":"Offre une vitesse de r\xe9ponse exceptionnelle et un excellent rapport qualit\xe9-prix, offrant aux clients une flexibilit\xe9 accrue pour diff\xe9rents sc\xe9narios. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 32k."},"doubao-lite-4k":{"description":"Offre une vitesse de r\xe9ponse exceptionnelle et un excellent rapport qualit\xe9-prix, offrant aux clients une flexibilit\xe9 accrue pour diff\xe9rents sc\xe9narios. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 4k."},"doubao-pro-256k":{"description":"Mod\xe8le principal le plus performant, adapt\xe9 aux t\xe2ches complexes, avec d\'excellents r\xe9sultats dans les domaines des questions-r\xe9ponses, r\xe9sum\xe9s, cr\xe9ation, classification de texte, jeu de r\xf4le, etc. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 256k."},"doubao-pro-32k":{"description":"Mod\xe8le principal le plus performant, adapt\xe9 aux t\xe2ches complexes, avec d\'excellents r\xe9sultats dans les domaines des questions-r\xe9ponses, r\xe9sum\xe9s, cr\xe9ation, classification de texte, jeu de r\xf4le, etc. Prend en charge l\'inf\xe9rence et le fine-tuning avec une fen\xeatre contextuelle de 32k."},"doubao-seed-1.6":{"description":"Doubao-Seed-1.6 est un tout nouveau mod\xe8le multimodal de r\xe9flexion profonde, supportant trois modes de pens\xe9e : auto, r\xe9flexion et non-r\xe9flexion. En mode non-r\xe9flexion, les performances du mod\xe8le sont largement am\xe9lior\xe9es par rapport \xe0 Doubao-1.5-pro/250115. Il prend en charge une fen\xeatre contextuelle de 256k et une longueur de sortie maximale de 16k tokens."},"doubao-seed-1.6-flash":{"description":"Doubao-Seed-1.6-flash est un mod\xe8le multimodal de r\xe9flexion profonde \xe0 vitesse d\'inf\xe9rence extr\xeame, avec un TPOT de seulement 10 ms ; il supporte \xe0 la fois la compr\xe9hension textuelle et visuelle, avec une capacit\xe9 de compr\xe9hension textuelle sup\xe9rieure \xe0 la g\xe9n\xe9ration lite pr\xe9c\xe9dente, et une compr\xe9hension visuelle comparable aux mod\xe8les pro des concurrents. Il prend en charge une fen\xeatre contextuelle de 256k et une longueur de sortie maximale de 16k tokens."},"doubao-seed-1.6-lite":{"description":"Doubao-Seed-1.6-lite est un tout nouveau mod\xe8le multimodal de raisonnement profond, avec un effort de raisonnement ajustable (Minimal, Faible, Moyen, \xc9lev\xe9). Il offre un excellent rapport qualit\xe9-prix et constitue le meilleur choix pour les t\xe2ches courantes, avec une fen\xeatre de contexte allant jusqu\'\xe0 256k."},"doubao-seed-1.6-thinking":{"description":"Le mod\xe8le Doubao-Seed-1.6-thinking a une capacit\xe9 de r\xe9flexion consid\xe9rablement renforc\xe9e. Par rapport \xe0 Doubao-1.5-thinking-pro, il am\xe9liore davantage les comp\xe9tences fondamentales telles que le codage, les math\xe9matiques et le raisonnement logique, tout en supportant la compr\xe9hension visuelle. Il prend en charge une fen\xeatre contextuelle de 256k et une longueur de sortie maximale de 16k tokens."},"doubao-seed-1.6-vision":{"description":"Doubao-Seed-1.6-vision est un mod\xe8le de r\xe9flexion profonde visuelle, d\xe9montrant une compr\xe9hension multimodale g\xe9n\xe9rale et des capacit\xe9s de raisonnement renforc\xe9es dans des sc\xe9narios tels que l\'\xe9ducation, la mod\xe9ration d\'images, l\'inspection, la s\xe9curit\xe9 et la recherche de questions-r\xe9ponses AI. Il supporte une fen\xeatre contextuelle de 256k et une longueur de sortie maximale de 64k tokens."},"doubao-seededit-3-0-i2i-250628":{"description":"Le mod\xe8le de g\xe9n\xe9ration d\'images Doubao, d\xe9velopp\xe9 par l\'\xe9quipe Seed de ByteDance, prend en charge les entr\xe9es texte et image, offrant une exp\xe9rience de g\xe9n\xe9ration d\'images de haute qualit\xe9 et tr\xe8s contr\xf4lable. Il permet d\'\xe9diter des images via des instructions textuelles, avec des dimensions d\'image entre 512 et 1536 pixels."},"doubao-seedream-3-0-t2i-250415":{"description":"Le mod\xe8le de g\xe9n\xe9ration d\'images Seedream 3.0, d\xe9velopp\xe9 par l\'\xe9quipe Seed de ByteDance, prend en charge les entr\xe9es texte et image, offrant une exp\xe9rience de g\xe9n\xe9ration d\'images de haute qualit\xe9 et tr\xe8s contr\xf4lable. Il g\xe9n\xe8re des images \xe0 partir d\'invites textuelles."},"doubao-seedream-4-0-250828":{"description":"Le mod\xe8le de g\xe9n\xe9ration d\'images Seedream 4.0, d\xe9velopp\xe9 par l\'\xe9quipe Seed de ByteDance, prend en charge les entr\xe9es texte et image, offrant une exp\xe9rience de g\xe9n\xe9ration d\'images de haute qualit\xe9 et tr\xe8s contr\xf4lable. Il g\xe9n\xe8re des images \xe0 partir d\'invites textuelles."},"doubao-vision-lite-32k":{"description":"Le mod\xe8le Doubao-vision est un grand mod\xe8le multimodal d\xe9velopp\xe9 par Doubao, dot\xe9 de puissantes capacit\xe9s de compr\xe9hension et de raisonnement d\'images, ainsi que d\'une compr\xe9hension pr\xe9cise des instructions. Il excelle dans l\'extraction d\'informations texte-image et les t\xe2ches de raisonnement bas\xe9es sur l\'image, pouvant \xeatre appliqu\xe9 \xe0 des t\xe2ches de questions-r\xe9ponses visuelles plus complexes et \xe9tendues."},"doubao-vision-pro-32k":{"description":"Le mod\xe8le Doubao-vision est un grand mod\xe8le multimodal d\xe9velopp\xe9 par Doubao, dot\xe9 de puissantes capacit\xe9s de compr\xe9hension et de raisonnement d\'images, ainsi que d\'une compr\xe9hension pr\xe9cise des instructions. Il excelle dans l\'extraction d\'informations texte-image et les t\xe2ches de raisonnement bas\xe9es sur l\'image, pouvant \xeatre appliqu\xe9 \xe0 des t\xe2ches de questions-r\xe9ponses visuelles plus complexes et \xe9tendues."},"emohaa":{"description":"Emohaa est un mod\xe8le psychologique, dot\xe9 de comp\xe9tences de conseil professionnel, aidant les utilisateurs \xe0 comprendre les probl\xe8mes \xe9motionnels."},"ernie-4.5-0.3b":{"description":"ERNIE 4.5 0.3B, mod\xe8le open source l\xe9ger, id\xe9al pour les d\xe9ploiements locaux et personnalis\xe9s."},"ernie-4.5-21b-a3b":{"description":"ERNIE 4.5 21B A3B, mod\xe8le open source \xe0 grande capacit\xe9, offrant de meilleures performances en compr\xe9hension et g\xe9n\xe9ration."},"ernie-4.5-300b-a47b":{"description":"ERNIE 4.5 300B A47B est un mod\xe8le \xe0 tr\xe8s grande \xe9chelle \xe0 experts mixtes lanc\xe9 par Wenxin de Baidu, dot\xe9 d\'excellentes capacit\xe9s de raisonnement."},"ernie-4.5-8k-preview":{"description":"ERNIE 4.5 8K Preview, mod\xe8le de pr\xe9visualisation avec contexte 8K, pour tester les capacit\xe9s de Wenxin 4.5."},"ernie-4.5-turbo-128k":{"description":"ERNIE 4.5 Turbo 128K, mod\xe8le universel haute performance, prenant en charge la recherche augment\xe9e et l\'appel d\'outils, adapt\xe9 \xe0 divers sc\xe9narios comme les questions-r\xe9ponses, le code et les agents intelligents."},"ernie-4.5-turbo-128k-preview":{"description":"ERNIE 4.5 Turbo 128K Preview, version de pr\xe9visualisation offrant les m\xeames capacit\xe9s que la version officielle, id\xe9ale pour les tests et l\'int\xe9gration."},"ernie-4.5-turbo-32k":{"description":"ERNIE 4.5 Turbo 32K, version \xe0 contexte moyen-long, adapt\xe9e aux questions-r\xe9ponses, \xe0 la recherche dans les bases de connaissances et aux dialogues multi-tours."},"ernie-4.5-turbo-latest":{"description":"ERNIE 4.5 Turbo Latest, derni\xe8re version optimis\xe9e pour la performance globale, id\xe9ale comme mod\xe8le principal en production."},"ernie-4.5-turbo-vl":{"description":"ERNIE 4.5 Turbo VL, mod\xe8le multimodal mature, adapt\xe9 aux t\xe2ches de compr\xe9hension et de reconnaissance image-texte en production."},"ernie-4.5-turbo-vl-32k":{"description":"ERNIE 4.5 Turbo VL 32K, version multimodale \xe0 contexte moyen-long, con\xe7ue pour la compr\xe9hension conjointe de longs documents et d\'images."},"ernie-4.5-turbo-vl-32k-preview":{"description":"ERNIE 4.5 Turbo VL 32K Preview, version de pr\xe9visualisation multimodale 32K, facilitant l\'\xe9valuation des capacit\xe9s visuelles \xe0 long contexte."},"ernie-4.5-turbo-vl-latest":{"description":"ERNIE 4.5 Turbo VL Latest, derni\xe8re version multimodale, offrant une meilleure compr\xe9hension et inf\xe9rence image-texte."},"ernie-4.5-turbo-vl-preview":{"description":"ERNIE 4.5 Turbo VL Preview, mod\xe8le multimodal de pr\xe9visualisation, prenant en charge la compr\xe9hension et la g\xe9n\xe9ration image-texte, id\xe9al pour les questions-r\xe9ponses visuelles et l\'exploration de contenu."},"ernie-4.5-vl-28b-a3b":{"description":"ERNIE 4.5 VL 28B A3B, mod\xe8le multimodal open source, prenant en charge les t\xe2ches de compr\xe9hension et de raisonnement image-texte."},"ernie-5.0-thinking-preview":{"description":"Wenxin 5.0 Thinking Preview, mod\xe8le phare natif tout-modale, prenant en charge le texte, l\'image, l\'audio et la vid\xe9o, avec des capacit\xe9s globales am\xe9lior\xe9es, adapt\xe9 aux questions complexes, \xe0 la cr\xe9ation et aux agents intelligents."},"ernie-char-8k":{"description":"ERNIE Character 8K, mod\xe8le de dialogue avec personnalit\xe9, id\xe9al pour la cr\xe9ation de personnages IP et les conversations d\'accompagnement \xe0 long terme."},"ernie-char-fiction-8k":{"description":"ERNIE Character Fiction 8K, mod\xe8le de personnalit\xe9 destin\xe9 \xe0 la cr\xe9ation de romans et de sc\xe9narios, adapt\xe9 \xe0 la g\xe9n\xe9ration de r\xe9cits longs."},"ernie-char-fiction-8k-preview":{"description":"ERNIE Character Fiction 8K Preview, version de pr\xe9visualisation du mod\xe8le de cr\xe9ation de personnages et de sc\xe9narios, pour tester les fonctionnalit\xe9s."},"ernie-irag-edit":{"description":"ERNIE iRAG Edit, mod\xe8le d\'\xe9dition d\'image prenant en charge l\'effacement, la retouche et la g\xe9n\xe9ration de variantes."},"ernie-lite-8k":{"description":"ERNIE Lite 8K, mod\xe8le universel l\xe9ger, adapt\xe9 aux questions-r\xe9ponses quotidiennes et \xe0 la g\xe9n\xe9ration de contenu \xe0 faible co\xfbt."},"ernie-lite-pro-128k":{"description":"ERNIE Lite Pro 128K, mod\xe8le l\xe9ger haute performance, con\xe7u pour les sc\xe9narios sensibles \xe0 la latence et au co\xfbt."},"ernie-novel-8k":{"description":"ERNIE Novel 8K, mod\xe8le de cr\xe9ation de romans longs et de sc\xe9narios IP, expert en narration multi-personnages et multi-intrigues."},"ernie-speed-128k":{"description":"ERNIE Speed 128K, grand mod\xe8le sans frais d\'entr\xe9e/sortie, adapt\xe9 \xe0 la compr\xe9hension de longs textes et aux essais \xe0 grande \xe9chelle."},"ernie-speed-8k":{"description":"ERNIE Speed 8K, mod\xe8le rapide et gratuit, id\xe9al pour les dialogues quotidiens et les t\xe2ches textuelles l\xe9g\xe8res."},"ernie-speed-pro-128k":{"description":"ERNIE Speed Pro 128K, mod\xe8le haute performance et \xe9conomique, adapt\xe9 aux services en ligne \xe0 grande \xe9chelle et aux applications d\'entreprise."},"ernie-tiny-8k":{"description":"ERNIE Tiny 8K, mod\xe8le ultra-l\xe9ger, adapt\xe9 aux sc\xe9narios d\'inf\xe9rence \xe0 faible co\xfbt comme les questions simples et la classification."},"ernie-x1-turbo-32k":{"description":"ERNIE X1 Turbo 32K, mod\xe8le de r\xe9flexion rapide avec contexte long 32K, adapt\xe9 au raisonnement complexe et aux dialogues multi-tours."},"ernie-x1.1-preview":{"description":"ERNIE X1.1 Preview, version de pr\xe9visualisation du mod\xe8le de r\xe9flexion ERNIE X1.1, pour validation et test des capacit\xe9s."},"fal-ai/bytedance/seedream/v4":{"description":"Le mod\xe8le de g\xe9n\xe9ration d\'images Seedream 4.0, d\xe9velopp\xe9 par l\'\xe9quipe Seed de ByteDance, prend en charge les entr\xe9es texte et image, offrant une exp\xe9rience de g\xe9n\xe9ration d\'images de haute qualit\xe9 et tr\xe8s contr\xf4lable. Il g\xe9n\xe8re des images \xe0 partir d\'invites textuelles."},"fal-ai/flux-kontext/dev":{"description":"Mod\xe8le FLUX.1 d\xe9di\xe9 aux t\xe2ches d\'\xe9dition d\'images, prenant en charge les entr\xe9es texte et image."},"fal-ai/flux-pro/kontext":{"description":"FLUX.1 Kontext [pro] peut traiter du texte et des images de r\xe9f\xe9rence en entr\xe9e, permettant des \xe9ditions locales cibl\xe9es et des transformations complexes de sc\xe8nes globales de mani\xe8re fluide."},"fal-ai/flux/krea":{"description":"Flux Krea [dev] est un mod\xe8le de g\xe9n\xe9ration d\'images avec une pr\xe9f\xe9rence esth\xe9tique, visant \xe0 produire des images plus r\xe9alistes et naturelles."},"fal-ai/flux/schnell":{"description":"FLUX.1 [schnell] est un mod\xe8le de g\xe9n\xe9ration d\'images de 12 milliards de param\xe8tres, sp\xe9cialis\xe9 dans la g\xe9n\xe9ration rapide d\'images de haute qualit\xe9."},"fal-ai/hunyuan-image/v3":{"description":"Un puissant mod\xe8le natif de g\xe9n\xe9ration d\'images multimodales"},"fal-ai/imagen4/preview":{"description":"Mod\xe8le de g\xe9n\xe9ration d\'images de haute qualit\xe9 fourni par Google."},"fal-ai/nano-banana":{"description":"Nano Banana est le mod\xe8le multimodal natif le plus r\xe9cent, rapide et efficace de Google, permettant de g\xe9n\xe9rer et d\'\xe9diter des images par conversation."},"fal-ai/qwen-image":{"description":"Mod\xe8le puissant de g\xe9n\xe9ration d\'images brutes de l\'\xe9quipe Qwen, avec une capacit\xe9 impressionnante de g\xe9n\xe9ration de texte en chinois et une grande diversit\xe9 de styles visuels."},"fal-ai/qwen-image-edit":{"description":"Mod\xe8le professionnel d\'\xe9dition d\'images publi\xe9 par l\'\xe9quipe Qwen, supportant l\'\xe9dition s\xe9mantique et esth\xe9tique, capable d\'\xe9diter pr\xe9cis\xe9ment les textes en chinois et en anglais, et r\xe9alisant des transformations de style, rotations d\'objets et autres \xe9ditions d\'images de haute qualit\xe9."},"flux-1-schnell":{"description":"Mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte de 12 milliards de param\xe8tres d\xe9velopp\xe9 par Black Forest Labs, utilisant la distillation par diffusion antagoniste latente, capable de g\xe9n\xe9rer des images de haute qualit\xe9 en 1 \xe0 4 \xe9tapes. Ses performances rivalisent avec des alternatives propri\xe9taires et il est publi\xe9 sous licence Apache-2.0, adapt\xe9 \xe0 un usage personnel, scientifique et commercial."},"flux-dev":{"description":"FLUX.1 [dev] est un mod\xe8le open source affin\xe9 destin\xe9 \xe0 un usage non commercial. Il maintient une qualit\xe9 d\'image et une adh\xe9rence aux instructions proches de la version professionnelle FLUX, tout en offrant une efficacit\xe9 d\'ex\xe9cution sup\xe9rieure. Par rapport aux mod\xe8les standards de m\xeame taille, il est plus efficace en termes d\'utilisation des ressources."},"flux-kontext-max":{"description":"G\xe9n\xe9ration et \xe9dition d’images contextuelles de pointe — combinant texte et image pour des r\xe9sultats pr\xe9cis et coh\xe9rents."},"flux-kontext-pro":{"description":"G\xe9n\xe9ration et \xe9dition d\'images contextuelles de pointe — alliant texte et image pour des r\xe9sultats pr\xe9cis et coh\xe9rents."},"flux-merged":{"description":"Le mod\xe8le FLUX.1-merged combine les caract\xe9ristiques approfondies explor\xe9es durant la phase de d\xe9veloppement \xab DEV \xbb et les avantages d\'ex\xe9cution rapide repr\xe9sent\xe9s par \xab Schnell \xbb. Cette fusion am\xe9liore non seulement les performances du mod\xe8le mais \xe9tend \xe9galement son champ d\'application."},"flux-pro":{"description":"Mod\xe8le d\'IA commercial de g\xe9n\xe9ration d\'images de premier ordre — qualit\xe9 d\'image in\xe9gal\xe9e et grande diversit\xe9 de rendus."},"flux-pro-1.1":{"description":"Mod\xe8le professionnel am\xe9lior\xe9 de g\xe9n\xe9ration d\'images par IA — offrant une qualit\xe9 d\'image exceptionnelle et une grande fid\xe9lit\xe9 aux instructions de prompt."},"flux-pro-1.1-ultra":{"description":"G\xe9n\xe9ration d\'images IA en ultra haute r\xe9solution — prise en charge d\'une sortie jusqu\'\xe0 4 m\xe9gapixels, cr\xe9ation d\'images ultra-nettes en moins de 10 secondes."},"flux-schnell":{"description":"FLUX.1 [schnell], actuellement le mod\xe8le open source le plus avanc\xe9 \xe0 faible nombre d\'\xe9tapes, d\xe9passe non seulement ses concurrents mais aussi des mod\xe8les puissants non affin\xe9s tels que Midjourney v6.0 et DALL\xb7E 3 (HD). Ce mod\xe8le est sp\xe9cialement affin\xe9 pour conserver toute la diversit\xe9 de sortie de la phase de pr\xe9-entra\xeenement. Par rapport aux mod\xe8les les plus avanc\xe9s du march\xe9, FLUX.1 [schnell] am\xe9liore significativement la qualit\xe9 visuelle, l\'adh\xe9rence aux instructions, la gestion des dimensions/proportions, le traitement des polices et la diversit\xe9 des sorties, offrant une exp\xe9rience de g\xe9n\xe9ration d\'images cr\xe9atives plus riche et vari\xe9e."},"flux.1-schnell":{"description":"FLUX.1-schnell, mod\xe8le de g\xe9n\xe9ration d\'image haute performance, id\xe9al pour cr\xe9er rapidement des images dans divers styles."},"gemini-1.0-pro-001":{"description":"Gemini 1.0 Pro 001 (Ajustement) offre des performances stables et ajustables, ce qui en fait un choix id\xe9al pour des solutions de t\xe2ches complexes."},"gemini-1.0-pro-002":{"description":"Gemini 1.0 Pro 002 (Ajustement) offre un excellent soutien multimodal, se concentrant sur la r\xe9solution efficace de t\xe2ches complexes."},"gemini-1.0-pro-latest":{"description":"Gemini 1.0 Pro est le mod\xe8le d\'IA haute performance de Google, con\xe7u pour une large extension des t\xe2ches."},"gemini-1.5-flash-001":{"description":"Gemini 1.5 Flash 001 est un mod\xe8le multimodal efficace, prenant en charge l\'extension d\'applications vari\xe9es."},"gemini-1.5-flash-002":{"description":"Gemini 1.5 Flash 002 est un mod\xe8le multimodal efficace, prenant en charge une large gamme d\'applications."},"gemini-1.5-flash-8b":{"description":"Gemini 1.5 Flash 8B est un mod\xe8le multimodal efficace, prenant en charge une large gamme d\'applications."},"gemini-1.5-flash-8b-exp-0924":{"description":"Gemini 1.5 Flash 8B 0924 est le dernier mod\xe8le exp\xe9rimental, offrant des am\xe9liorations significatives en termes de performance dans les cas d\'utilisation textuels et multimodaux."},"gemini-1.5-flash-8b-latest":{"description":"Gemini 1.5 Flash 8B est un mod\xe8le multimodal efficace prenant en charge une large gamme d\'applications extensibles."},"gemini-1.5-flash-exp-0827":{"description":"Gemini 1.5 Flash 0827 offre des capacit\xe9s de traitement multimodal optimis\xe9es, adapt\xe9es \xe0 divers sc\xe9narios de t\xe2ches complexes."},"gemini-1.5-flash-latest":{"description":"Gemini 1.5 Flash est le dernier mod\xe8le d\'IA multimodal de Google, dot\xe9 de capacit\xe9s de traitement rapide, prenant en charge les entr\xe9es de texte, d\'images et de vid\xe9os, adapt\xe9 \xe0 une large gamme de t\xe2ches pour une extension efficace."},"gemini-1.5-pro-001":{"description":"Gemini 1.5 Pro 001 est une solution d\'IA multimodale extensible, prenant en charge une large gamme de t\xe2ches complexes."},"gemini-1.5-pro-002":{"description":"Gemini 1.5 Pro 002 est le dernier mod\xe8le pr\xeat pour la production, offrant une qualit\xe9 de sortie sup\xe9rieure, avec des am\xe9liorations notables dans les domaines des math\xe9matiques, des contextes longs et des t\xe2ches visuelles."},"gemini-1.5-pro-exp-0801":{"description":"Gemini 1.5 Pro 0801 offre d\'excellentes capacit\xe9s de traitement multimodal, apportant plus de flexibilit\xe9 au d\xe9veloppement d\'applications."},"gemini-1.5-pro-exp-0827":{"description":"Gemini 1.5 Pro 0827 combine les derni\xe8res technologies d\'optimisation pour offrir des capacit\xe9s de traitement de donn\xe9es multimodales plus efficaces."},"gemini-1.5-pro-latest":{"description":"Gemini 1.5 Pro prend en charge jusqu\'\xe0 2 millions de tokens, ce qui en fait un choix id\xe9al pour un mod\xe8le multimodal de taille moyenne, adapt\xe9 \xe0 un soutien polyvalent pour des t\xe2ches complexes."},"gemini-2.0-flash":{"description":"Gemini 2.0 Flash propose des fonctionnalit\xe9s et des am\xe9liorations de nouvelle g\xe9n\xe9ration, y compris une vitesse exceptionnelle, l\'utilisation d\'outils natifs, la g\xe9n\xe9ration multimodale et une fen\xeatre de contexte de 1M tokens."},"gemini-2.0-flash-001":{"description":"Gemini 2.0 Flash propose des fonctionnalit\xe9s et des am\xe9liorations de nouvelle g\xe9n\xe9ration, y compris une vitesse exceptionnelle, l\'utilisation d\'outils natifs, la g\xe9n\xe9ration multimodale et une fen\xeatre de contexte de 1M tokens."},"gemini-2.0-flash-exp":{"description":"Mod\xe8le variant Gemini 2.0 Flash, optimis\xe9 pour des objectifs tels que le rapport co\xfbt-efficacit\xe9 et la faible latence."},"gemini-2.0-flash-exp-image-generation":{"description":"Mod\xe8le exp\xe9rimental Gemini 2.0 Flash, prenant en charge la g\xe9n\xe9ration d\'images"},"gemini-2.0-flash-lite":{"description":"Une variante du mod\xe8le Gemini 2.0 Flash, optimis\xe9e pour des objectifs tels que le rapport co\xfbt-efficacit\xe9 et la faible latence."},"gemini-2.0-flash-lite-001":{"description":"Une variante du mod\xe8le Gemini 2.0 Flash, optimis\xe9e pour des objectifs tels que le rapport co\xfbt-efficacit\xe9 et la faible latence."},"gemini-2.5-flash":{"description":"Gemini 2.5 Flash est le mod\xe8le le plus rentable de Google, offrant des fonctionnalit\xe9s compl\xe8tes."},"gemini-2.5-flash-image":{"description":"Nano Banana est le dernier mod\xe8le multimodal natif de Google, le plus rapide et le plus efficace, permettant de g\xe9n\xe9rer et d’\xe9diter des images par conversation."},"gemini-2.5-flash-image-preview":{"description":"Nano Banana est le tout dernier mod\xe8le multimodal natif de Google, le plus rapide et le plus efficace, qui vous permet de g\xe9n\xe9rer et d\'\xe9diter des images par conversation."},"gemini-2.5-flash-image-preview:image":{"description":"Nano Banana est le tout dernier mod\xe8le multimodal natif de Google, le plus rapide et le plus efficace, qui vous permet de g\xe9n\xe9rer et d\'\xe9diter des images par conversation."},"gemini-2.5-flash-image:image":{"description":"Nano Banana est le dernier mod\xe8le multimodal natif de Google, le plus rapide et le plus efficace, permettant de g\xe9n\xe9rer et d’\xe9diter des images par conversation."},"gemini-2.5-flash-lite":{"description":"Gemini 2.5 Flash-Lite est le mod\xe8le le plus petit et le plus rentable de Google, con\xe7u pour une utilisation \xe0 grande \xe9chelle."},"gemini-2.5-flash-lite-preview-06-17":{"description":"Gemini 2.5 Flash-Lite Preview est le mod\xe8le le plus compact et rentable de Google, con\xe7u pour une utilisation \xe0 grande \xe9chelle."},"gemini-2.5-flash-lite-preview-09-2025":{"description":"Version de pr\xe9visualisation (25 septembre 2025) de Gemini 2.5 Flash-Lite"},"gemini-2.5-flash-preview-04-17":{"description":"Gemini 2.5 Flash Preview est le mod\xe8le le plus rentable de Google, offrant des fonctionnalit\xe9s compl\xe8tes."},"gemini-2.5-flash-preview-09-2025":{"description":"Version de pr\xe9visualisation (25 septembre 2025) de Gemini 2.5 Flash"},"gemini-2.5-pro":{"description":"Gemini 2.5 Pro est le mod\xe8le de raisonnement le plus avanc\xe9 de Google, capable de traiter des probl\xe8mes complexes en code, math\xe9matiques et domaines STEM, ainsi que d\'analyser de grands ensembles de donn\xe9es, des bases de code et des documents avec un contexte \xe9tendu."},"gemini-2.5-pro-preview-03-25":{"description":"Gemini 2.5 Pro Preview est le mod\xe8le de pens\xe9e le plus avanc\xe9 de Google, capable de raisonner sur des probl\xe8mes complexes en code, math\xe9matiques et domaines STEM, ainsi que d\'analyser de grands ensembles de donn\xe9es, biblioth\xe8ques de code et documents en utilisant un long contexte."},"gemini-2.5-pro-preview-05-06":{"description":"Gemini 2.5 Pro Preview est le mod\xe8le de pens\xe9e le plus avanc\xe9 de Google, capable de raisonner sur des probl\xe8mes complexes dans les domaines du code, des math\xe9matiques et des STEM, ainsi que d\'analyser de grands ensembles de donn\xe9es, des biblioth\xe8ques de code et des documents en utilisant une analyse de long contexte."},"gemini-2.5-pro-preview-06-05":{"description":"Gemini 2.5 Pro Preview est le mod\xe8le de pens\xe9e le plus avanc\xe9 de Google, capable de raisonner sur des probl\xe8mes complexes en code, math\xe9matiques et domaines STEM, ainsi que d\'analyser de grands ensembles de donn\xe9es, biblioth\xe8ques de code et documents avec un contexte \xe9tendu."},"gemini-3-pro-preview":{"description":"Gemini 3 Pro est le mod\xe8le le plus intelligent de Google, dot\xe9 d’un raisonnement de pointe, d’une compr\xe9hension multimodale, ainsi que de puissantes capacit\xe9s d’agent et de codage d’ambiance."},"gemini-flash-latest":{"description":"Derni\xe8re version de Gemini Flash"},"gemini-flash-lite-latest":{"description":"Derni\xe8re version de Gemini Flash-Lite"},"gemini-pro-latest":{"description":"Derni\xe8re version de Gemini Pro"},"gemma-7b-it":{"description":"Gemma 7B est adapt\xe9 au traitement de t\xe2ches de taille moyenne, alliant co\xfbt et efficacit\xe9."},"gemma2":{"description":"Gemma 2 est un mod\xe8le efficace lanc\xe9 par Google, couvrant une vari\xe9t\xe9 de sc\xe9narios d\'application allant des petites applications au traitement de donn\xe9es complexes."},"gemma2-9b-it":{"description":"Gemma 2 9B est un mod\xe8le optimis\xe9 pour des t\xe2ches sp\xe9cifiques et l\'int\xe9gration d\'outils."},"gemma2:27b":{"description":"Gemma 2 est un mod\xe8le efficace lanc\xe9 par Google, couvrant une vari\xe9t\xe9 de sc\xe9narios d\'application allant des petites applications au traitement de donn\xe9es complexes."},"gemma2:2b":{"description":"Gemma 2 est un mod\xe8le efficace lanc\xe9 par Google, couvrant une vari\xe9t\xe9 de sc\xe9narios d\'application allant des petites applications au traitement de donn\xe9es complexes."},"generalv3":{"description":"Spark Pro est un mod\xe8le de langage de haute performance optimis\xe9 pour des domaines professionnels, se concentrant sur les math\xe9matiques, la programmation, la m\xe9decine, l\'\xe9ducation, etc., et supportant la recherche en ligne ainsi que des plugins int\xe9gr\xe9s pour la m\xe9t\xe9o, la date, etc. Son mod\xe8le optimis\xe9 affiche d\'excellentes performances et une efficacit\xe9 dans des t\xe2ches complexes de questions-r\xe9ponses, de compr\xe9hension linguistique et de cr\xe9ation de textes de haut niveau, en faisant un choix id\xe9al pour des applications professionnelles."},"generalv3.5":{"description":"Spark3.5 Max est la version la plus compl\xe8te, supportant la recherche en ligne et de nombreux plugins int\xe9gr\xe9s. Ses capacit\xe9s centrales enti\xe8rement optimis\xe9es, ainsi que la d\xe9finition des r\xf4les syst\xe8me et la fonction d\'appel de fonctions, lui permettent d\'exceller dans divers sc\xe9narios d\'application complexes."},"glm-4":{"description":"GLM-4 est l\'ancienne version phare publi\xe9e en janvier 2024, actuellement remplac\xe9e par le plus puissant GLM-4-0520."},"glm-4-0520":{"description":"GLM-4-0520 est la derni\xe8re version du mod\xe8le, con\xe7ue pour des t\xe2ches hautement complexes et diversifi\xe9es, avec des performances exceptionnelles."},"glm-4-32b-0414":{"description":"GLM-4 32B 0414, version universelle de la s\xe9rie GLM, prenant en charge la g\xe9n\xe9ration et la compr\xe9hension de texte multi-t\xe2ches."},"glm-4-9b-chat":{"description":"GLM-4-9B-Chat offre des performances \xe9lev\xe9es dans les domaines de la s\xe9mantique, des math\xe9matiques, du raisonnement, du code et des connaissances. Il prend \xe9galement en charge la navigation web, l\'ex\xe9cution de code, l\'appel d\'outils personnalis\xe9s et le raisonnement sur de longs textes. Prise en charge de 26 langues, dont le japonais, le cor\xe9en et l’allemand."},"glm-4-air":{"description":"GLM-4-Air est une version \xe9conomique, offrant des performances proches de GLM-4, avec une rapidit\xe9 et un prix abordable."},"glm-4-air-250414":{"description":"GLM-4-Air est une version \xe0 bon rapport qualit\xe9-prix, avec des performances proches de GLM-4, offrant rapidit\xe9 et prix abordable."},"glm-4-airx":{"description":"GLM-4-AirX offre une version efficace de GLM-4-Air, avec une vitesse d\'inf\xe9rence pouvant atteindre 2,6 fois celle de la version standard."},"glm-4-alltools":{"description":"GLM-4-AllTools est un mod\xe8le d\'agent multifonctionnel, optimis\xe9 pour prendre en charge la planification d\'instructions complexes et les appels d\'outils, tels que la navigation sur le web, l\'interpr\xe9tation de code et la g\xe9n\xe9ration de texte, adapt\xe9 \xe0 l\'ex\xe9cution de multiples t\xe2ches."},"glm-4-flash":{"description":"GLM-4-Flash est le choix id\xe9al pour traiter des t\xe2ches simples, avec la vitesse la plus rapide et le prix le plus avantageux."},"glm-4-flash-250414":{"description":"GLM-4-Flash est le choix id\xe9al pour traiter des t\xe2ches simples, offrant la vitesse la plus rapide et \xe9tant gratuit."},"glm-4-flashx":{"description":"GLM-4-FlashX est une version am\xe9lior\xe9e de Flash, offrant une vitesse d\'inf\xe9rence ultra-rapide."},"glm-4-long":{"description":"GLM-4-Long prend en charge des entr\xe9es de texte ultra-longues, adapt\xe9 aux t\xe2ches de m\xe9moire et au traitement de documents \xe0 grande \xe9chelle."},"glm-4-plus":{"description":"GLM-4-Plus, en tant que mod\xe8le phare de haute intelligence, poss\xe8de de puissantes capacit\xe9s de traitement de longs textes et de t\xe2ches complexes, avec des performances globalement am\xe9lior\xe9es."},"glm-4.1v-thinking-flash":{"description":"La s\xe9rie GLM-4.1V-Thinking est actuellement le mod\xe8le visuel le plus performant connu dans la cat\xe9gorie des VLM de 10 milliards de param\xe8tres. Elle int\xe8gre les meilleures performances SOTA dans diverses t\xe2ches de langage visuel, incluant la compr\xe9hension vid\xe9o, les questions-r\xe9ponses sur images, la r\xe9solution de probl\xe8mes disciplinaires, la reconnaissance OCR, l\'interpr\xe9tation de documents et graphiques, les agents GUI, le codage web frontal, le grounding, etc. Ses capacit\xe9s surpassent m\xeame celles du Qwen2.5-VL-72B, qui poss\xe8de plus de huit fois plus de param\xe8tres. Gr\xe2ce \xe0 des techniques avanc\xe9es d\'apprentissage par renforcement, le mod\xe8le ma\xeetrise le raisonnement par cha\xeene de pens\xe9e, am\xe9liorant la pr\xe9cision et la richesse des r\xe9ponses, surpassant nettement les mod\xe8les traditionnels sans m\xe9canisme de pens\xe9e en termes de r\xe9sultats finaux et d\'explicabilit\xe9."},"glm-4.1v-thinking-flashx":{"description":"La s\xe9rie GLM-4.1V-Thinking est actuellement le mod\xe8le visuel le plus performant connu dans la cat\xe9gorie des VLM de 10 milliards de param\xe8tres. Elle int\xe8gre les meilleures performances SOTA dans diverses t\xe2ches de langage visuel, incluant la compr\xe9hension vid\xe9o, les questions-r\xe9ponses sur images, la r\xe9solution de probl\xe8mes disciplinaires, la reconnaissance OCR, l\'interpr\xe9tation de documents et graphiques, les agents GUI, le codage web frontal, le grounding, etc. Ses capacit\xe9s surpassent m\xeame celles du Qwen2.5-VL-72B, qui poss\xe8de plus de huit fois plus de param\xe8tres. Gr\xe2ce \xe0 des techniques avanc\xe9es d\'apprentissage par renforcement, le mod\xe8le ma\xeetrise le raisonnement par cha\xeene de pens\xe9e, am\xe9liorant la pr\xe9cision et la richesse des r\xe9ponses, surpassant nettement les mod\xe8les traditionnels sans m\xe9canisme de pens\xe9e en termes de r\xe9sultats finaux et d\'explicabilit\xe9."},"glm-4.5":{"description":"Mod\xe8le phare de Zhipu, supportant le changement de mode de r\xe9flexion, avec des capacit\xe9s globales atteignant le niveau SOTA des mod\xe8les open source, et une longueur de contexte pouvant atteindre 128K."},"glm-4.5-air":{"description":"Version all\xe9g\xe9e de GLM-4.5, \xe9quilibrant performance et rapport qualit\xe9-prix, avec une commutation flexible entre mod\xe8les de r\xe9flexion hybrides."},"glm-4.5-airx":{"description":"Version ultra-rapide de GLM-4.5-Air, offrant une r\xe9activit\xe9 accrue, con\xe7ue pour des besoins \xe0 grande \xe9chelle et haute vitesse."},"glm-4.5-flash":{"description":"Version gratuite de GLM-4.5, performante dans les t\xe2ches d\'inf\xe9rence, de codage et d\'agents intelligents."},"glm-4.5-x":{"description":"Version ultra-rapide de GLM-4.5, combinant une forte performance avec une vitesse de g\xe9n\xe9ration atteignant 100 tokens par seconde."},"glm-4.5v":{"description":"La nouvelle g\xe9n\xe9ration de mod\xe8le d\'inf\xe9rence visuelle de Zhipu, bas\xe9e sur l\'architecture MOE (Mixture-of-Experts), avec un total de 106 milliards de param\xe8tres et 12 milliards de param\xe8tres d\'activation, atteint l\'\xe9tat de l\'art (SOTA) parmi les mod\xe8les multimodaux open source de m\xeame niveau au niveau mondial sur divers benchmarks, couvrant les t\xe2ches courantes telles que la compr\xe9hension d\'images, de vid\xe9os, de documents et d\'interfaces graphiques (GUI)."},"glm-4.6":{"description":"Le dernier mod\xe8le phare de Zhipu, GLM-4.6 (355B), surpasse enti\xe8rement la g\xe9n\xe9ration pr\xe9c\xe9dente en codage avanc\xe9, traitement de longs textes, inf\xe9rence et capacit\xe9s d\'agent intelligent, notamment en alignant ses comp\xe9tences en programmation avec Claude Sonnet 4, devenant ainsi le mod\xe8le de codage de pointe en Chine."},"glm-4v":{"description":"GLM-4V offre de puissantes capacit\xe9s de compr\xe9hension et de raisonnement d\'image, prenant en charge diverses t\xe2ches visuelles."},"glm-4v-flash":{"description":"GLM-4V-Flash se concentre sur la compr\xe9hension efficace d\'une seule image, adapt\xe9 aux sc\xe9narios d\'analyse d\'image rapide, tels que l\'analyse d\'image en temps r\xe9el ou le traitement d\'images en lot."},"glm-4v-plus":{"description":"GLM-4V-Plus poss\xe8de la capacit\xe9 de comprendre le contenu vid\xe9o et plusieurs images, adapt\xe9 aux t\xe2ches multimodales."},"glm-4v-plus-0111":{"description":"GLM-4V-Plus poss\xe8de des capacit\xe9s de compr\xe9hension de contenu vid\xe9o et de plusieurs images, adapt\xe9 aux t\xe2ches multimodales."},"glm-z1-air":{"description":"Mod\xe8le de raisonnement : dot\xe9 de puissantes capacit\xe9s de raisonnement, adapt\xe9 aux t\xe2ches n\xe9cessitant un raisonnement approfondi."},"glm-z1-airx":{"description":"Raisonnement ultra-rapide : offrant une vitesse de raisonnement extr\xeamement rapide et des r\xe9sultats de raisonnement puissants."},"glm-z1-flash":{"description":"La s\xe9rie GLM-Z1 offre de puissantes capacit\xe9s de raisonnement complexe, avec d\'excellentes performances en logique, math\xe9matiques et programmation."},"glm-z1-flashx":{"description":"Haute vitesse et faible co\xfbt : version am\xe9lior\xe9e Flash, vitesse d\'inf\xe9rence ultra-rapide, meilleure garantie de concurrence."},"glm-zero-preview":{"description":"GLM-Zero-Preview poss\xe8de de puissantes capacit\xe9s de raisonnement complexe, se distinguant dans les domaines du raisonnement logique, des math\xe9matiques et de la programmation."},"google/gemini-2.0-flash":{"description":"Gemini 2.0 Flash offre des fonctionnalit\xe9s de nouvelle g\xe9n\xe9ration et des am\xe9liorations, incluant une vitesse exceptionnelle, l\'utilisation d\'outils int\xe9gr\xe9s, la g\xe9n\xe9ration multimodale et une fen\xeatre de contexte de 1 million de tokens."},"google/gemini-2.0-flash-001":{"description":"Gemini 2.0 Flash propose des fonctionnalit\xe9s et des am\xe9liorations de nouvelle g\xe9n\xe9ration, y compris une vitesse exceptionnelle, l\'utilisation d\'outils natifs, la g\xe9n\xe9ration multimodale et une fen\xeatre de contexte de 1M tokens."},"google/gemini-2.0-flash-exp:free":{"description":"Gemini 2.0 Flash Experimental est le dernier mod\xe8le d\'IA multimodal exp\xe9rimental de Google, offrant une am\xe9lioration de qualit\xe9 par rapport aux versions pr\xe9c\xe9dentes, en particulier pour les connaissances g\xe9n\xe9rales, le code et les longs contextes."},"google/gemini-2.0-flash-lite":{"description":"Gemini 2.0 Flash Lite offre des fonctionnalit\xe9s de nouvelle g\xe9n\xe9ration et des am\xe9liorations, incluant une vitesse exceptionnelle, l\'utilisation d\'outils int\xe9gr\xe9s, la g\xe9n\xe9ration multimodale et une fen\xeatre de contexte de 1 million de tokens."},"google/gemini-2.5-flash":{"description":"Gemini 2.5 Flash est un mod\xe8le de r\xe9flexion offrant d\'excellentes capacit\xe9s globales. Il vise un \xe9quilibre entre prix et performance, supportant le multimodal et une fen\xeatre de contexte de 1 million de tokens."},"google/gemini-2.5-flash-image-preview":{"description":"Mod\xe8le exp\xe9rimental Gemini 2.5 Flash, supportant la g\xe9n\xe9ration d\'images."},"google/gemini-2.5-flash-lite":{"description":"Gemini 2.5 Flash-Lite est un mod\xe8le \xe9quilibr\xe9 \xe0 faible latence, avec un budget de r\xe9flexion configurable et une connectivit\xe9 aux outils (par exemple, recherche Google ancr\xe9e et ex\xe9cution de code). Il supporte les entr\xe9es multimodales et offre une fen\xeatre de contexte de 1 million de tokens."},"google/gemini-2.5-flash-preview":{"description":"Gemini 2.5 Flash est le mod\xe8le phare le plus avanc\xe9 de Google, con\xe7u pour des t\xe2ches de raisonnement avanc\xe9, de codage, de math\xe9matiques et de sciences. Il comprend des capacit\xe9s de \'pens\xe9e\' int\xe9gr\xe9es, lui permettant de fournir des r\xe9ponses avec une plus grande pr\xe9cision et un traitement contextuel d\xe9taill\xe9.\\n\\nRemarque : ce mod\xe8le a deux variantes : pens\xe9e et non-pens\xe9e. La tarification de sortie varie consid\xe9rablement en fonction de l\'activation de la capacit\xe9 de pens\xe9e. Si vous choisissez la variante standard (sans le suffixe \':thinking\'), le mod\xe8le \xe9vitera explicitement de g\xe9n\xe9rer des jetons de pens\xe9e.\\n\\nPour tirer parti de la capacit\xe9 de pens\xe9e et recevoir des jetons de pens\xe9e, vous devez choisir la variante \':thinking\', ce qui entra\xeenera une tarification de sortie de pens\xe9e plus \xe9lev\xe9e.\\n\\nDe plus, Gemini 2.5 Flash peut \xeatre configur\xe9 via le param\xe8tre \'nombre maximal de jetons de raisonnement\', comme d\xe9crit dans la documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."},"google/gemini-2.5-flash-preview:thinking":{"description":"Gemini 2.5 Flash est le mod\xe8le phare le plus avanc\xe9 de Google, con\xe7u pour des t\xe2ches de raisonnement avanc\xe9, de codage, de math\xe9matiques et de sciences. Il comprend des capacit\xe9s de \'pens\xe9e\' int\xe9gr\xe9es, lui permettant de fournir des r\xe9ponses avec une plus grande pr\xe9cision et un traitement contextuel d\xe9taill\xe9.\\n\\nRemarque : ce mod\xe8le a deux variantes : pens\xe9e et non-pens\xe9e. La tarification de sortie varie consid\xe9rablement en fonction de l\'activation de la capacit\xe9 de pens\xe9e. Si vous choisissez la variante standard (sans le suffixe \':thinking\'), le mod\xe8le \xe9vitera explicitement de g\xe9n\xe9rer des jetons de pens\xe9e.\\n\\nPour tirer parti de la capacit\xe9 de pens\xe9e et recevoir des jetons de pens\xe9e, vous devez choisir la variante \':thinking\', ce qui entra\xeenera une tarification de sortie de pens\xe9e plus \xe9lev\xe9e.\\n\\nDe plus, Gemini 2.5 Flash peut \xeatre configur\xe9 via le param\xe8tre \'nombre maximal de jetons de raisonnement\', comme d\xe9crit dans la documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."},"google/gemini-2.5-pro":{"description":"Gemini 2.5 Pro est notre mod\xe8le Gemini de raisonnement le plus avanc\xe9, capable de r\xe9soudre des probl\xe8mes complexes. Il dispose d\'une fen\xeatre de contexte de 2 millions de tokens et supporte des entr\xe9es multimodales incluant texte, images, audio, vid\xe9o et documents PDF."},"google/gemini-2.5-pro-preview":{"description":"Gemini 2.5 Pro Preview est le mod\xe8le de pens\xe9e le plus avanc\xe9 de Google, capable de raisonner sur des probl\xe8mes complexes en code, math\xe9matiques et domaines STEM, ainsi que d\'analyser de grands ensembles de donn\xe9es, des bases de code et des documents en utilisant un contexte \xe9tendu."},"google/gemini-embedding-001":{"description":"Mod\xe8le d\'embedding de pointe, performant en anglais, multilingue et t\xe2ches de code."},"google/gemini-flash-1.5":{"description":"Gemini 1.5 Flash propose des capacit\xe9s de traitement multimodal optimis\xe9es, adapt\xe9es \xe0 divers sc\xe9narios de t\xe2ches complexes."},"google/gemini-pro-1.5":{"description":"Gemini 1.5 Pro combine les derni\xe8res technologies d\'optimisation pour offrir une capacit\xe9 de traitement de donn\xe9es multimodales plus efficace."},"google/gemma-2-27b":{"description":"Gemma 2 est un mod\xe8le efficace lanc\xe9 par Google, couvrant une vari\xe9t\xe9 de sc\xe9narios d\'application allant des petites applications au traitement de donn\xe9es complexes."},"google/gemma-2-27b-it":{"description":"Gemma 2 poursuit le concept de conception l\xe9g\xe8re et efficace."},"google/gemma-2-2b-it":{"description":"Mod\xe8le d\'optimisation des instructions l\xe9ger de Google."},"google/gemma-2-9b":{"description":"Gemma 2 est un mod\xe8le efficace lanc\xe9 par Google, couvrant une vari\xe9t\xe9 de sc\xe9narios d\'application allant des petites applications au traitement de donn\xe9es complexes."},"google/gemma-2-9b-it":{"description":"Gemma 2 est une s\xe9rie de mod\xe8les de texte open source all\xe9g\xe9s de Google."},"google/gemma-2-9b-it:free":{"description":"Gemma 2 est une s\xe9rie de mod\xe8les de texte open source all\xe9g\xe9s de Google."},"google/gemma-2b-it":{"description":"Gemma Instruct (2B) offre des capacit\xe9s de traitement d\'instructions de base, adapt\xe9 aux applications l\xe9g\xe8res."},"google/gemma-3-12b-it":{"description":"Gemma 3 12B est un mod\xe8le de langage open source de Google, \xe9tablissant de nouvelles normes en mati\xe8re d\'efficacit\xe9 et de performance."},"google/gemma-3-27b-it":{"description":"Gemma 3 27B est un mod\xe8le de langage open source de Google, qui a \xe9tabli de nouvelles normes en mati\xe8re d\'efficacit\xe9 et de performance."},"google/text-embedding-005":{"description":"Mod\xe8le d\'embedding textuel focalis\xe9 sur l\'anglais, optimis\xe9 pour les t\xe2ches de code et de langue anglaise."},"google/text-multilingual-embedding-002":{"description":"Mod\xe8le d\'embedding textuel multilingue optimis\xe9 pour les t\xe2ches interlinguistiques, supportant plusieurs langues."},"gpt-3.5-turbo":{"description":"GPT 3.5 Turbo, adapt\xe9 \xe0 diverses t\xe2ches de g\xe9n\xe9ration et de compr\xe9hension de texte, pointe actuellement vers gpt-3.5-turbo-0125."},"gpt-3.5-turbo-0125":{"description":"GPT 3.5 Turbo, adapt\xe9 \xe0 diverses t\xe2ches de g\xe9n\xe9ration et de compr\xe9hension de texte, pointe actuellement vers gpt-3.5-turbo-0125."},"gpt-3.5-turbo-1106":{"description":"GPT 3.5 Turbo, adapt\xe9 \xe0 diverses t\xe2ches de g\xe9n\xe9ration et de compr\xe9hension de texte, pointe actuellement vers gpt-3.5-turbo-0125."},"gpt-3.5-turbo-instruct":{"description":"GPT 3.5 Turbo, adapt\xe9 \xe0 diverses t\xe2ches de g\xe9n\xe9ration et de compr\xe9hension de texte, pointe actuellement vers gpt-3.5-turbo-0125."},"gpt-35-turbo":{"description":"GPT 3.5 Turbo, un mod\xe8le efficace propos\xe9 par OpenAI, adapt\xe9 aux t\xe2ches de chat et de g\xe9n\xe9ration de texte, prenant en charge les appels de fonction en parall\xe8le."},"gpt-35-turbo-16k":{"description":"GPT 3.5 Turbo 16k, un mod\xe8le de g\xe9n\xe9ration de texte \xe0 haute capacit\xe9, adapt\xe9 aux t\xe2ches complexes."},"gpt-4":{"description":"GPT-4 offre une fen\xeatre contextuelle plus grande, capable de traiter des entr\xe9es textuelles plus longues, adapt\xe9 aux sc\xe9narios n\xe9cessitant une int\xe9gration d\'informations \xe9tendue et une analyse de donn\xe9es."},"gpt-4-0125-preview":{"description":"Le dernier mod\xe8le GPT-4 Turbo dispose de fonctionnalit\xe9s visuelles. D\xe9sormais, les requ\xeates visuelles peuvent \xeatre effectu\xe9es en utilisant le mode JSON et les appels de fonction. GPT-4 Turbo est une version am\xe9lior\xe9e, offrant un soutien rentable pour les t\xe2ches multimodales. Il trouve un \xe9quilibre entre pr\xe9cision et efficacit\xe9, adapt\xe9 aux applications n\xe9cessitant des interactions en temps r\xe9el."},"gpt-4-0613":{"description":"GPT-4 offre une fen\xeatre contextuelle plus grande, capable de traiter des entr\xe9es textuelles plus longues, adapt\xe9 aux sc\xe9narios n\xe9cessitant une int\xe9gration d\'informations \xe9tendue et une analyse de donn\xe9es."},"gpt-4-1106-preview":{"description":"Le dernier mod\xe8le GPT-4 Turbo dispose de fonctionnalit\xe9s visuelles. D\xe9sormais, les requ\xeates visuelles peuvent \xeatre effectu\xe9es en utilisant le mode JSON et les appels de fonction. GPT-4 Turbo est une version am\xe9lior\xe9e, offrant un soutien rentable pour les t\xe2ches multimodales. Il trouve un \xe9quilibre entre pr\xe9cision et efficacit\xe9, adapt\xe9 aux applications n\xe9cessitant des interactions en temps r\xe9el."},"gpt-4-32k":{"description":"GPT-4 offre une fen\xeatre contextuelle plus grande, capable de traiter des entr\xe9es textuelles plus longues, adapt\xe9 aux sc\xe9narios n\xe9cessitant une int\xe9gration d\'informations \xe9tendue et une analyse de donn\xe9es."},"gpt-4-32k-0613":{"description":"GPT-4 offre une fen\xeatre contextuelle plus grande, capable de traiter des entr\xe9es textuelles plus longues, adapt\xe9 aux sc\xe9narios n\xe9cessitant une int\xe9gration d\'informations \xe9tendue et une analyse de donn\xe9es."},"gpt-4-turbo":{"description":"Le dernier mod\xe8le GPT-4 Turbo dispose de fonctionnalit\xe9s visuelles. D\xe9sormais, les requ\xeates visuelles peuvent \xeatre effectu\xe9es en utilisant le mode JSON et les appels de fonction. GPT-4 Turbo est une version am\xe9lior\xe9e, offrant un soutien rentable pour les t\xe2ches multimodales. Il trouve un \xe9quilibre entre pr\xe9cision et efficacit\xe9, adapt\xe9 aux applications n\xe9cessitant des interactions en temps r\xe9el."},"gpt-4-turbo-2024-04-09":{"description":"Le dernier mod\xe8le GPT-4 Turbo dispose de fonctionnalit\xe9s visuelles. D\xe9sormais, les requ\xeates visuelles peuvent \xeatre effectu\xe9es en utilisant le mode JSON et les appels de fonction. GPT-4 Turbo est une version am\xe9lior\xe9e, offrant un soutien rentable pour les t\xe2ches multimodales. Il trouve un \xe9quilibre entre pr\xe9cision et efficacit\xe9, adapt\xe9 aux applications n\xe9cessitant des interactions en temps r\xe9el."},"gpt-4-turbo-preview":{"description":"Le dernier mod\xe8le GPT-4 Turbo dispose de fonctionnalit\xe9s visuelles. D\xe9sormais, les requ\xeates visuelles peuvent \xeatre effectu\xe9es en utilisant le mode JSON et les appels de fonction. GPT-4 Turbo est une version am\xe9lior\xe9e, offrant un soutien rentable pour les t\xe2ches multimodales. Il trouve un \xe9quilibre entre pr\xe9cision et efficacit\xe9, adapt\xe9 aux applications n\xe9cessitant des interactions en temps r\xe9el."},"gpt-4-vision-preview":{"description":"Le dernier mod\xe8le GPT-4 Turbo dispose de fonctionnalit\xe9s visuelles. D\xe9sormais, les requ\xeates visuelles peuvent \xeatre effectu\xe9es en utilisant le mode JSON et les appels de fonction. GPT-4 Turbo est une version am\xe9lior\xe9e, offrant un soutien rentable pour les t\xe2ches multimodales. Il trouve un \xe9quilibre entre pr\xe9cision et efficacit\xe9, adapt\xe9 aux applications n\xe9cessitant des interactions en temps r\xe9el."},"gpt-4.1":{"description":"GPT-4.1 est notre mod\xe8le phare pour des t\xe2ches complexes. Il est particuli\xe8rement adapt\xe9 \xe0 la r\xe9solution de probl\xe8mes interdomaines."},"gpt-4.1-mini":{"description":"GPT-4.1 mini offre un \xe9quilibre entre intelligence, rapidit\xe9 et co\xfbt, ce qui en fait un mod\xe8le attrayant pour de nombreux cas d\'utilisation."},"gpt-4.1-nano":{"description":"GPT-4.1 mini offre un \xe9quilibre entre intelligence, rapidit\xe9 et co\xfbt, ce qui en fait un mod\xe8le attrayant pour de nombreux cas d\'utilisation."},"gpt-4.5-preview":{"description":"GPT-4.5-preview est le mod\xe8le g\xe9n\xe9ral le plus r\xe9cent, dot\xe9 d\'une vaste connaissance du monde et d\'une meilleure compr\xe9hension des intentions des utilisateurs ; il excelle dans les t\xe2ches cr\xe9atives et la planification d\'agents. Les connaissances de ce mod\xe8le sont \xe0 jour jusqu\'en octobre 2023."},"gpt-4o":{"description":"ChatGPT-4o est un mod\xe8le dynamique, mis \xe0 jour en temps r\xe9el pour rester \xe0 jour avec la derni\xe8re version. Il combine une compr\xe9hension et une g\xe9n\xe9ration de langage puissantes, adapt\xe9 \xe0 des sc\xe9narios d\'application \xe0 grande \xe9chelle, y compris le service client, l\'\xe9ducation et le support technique."},"gpt-4o-2024-05-13":{"description":"ChatGPT-4o est un mod\xe8le dynamique, mis \xe0 jour en temps r\xe9el pour rester \xe0 jour avec la derni\xe8re version. Il combine une compr\xe9hension et une g\xe9n\xe9ration de langage puissantes, adapt\xe9 \xe0 des sc\xe9narios d\'application \xe0 grande \xe9chelle, y compris le service client, l\'\xe9ducation et le support technique."},"gpt-4o-2024-08-06":{"description":"ChatGPT-4o est un mod\xe8le dynamique, mis \xe0 jour en temps r\xe9el pour rester \xe0 jour avec la derni\xe8re version. Il combine une compr\xe9hension et une g\xe9n\xe9ration de langage puissantes, adapt\xe9 \xe0 des sc\xe9narios d\'application \xe0 grande \xe9chelle, y compris le service client, l\'\xe9ducation et le support technique."},"gpt-4o-2024-11-20":{"description":"ChatGPT-4o est un mod\xe8le dynamique, mis \xe0 jour en temps r\xe9el pour rester \xe0 jour avec la derni\xe8re version. Il combine une compr\xe9hension linguistique puissante et des capacit\xe9s de g\xe9n\xe9ration, adapt\xe9 aux sc\xe9narios d\'application \xe0 grande \xe9chelle, y compris le service client, l\'\xe9ducation et le support technique."},"gpt-4o-audio-preview":{"description":"Mod\xe8le GPT-4o Audio Preview, supportant l\'entr\xe9e et la sortie audio."},"gpt-4o-mini":{"description":"GPT-4o mini est le dernier mod\xe8le lanc\xe9 par OpenAI apr\xe8s le GPT-4 Omni, prenant en charge les entr\xe9es multimodales et produisant des sorties textuelles. En tant que leur mod\xe8le compact le plus avanc\xe9, il est beaucoup moins cher que d\'autres mod\xe8les de pointe r\xe9cents et co\xfbte plus de 60 % de moins que le GPT-3.5 Turbo. Il maintient une intelligence de pointe tout en offrant un rapport qualit\xe9-prix significatif. Le GPT-4o mini a obtenu un score de 82 % au test MMLU et se classe actuellement au-dessus du GPT-4 en termes de pr\xe9f\xe9rences de chat."},"gpt-4o-mini-audio-preview":{"description":"Mod\xe8le GPT-4o mini Audio, supportant l\'entr\xe9e et la sortie audio."},"gpt-4o-mini-realtime-preview":{"description":"Version mini en temps r\xe9el de GPT-4o, prenant en charge les entr\xe9es et sorties audio et textuelles en temps r\xe9el."},"gpt-4o-mini-search-preview":{"description":"La version pr\xe9liminaire GPT-4o mini Search est un mod\xe8le sp\xe9cialement entra\xeen\xe9 pour comprendre et ex\xe9cuter des requ\xeates de recherche web, utilisant l’API Chat Completions. En plus des frais de jetons, les requ\xeates de recherche web sont factur\xe9es par appel d’outil."},"gpt-4o-mini-transcribe":{"description":"GPT-4o Mini Transcribe est un mod\xe8le de transcription audio en texte utilisant GPT-4o. Par rapport au mod\xe8le Whisper original, il am\xe9liore le taux d\'erreur des mots ainsi que la reconnaissance et la pr\xe9cision linguistiques. Utilisez-le pour obtenir des transcriptions plus pr\xe9cises."},"gpt-4o-mini-tts":{"description":"GPT-4o mini TTS est un mod\xe8le de synth\xe8se vocale bas\xe9 sur GPT-4o mini, offrant une g\xe9n\xe9ration de voix de haute qualit\xe9 \xe0 un co\xfbt plus faible."},"gpt-4o-realtime-preview":{"description":"Version en temps r\xe9el de GPT-4o, prenant en charge les entr\xe9es et sorties audio et textuelles en temps r\xe9el."},"gpt-4o-realtime-preview-2024-10-01":{"description":"Version en temps r\xe9el de GPT-4o, prenant en charge les entr\xe9es et sorties audio et textuelles en temps r\xe9el."},"gpt-4o-realtime-preview-2025-06-03":{"description":"Version en temps r\xe9el de GPT-4o, prenant en charge l\'entr\xe9e et la sortie audio et texte en temps r\xe9el."},"gpt-4o-search-preview":{"description":"La version pr\xe9liminaire GPT-4o Search est un mod\xe8le sp\xe9cialement entra\xeen\xe9 pour comprendre et ex\xe9cuter des requ\xeates de recherche web, utilisant l’API Chat Completions. En plus des frais de jetons, les requ\xeates de recherche web sont factur\xe9es par appel d’outil."},"gpt-4o-transcribe":{"description":"GPT-4o Transcribe est un mod\xe8le de transcription audio en texte utilisant GPT-4o. Par rapport au mod\xe8le Whisper original, il am\xe9liore le taux d\'erreur des mots ainsi que la reconnaissance et la pr\xe9cision linguistiques. Utilisez-le pour obtenir des transcriptions plus pr\xe9cises."},"gpt-5":{"description":"Le meilleur mod\xe8le pour les t\xe2ches de codage inter-domaines et d\'agents. GPT-5 r\xe9alise un bond en avant en pr\xe9cision, vitesse, raisonnement, reconnaissance contextuelle, pens\xe9e structur\xe9e et r\xe9solution de probl\xe8mes."},"gpt-5-chat":{"description":"GPT-5 Chat est une version pr\xe9liminaire optimis\xe9e pour les sc\xe9narios de conversation. Il prend en charge les entr\xe9es texte et image, mais ne g\xe9n\xe8re que du texte, id\xe9al pour les chatbots et les applications d’IA conversationnelle."},"gpt-5-chat-latest":{"description":"Mod\xe8le GPT-5 utilis\xe9 dans ChatGPT. Allie une compr\xe9hension et une g\xe9n\xe9ration linguistique puissantes, id\xe9al pour les applications d\'interaction conversationnelle."},"gpt-5-codex":{"description":"GPT-5 Codex est une version optimis\xe9e de GPT-5 pour les t\xe2ches d\'encodage d\'agents dans Codex ou environnements similaires."},"gpt-5-mini":{"description":"Version plus rapide et \xe9conomique de GPT-5, adapt\xe9e aux t\xe2ches bien d\xe9finies. Offre une r\xe9ponse plus rapide tout en maintenant une sortie de haute qualit\xe9."},"gpt-5-nano":{"description":"Version la plus rapide et la plus \xe9conomique de GPT-5. Parfaitement adapt\xe9e aux sc\xe9narios n\xe9cessitant une r\xe9ponse rapide et sensibles aux co\xfbts."},"gpt-5-pro":{"description":"GPT-5 Pro utilise davantage de puissance de calcul pour approfondir sa r\xe9flexion et fournir des r\xe9ponses toujours plus pertinentes."},"gpt-5.1":{"description":"GPT-5.1 — Mod\xe8le phare optimis\xe9 pour les t\xe2ches de codage et d\'agents, avec une intensit\xe9 de raisonnement configurable et un contexte \xe9tendu."},"gpt-5.1-chat-latest":{"description":"GPT-5.1 Chat : Variante de GPT-5.1 con\xe7ue pour ChatGPT, id\xe9ale pour les interactions conversationnelles."},"gpt-5.1-codex":{"description":"GPT-5.1 Codex : Version de GPT-5.1 optimis\xe9e pour les t\xe2ches de codage agentiques, utilisable via l\'API Responses pour des flux de travail code/agent plus complexes."},"gpt-5.1-codex-mini":{"description":"GPT-5.1 Codex mini : Variante plus l\xe9g\xe8re et \xe9conomique de Codex, optimis\xe9e pour les t\xe2ches de codage agentiques."},"gpt-audio":{"description":"GPT Audio est un mod\xe8le de chat universel orient\xe9 vers l\'entr\xe9e et la sortie audio, supportant l\'utilisation d\'entr\xe9es/sorties audio dans l\'API Chat Completions."},"gpt-image-1":{"description":"Mod\xe8le natif multimodal de g\xe9n\xe9ration d\'images de ChatGPT."},"gpt-image-1-mini":{"description":"Une version plus \xe9conomique de GPT Image 1, prenant en charge nativement les entr\xe9es texte et image, et g\xe9n\xe9rant des sorties visuelles."},"gpt-oss-120b":{"description":"Ce mod\xe8le n\xe9cessite une demande d\'acc\xe8s. GPT-OSS-120B est un mod\xe8le de langage open source \xe0 grande \xe9chelle lanc\xe9 par OpenAI, dot\xe9 de puissantes capacit\xe9s de g\xe9n\xe9ration de texte."},"gpt-oss-20b":{"description":"Ce mod\xe8le n\xe9cessite une demande d\'acc\xe8s. GPT-OSS-20B est un mod\xe8le de langage open source de taille moyenne lanc\xe9 par OpenAI, offrant une g\xe9n\xe9ration de texte efficace."},"gpt-oss:120b":{"description":"GPT-OSS 120B est un grand mod\xe8le de langage open source publi\xe9 par OpenAI, utilisant la technologie de quantification MXFP4, con\xe7u comme un mod\xe8le phare. Il n\xe9cessite un environnement multi-GPU ou une station de travail haute performance, offrant des performances exceptionnelles en raisonnement complexe, g\xe9n\xe9ration de code et traitement multilingue, avec prise en charge avanc\xe9e des appels de fonctions et de l\'int\xe9gration d\'outils."},"gpt-oss:20b":{"description":"GPT-OSS 20B est un grand mod\xe8le de langage open source publi\xe9 par OpenAI, utilisant la quantification MXFP4, adapt\xe9 pour fonctionner sur des GPU grand public haut de gamme ou sur Mac Apple Silicon. Ce mod\xe8le excelle dans la g\xe9n\xe9ration de dialogues, la r\xe9daction de code et les t\xe2ches de raisonnement, supportant les appels de fonctions et l\'utilisation d\'outils."},"gpt-realtime":{"description":"Mod\xe8le universel en temps r\xe9el, supportant les entr\xe9es et sorties textuelles et audio en temps r\xe9el, ainsi que les entr\xe9es d\'images."},"grok-2-image-1212":{"description":"Notre dernier mod\xe8le de g\xe9n\xe9ration d\'images peut cr\xe9er des images vivantes et r\xe9alistes \xe0 partir d\'invites textuelles. Il excelle dans la g\xe9n\xe9ration d\'images pour le marketing, les r\xe9seaux sociaux et le divertissement."},"grok-2-vision-1212":{"description":"Ce mod\xe8le a \xe9t\xe9 am\xe9lior\xe9 en termes de pr\xe9cision, de respect des instructions et de capacit\xe9s multilingues."},"grok-3":{"description":"Mod\xe8le phare, expert en extraction de donn\xe9es, programmation et r\xe9sum\xe9 de texte pour des applications d\'entreprise, avec une connaissance approfondie des domaines financier, m\xe9dical, juridique et scientifique."},"grok-3-mini":{"description":"Mod\xe8le l\xe9ger, r\xe9fl\xe9chit avant de r\xe9pondre. Rapide et intelligent, adapt\xe9 aux t\xe2ches logiques ne n\xe9cessitant pas de connaissances approfondies, avec acc\xe8s \xe0 la trace de pens\xe9e originale."},"grok-4":{"description":"Notre tout dernier mod\xe8le phare, le plus puissant, excelle dans le traitement du langage naturel, le calcul math\xe9matique et le raisonnement — un v\xe9ritable champion polyvalent."},"grok-4-0709":{"description":"Grok 4 de xAI, dot\xe9 de puissantes capacit\xe9s de raisonnement."},"grok-4-1-fast-non-reasoning":{"description":"Mod\xe8le multimodal de pointe, sp\xe9cialement optimis\xe9 pour l\'appel d\'outils d\'agents haute performance."},"grok-4-1-fast-reasoning":{"description":"Mod\xe8le multimodal de pointe, sp\xe9cialement optimis\xe9 pour l\'appel d\'outils d\'agents haute performance."},"grok-4-fast-non-reasoning":{"description":"Nous sommes ravis de pr\xe9senter Grok 4 Fast, notre derni\xe8re avanc\xe9e dans les mod\xe8les de raisonnement \xe0 rapport co\xfbt-efficacit\xe9 optimis\xe9."},"grok-4-fast-reasoning":{"description":"Nous sommes ravis de pr\xe9senter Grok 4 Fast, notre derni\xe8re avanc\xe9e dans les mod\xe8les de raisonnement \xe0 rapport co\xfbt-efficacit\xe9 optimis\xe9."},"grok-code-fast-1":{"description":"Nous sommes ravis de pr\xe9senter grok-code-fast-1, un mod\xe8le d\'inf\xe9rence rapide et \xe9conomique, excellent dans le codage des agents."},"groq/compound":{"description":"Compound est un syst\xe8me d\'IA composite, soutenu par plusieurs mod\xe8les ouverts d\xe9j\xe0 disponibles dans GroqCloud, capable d\'utiliser intelligemment et s\xe9lectivement des outils pour r\xe9pondre aux requ\xeates des utilisateurs."},"groq/compound-mini":{"description":"Compound-mini est un syst\xe8me d\'IA composite, soutenu par des mod\xe8les publics d\xe9j\xe0 disponibles dans GroqCloud, capable d\'utiliser intelligemment et s\xe9lectivement des outils pour r\xe9pondre aux requ\xeates des utilisateurs."},"gryphe/mythomax-l2-13b":{"description":"MythoMax l2 13B est un mod\xe8le linguistique combinant cr\xe9ativit\xe9 et intelligence, int\xe9grant plusieurs mod\xe8les de pointe."},"hunyuan-a13b":{"description":"Hunyuan est le premier mod\xe8le de raisonnement hybride, une version am\xe9lior\xe9e de hunyuan-standard-256K, avec un total de 80 milliards de param\xe8tres et 13 milliards activ\xe9s. Par d\xe9faut, il fonctionne en mode de r\xe9flexion lente, mais supporte le basculement entre modes de r\xe9flexion rapide et lente via param\xe8tres ou instructions, en ajoutant / no_think avant la requ\xeate. Ses capacit\xe9s globales sont am\xe9lior\xe9es par rapport \xe0 la g\xe9n\xe9ration pr\xe9c\xe9dente, notamment en math\xe9matiques, sciences, compr\xe9hension de longs textes et capacit\xe9s d\'agent."},"hunyuan-code":{"description":"Dernier mod\xe8le de g\xe9n\xe9ration de code Hunyuan, form\xe9 sur un mod\xe8le de base avec 200B de donn\xe9es de code de haute qualit\xe9, entra\xeen\xe9 pendant six mois avec des donn\xe9es SFT de haute qualit\xe9, avec une longueur de fen\xeatre contextuelle augment\xe9e \xe0 8K, se classant parmi les meilleurs sur les indicateurs d\'\xe9valuation automatique de g\xe9n\xe9ration de code dans cinq langages ; en premi\xe8re ligne des \xe9valuations de qualit\xe9 humaine sur dix aspects de t\xe2ches de code dans cinq langages."},"hunyuan-functioncall":{"description":"Dernier mod\xe8le FunctionCall de l\'architecture MOE Hunyuan, form\xe9 sur des donn\xe9es FunctionCall de haute qualit\xe9, avec une fen\xeatre contextuelle atteignant 32K, se classant parmi les meilleurs sur plusieurs dimensions d\'\xe9valuation."},"hunyuan-large":{"description":"Le mod\xe8le Hunyuan-large a un nombre total de param\xe8tres d\'environ 389B, avec environ 52B de param\xe8tres activ\xe9s, ce qui en fait le mod\xe8le MoE open source de l\'architecture Transformer avec le plus grand nombre de param\xe8tres et les meilleures performances dans l\'industrie."},"hunyuan-large-longcontext":{"description":"Expert dans le traitement des t\xe2ches de longs documents telles que le r\xe9sum\xe9 de documents et les questions-r\xe9ponses sur des documents, tout en ayant \xe9galement la capacit\xe9 de traiter des t\xe2ches de g\xe9n\xe9ration de texte g\xe9n\xe9ral. Il excelle dans l\'analyse et la g\xe9n\xe9ration de longs textes, capable de r\xe9pondre efficacement aux besoins de traitement de contenus longs complexes et d\xe9taill\xe9s."},"hunyuan-large-vision":{"description":"Ce mod\xe8le est adapt\xe9 aux sc\xe9narios de compr\xe9hension image-texte. Bas\xe9 sur le mod\xe8le Hunyuan Large, il s’agit d’un grand mod\xe8le visuel-langage supportant l’entr\xe9e de plusieurs images \xe0 r\xe9solution arbitraire ainsi que du texte, g\xe9n\xe9rant du contenu textuel. Il se concentre sur les t\xe2ches li\xe9es \xe0 la compr\xe9hension image-texte et pr\xe9sente une am\xe9lioration significative des capacit\xe9s multilingues dans ce domaine."},"hunyuan-lite":{"description":"Mise \xe0 niveau vers une structure MOE, avec une fen\xeatre contextuelle de 256k, en t\xeate de nombreux mod\xe8les open source dans les \xe9valuations NLP, code, math\xe9matiques, industrie, etc."},"hunyuan-lite-vision":{"description":"Le dernier mod\xe8le multimodal 7B de Hunyuan, avec une fen\xeatre contextuelle de 32K, prend en charge les dialogues multimodaux en chinois et en anglais, la reconnaissance d\'objets d\'images, la compr\xe9hension de documents et de tableaux, ainsi que les math\xe9matiques multimodales, surpassant les mod\xe8les concurrents de 7B sur plusieurs dimensions d\'\xe9valuation."},"hunyuan-pro":{"description":"Mod\xe8le de long texte MOE-32K avec un milliard de param\xe8tres. Atteint un niveau de performance absolument sup\xe9rieur sur divers benchmarks, capable de traiter des instructions complexes et de raisonner, avec des capacit\xe9s math\xe9matiques avanc\xe9es, prenant en charge les appels de fonction, optimis\xe9 pour des domaines tels que la traduction multilingue, le droit financier et m\xe9dical."},"hunyuan-role":{"description":"Dernier mod\xe8le de jeu de r\xf4le Hunyuan, un mod\xe8le de jeu de r\xf4le affin\xe9 et form\xe9 par l\'\xe9quipe officielle de Hunyuan, bas\xe9 sur le mod\xe8le Hunyuan et des ensembles de donn\xe9es de sc\xe9narios de jeu de r\xf4le, offrant de meilleures performances de base dans les sc\xe9narios de jeu de r\xf4le."},"hunyuan-standard":{"description":"Utilise une strat\xe9gie de routage am\xe9lior\xe9e tout en att\xe9nuant les probl\xe8mes d\'\xe9quilibrage de charge et de convergence des experts. Pour les longs textes, l\'indice de recherche atteint 99,9 %. MOE-32K offre un meilleur rapport qualit\xe9-prix, \xe9quilibrant efficacit\xe9 et co\xfbt tout en permettant le traitement des entr\xe9es de longs textes."},"hunyuan-standard-256K":{"description":"Utilise une strat\xe9gie de routage am\xe9lior\xe9e tout en att\xe9nuant les probl\xe8mes d\'\xe9quilibrage de charge et de convergence des experts. Pour les longs textes, l\'indice de recherche atteint 99,9 %. MOE-256K franchit de nouvelles \xe9tapes en termes de longueur et d\'efficacit\xe9, \xe9largissant consid\xe9rablement la longueur d\'entr\xe9e possible."},"hunyuan-standard-vision":{"description":"Le dernier mod\xe8le multimodal de Hunyuan, prenant en charge les r\xe9ponses multilingues, avec des capacit\xe9s \xe9quilibr\xe9es en chinois et en anglais."},"hunyuan-t1-20250321":{"description":"Mod\xe8le complet construit pour les capacit\xe9s en sciences humaines et exactes, avec une forte capacit\xe9 de capture d\'informations dans de longs textes. Prend en charge le raisonnement pour r\xe9pondre \xe0 divers probl\xe8mes scientifiques de math\xe9matiques/logique/sciences/code, quel que soit leur niveau de difficult\xe9."},"hunyuan-t1-20250403":{"description":"Am\xe9lioration des capacit\xe9s de g\xe9n\xe9ration de code au niveau projet ; am\xe9lioration de la qualit\xe9 de la r\xe9daction g\xe9n\xe9r\xe9e ; am\xe9lioration de la compr\xe9hension multi-tours des sujets, de la conformit\xe9 aux instructions toB et de la compr\xe9hension des mots ; optimisation des probl\xe8mes li\xe9s \xe0 la sortie mixte de caract\xe8res simplifi\xe9s/traditionnels et chinois/anglais."},"hunyuan-t1-20250529":{"description":"Optimis\xe9 pour la cr\xe9ation de textes, la r\xe9daction d\'essais, ainsi que pour les comp\xe9tences en codage frontend, math\xe9matiques et raisonnement logique, avec une am\xe9lioration de la capacit\xe9 \xe0 suivre les instructions."},"hunyuan-t1-20250711":{"description":"Am\xe9lioration significative des capacit\xe9s en math\xe9matiques complexes, logique et codage, optimisation de la stabilit\xe9 des sorties du mod\xe8le et am\xe9lioration des capacit\xe9s de traitement de longs textes."},"hunyuan-t1-latest":{"description":"Am\xe9lioration significative des capacit\xe9s du mod\xe8le principal de r\xe9flexion lente dans les domaines des math\xe9matiques avanc\xe9es, du raisonnement complexe, du code difficile, du respect des instructions et de la qualit\xe9 de la cr\xe9ation textuelle."},"hunyuan-t1-vision-20250619":{"description":"La derni\xe8re version du mod\xe8le de r\xe9flexion profonde multimodale t1-vision de Hunyuan, supportant une cha\xeene de pens\xe9e native multimodale, avec des am\xe9liorations globales par rapport \xe0 la version par d\xe9faut pr\xe9c\xe9dente."},"hunyuan-t1-vision-20250916":{"description":"La derni\xe8re version du mod\xe8le de raisonnement visuel profond Hunyuan t1-vision offre des am\xe9liorations significatives par rapport \xe0 la version pr\xe9c\xe9dente dans des t\xe2ches telles que les questions-r\xe9ponses image-texte, la localisation visuelle, l\'OCR, l\'interpr\xe9tation de graphiques, la r\xe9solution de probl\xe8mes \xe0 partir de photos et la cr\xe9ation d’images. Les performances en anglais et en langues rares ont \xe9galement \xe9t\xe9 nettement optimis\xe9es."},"hunyuan-turbo":{"description":"Version pr\xe9liminaire du nouveau mod\xe8le de langage de g\xe9n\xe9ration Hunyuan, utilisant une nouvelle structure de mod\xe8le d\'experts mixtes (MoE), offrant une efficacit\xe9 d\'inf\xe9rence plus rapide et de meilleures performances par rapport \xe0 Hunyuan-Pro."},"hunyuan-turbo-20241223":{"description":"Optimisations de cette version : mise \xe0 l\'\xe9chelle des instructions de donn\xe9es, augmentation significative de la capacit\xe9 de g\xe9n\xe9ralisation du mod\xe8le ; am\xe9lioration significative des capacit\xe9s en math\xe9matiques, en code et en raisonnement logique ; optimisation des capacit\xe9s de compr\xe9hension des mots dans le texte ; optimisation de la qualit\xe9 de g\xe9n\xe9ration de contenu dans la cr\xe9ation de texte."},"hunyuan-turbo-latest":{"description":"Optimisation de l\'exp\xe9rience g\xe9n\xe9rale, y compris la compr\xe9hension NLP, la cr\xe9ation de texte, les conversations informelles, les questions-r\xe9ponses, la traduction, et les domaines sp\xe9cifiques ; am\xe9lioration de l\'humanit\xe9 simul\xe9e, optimisation de l\'intelligence \xe9motionnelle du mod\xe8le ; am\xe9lioration de la capacit\xe9 du mod\xe8le \xe0 clarifier activement en cas d\'ambigu\xeft\xe9 d\'intention ; am\xe9lioration de la capacit\xe9 \xe0 traiter les questions de d\xe9composition de mots ; am\xe9lioration de la qualit\xe9 et de l\'interactivit\xe9 de la cr\xe9ation ; am\xe9lioration de l\'exp\xe9rience multi-tours."},"hunyuan-turbo-vision":{"description":"Le nouveau mod\xe8le phare de langage visuel de Hunyuan de nouvelle g\xe9n\xe9ration, utilisant une toute nouvelle structure de mod\xe8le d\'experts hybrides (MoE), avec des am\xe9liorations compl\xe8tes par rapport \xe0 la g\xe9n\xe9ration pr\xe9c\xe9dente dans les capacit\xe9s de reconnaissance de base, de cr\xe9ation de contenu, de questions-r\xe9ponses, et d\'analyse et de raisonnement li\xe9s \xe0 la compr\xe9hension d\'images et de textes."},"hunyuan-turbos-20250313":{"description":"Uniformisation du style des \xe9tapes de r\xe9solution math\xe9matique, renforcement des questions-r\xe9ponses math\xe9matiques multi-tours. Optimisation du style de r\xe9ponse en cr\xe9ation textuelle, suppression de l’aspect \xab IA \xbb, ajout d’\xe9l\xe9gance litt\xe9raire."},"hunyuan-turbos-20250416":{"description":"Mise \xe0 niveau de la base pr\xe9-entra\xeen\xe9e, renfor\xe7ant la compr\xe9hension et la conformit\xe9 aux instructions ; am\xe9lioration des comp\xe9tences en math\xe9matiques, code, logique et sciences durant la phase d’alignement ; am\xe9lioration de la qualit\xe9 de la cr\xe9ation litt\xe9raire, de la compr\xe9hension textuelle, de la pr\xe9cision des traductions et des r\xe9ponses aux questions de culture g\xe9n\xe9rale ; renforcement des capacit\xe9s des agents dans divers domaines, avec un accent particulier sur la compr\xe9hension des dialogues multi-tours."},"hunyuan-turbos-20250604":{"description":"Mise \xe0 niveau de la base pr\xe9-entra\xeen\xe9e, am\xe9lioration des capacit\xe9s d\'\xe9criture et de compr\xe9hension de lecture, augmentation significative des comp\xe9tences en codage et en sciences, avec un suivi continu des instructions complexes."},"hunyuan-turbos-20250926":{"description":"Am\xe9lioration de la qualit\xe9 des donn\xe9es de base pour la pr\xe9-formation. Optimisation de la strat\xe9gie d’entra\xeenement post-formation, avec une am\xe9lioration continue des capacit\xe9s des agents, des langues anglaises et petites langues, de la conformit\xe9 aux instructions, du code et des sciences."},"hunyuan-turbos-latest":{"description":"hunyuan-TurboS est la derni\xe8re version du mod\xe8le phare Hunyuan, offrant une capacit\xe9 de r\xe9flexion am\xe9lior\xe9e et une exp\xe9rience utilisateur optimis\xe9e."},"hunyuan-turbos-longtext-128k-20250325":{"description":"Expert dans le traitement de t\xe2ches de long texte telles que le r\xe9sum\xe9 de documents et les questions-r\xe9ponses, tout en ayant la capacit\xe9 de g\xe9rer des t\xe2ches de g\xe9n\xe9ration de texte g\xe9n\xe9ral. Il excelle dans l\'analyse et la g\xe9n\xe9ration de longs textes, r\xe9pondant efficacement aux besoins de traitement de contenus longs et complexes."},"hunyuan-turbos-role-plus":{"description":"Derni\xe8re version du mod\xe8le de jeu de r\xf4le Hunyuan, finement ajust\xe9 par l’\xe9quipe officielle Hunyuan. Ce mod\xe8le est entra\xeen\xe9 en suppl\xe9ment avec un jeu de donn\xe9es sp\xe9cifique aux sc\xe9narios de jeu de r\xf4le, offrant de meilleures performances de base dans ces contextes."},"hunyuan-turbos-vision":{"description":"Ce mod\xe8le est adapt\xe9 aux sc\xe9narios de compr\xe9hension image-texte. Bas\xe9 sur la derni\xe8re g\xe9n\xe9ration turbos de Hunyuan, c\'est un grand mod\xe8le phare de langage visuel, focalis\xe9 sur les t\xe2ches li\xe9es \xe0 la compr\xe9hension image-texte, incluant la reconnaissance d\'entit\xe9s bas\xe9e sur l\'image, les questions-r\xe9ponses de connaissances, la cr\xe9ation de contenu, la r\xe9solution de probl\xe8mes par photo, etc., avec des am\xe9liorations globales par rapport \xe0 la g\xe9n\xe9ration pr\xe9c\xe9dente."},"hunyuan-turbos-vision-20250619":{"description":"La derni\xe8re version du grand mod\xe8le phare de langage visuel turbos-vision de Hunyuan, avec des am\xe9liorations globales par rapport \xe0 la version par d\xe9faut pr\xe9c\xe9dente dans les t\xe2ches li\xe9es \xe0 la compr\xe9hension image-texte, incluant la reconnaissance d\'entit\xe9s bas\xe9e sur l\'image, les questions-r\xe9ponses de connaissances, la cr\xe9ation de contenu, la r\xe9solution de probl\xe8mes par photo, etc."},"hunyuan-vision":{"description":"Dernier mod\xe8le multimodal Hunyuan, prenant en charge l\'entr\xe9e d\'images et de textes pour g\xe9n\xe9rer du contenu textuel."},"image-01":{"description":"Nouveau mod\xe8le de g\xe9n\xe9ration d\'images avec des rendus d\xe9taill\xe9s, supportant la g\xe9n\xe9ration d\'images \xe0 partir de texte et d\'images."},"image-01-live":{"description":"Mod\xe8le de g\xe9n\xe9ration d\'images avec rendu d\xe9taill\xe9, supportant la g\xe9n\xe9ration d\'images \xe0 partir de texte avec r\xe9glage du style artistique."},"imagen-4.0-fast-generate-001":{"description":"Imagen, s\xe9rie de mod\xe8les de 4e g\xe9n\xe9ration pour la cr\xe9ation d\'images \xe0 partir de texte — version Fast"},"imagen-4.0-generate-001":{"description":"S\xe9rie de mod\xe8les Imagen de 4ᵉ g\xe9n\xe9ration pour la g\xe9n\xe9ration d\'images \xe0 partir de texte"},"imagen-4.0-generate-preview-06-06":{"description":"S\xe9rie de mod\xe8les de g\xe9n\xe9ration d\'images de quatri\xe8me g\xe9n\xe9ration Imagen"},"imagen-4.0-ultra-generate-001":{"description":"S\xe9rie de mod\xe8les Imagen 4e g\xe9n\xe9ration pour la g\xe9n\xe9ration d\'images \xe0 partir de texte — version Ultra"},"imagen-4.0-ultra-generate-preview-06-06":{"description":"Version Ultra de la s\xe9rie de mod\xe8les de g\xe9n\xe9ration d\'images de quatri\xe8me g\xe9n\xe9ration Imagen"},"inception/mercury-coder-small":{"description":"Mercury Coder Small est id\xe9al pour les t\xe2ches de g\xe9n\xe9ration, d\xe9bogage et refactorisation de code, avec une latence minimale."},"inclusionAI/Ling-1T":{"description":"Ling-1T est le premier mod\xe8le phare de la s\xe9rie \\"Ling 2.0\\", un mod\xe8le non-r\xe9fl\xe9chissant dot\xe9 de 1 000 milliards de param\xe8tres totaux et environ 50 milliards de param\xe8tres actifs par token. Construit sur l\'architecture Ling 2.0, Ling-1T vise \xe0 repousser les limites du raisonnement efficace et de la cognition \xe9volutive. Ling-1T-base a \xe9t\xe9 entra\xeen\xe9 sur plus de 20 000 milliards de tokens de haute qualit\xe9 et riches en raisonnement."},"inclusionAI/Ling-flash-2.0":{"description":"Ling-flash-2.0 est le troisi\xe8me mod\xe8le de la s\xe9rie d\'architectures Ling 2.0 publi\xe9 par l\'\xe9quipe Bailing du groupe Ant. C\'est un mod\xe8le d\'experts mixtes (MoE) avec un total de 100 milliards de param\xe8tres, mais n\'activant que 6,1 milliards de param\xe8tres par token (dont 4,8 milliards hors embeddings). En tant que mod\xe8le l\xe9ger, Ling-flash-2.0 affiche des performances comparables voire sup\xe9rieures \xe0 celles des mod\xe8les denses de 40 milliards de param\xe8tres et des mod\xe8les MoE de plus grande taille dans plusieurs \xe9valuations de r\xe9f\xe9rence reconnues. Ce mod\xe8le vise \xe0 explorer des voies d\'efficacit\xe9 sous le consensus \xab grand mod\xe8le = grand nombre de param\xe8tres \xbb gr\xe2ce \xe0 une conception d\'architecture et des strat\xe9gies d\'entra\xeenement optimales."},"inclusionAI/Ling-mini-2.0":{"description":"Ling-mini-2.0 est un mod\xe8le de langage de grande taille compact et performant bas\xe9 sur l\'architecture MoE. Il poss\xe8de 16 milliards de param\xe8tres au total, mais n\'active que 1,4 milliard par token (789 millions hors embeddings), permettant une vitesse de g\xe9n\xe9ration tr\xe8s \xe9lev\xe9e. Gr\xe2ce \xe0 une conception MoE efficace et \xe0 un entra\xeenement massif sur des donn\xe9es de haute qualit\xe9, Ling-mini-2.0 offre des performances de pointe sur les t\xe2ches en aval, comparables \xe0 celles des mod\xe8les denses de moins de 10 milliards de param\xe8tres et des mod\xe8les MoE plus grands."},"inclusionAI/Ring-1T":{"description":"Ring-1T est un mod\xe8le open source \xe0 l’\xe9chelle du billion de param\xe8tres, d\xe9velopp\xe9 par l’\xe9quipe Bailing. Il est bas\xe9 sur l’architecture Ling 2.0 et le mod\xe8le de base Ling-1T, avec un total de 1 000 milliards de param\xe8tres et 50 milliards de param\xe8tres actifs. Il prend en charge une fen\xeatre de contexte allant jusqu’\xe0 128K et a \xe9t\xe9 optimis\xe9 via un apprentissage par renforcement avec r\xe9compenses v\xe9rifiables \xe0 grande \xe9chelle."},"inclusionAI/Ring-flash-2.0":{"description":"Ring-flash-2.0 est un mod\xe8le de r\xe9flexion haute performance profond\xe9ment optimis\xe9 \xe0 partir de Ling-flash-2.0-base. Il utilise une architecture d\'experts mixtes (MoE) avec un total de 100 milliards de param\xe8tres, mais n\'active que 6,1 milliards de param\xe8tres \xe0 chaque inf\xe9rence. Ce mod\xe8le r\xe9sout, gr\xe2ce \xe0 l\'algorithme innovant icepop, l\'instabilit\xe9 des grands mod\xe8les MoE lors de l\'entra\xeenement par apprentissage par renforcement (RL), permettant une am\xe9lioration continue des capacit\xe9s de raisonnement complexe sur de longues p\xe9riodes d\'entra\xeenement. Ring-flash-2.0 a r\xe9alis\xe9 des avanc\xe9es significatives dans plusieurs benchmarks difficiles tels que les comp\xe9titions math\xe9matiques, la g\xe9n\xe9ration de code et le raisonnement logique. Ses performances surpassent non seulement les meilleurs mod\xe8les denses de moins de 40 milliards de param\xe8tres, mais rivalisent aussi avec des mod\xe8les MoE open source plus grands et des mod\xe8les de r\xe9flexion propri\xe9taires haute performance. Bien que focalis\xe9 sur le raisonnement complexe, il excelle \xe9galement dans les t\xe2ches de cr\xe9ation litt\xe9raire. De plus, gr\xe2ce \xe0 sa conception efficace, Ring-flash-2.0 offre des performances puissantes tout en assurant une inf\xe9rence rapide, r\xe9duisant consid\xe9rablement les co\xfbts de d\xe9ploiement dans des sc\xe9narios \xe0 forte concurrence."},"internlm/internlm2_5-7b-chat":{"description":"InternLM2.5 fournit des solutions de dialogue intelligent dans divers sc\xe9narios."},"internlm2.5-latest":{"description":"Notre derni\xe8re s\xe9rie de mod\xe8les, offrant des performances d\'inf\xe9rence exceptionnelles, prenant en charge une longueur de contexte de 1M et des capacit\xe9s am\xe9lior\xe9es de suivi des instructions et d\'appel d\'outils."},"internlm3-latest":{"description":"Notre derni\xe8re s\xe9rie de mod\xe8les, avec des performances d\'inf\xe9rence exceptionnelles, en t\xeate des mod\xe8les open source de m\xeame niveau. Par d\xe9faut, elle pointe vers notre derni\xe8re version du mod\xe8le InternLM3."},"internvl2.5-38b-mpo":{"description":"InternVL2.5 38B MPO, mod\xe8le pr\xe9-entra\xeen\xe9 multimodal, con\xe7u pour les t\xe2ches complexes de raisonnement image-texte."},"internvl2.5-latest":{"description":"Version InternVL2.5 que nous maintenons encore, offrant des performances excellentes et stables. Il pointe par d\xe9faut vers notre dernier mod\xe8le de la s\xe9rie InternVL2.5, actuellement vers internvl2.5-78b."},"internvl3-14b":{"description":"InternVL3 14B, mod\xe8le multimodal de taille moyenne, \xe9quilibrant performance et co\xfbt."},"internvl3-1b":{"description":"InternVL3 1B, mod\xe8le multimodal l\xe9ger, adapt\xe9 aux d\xe9ploiements dans des environnements \xe0 ressources limit\xe9es."},"internvl3-38b":{"description":"InternVL3 38B, grand mod\xe8le multimodal open source, con\xe7u pour des t\xe2ches de compr\xe9hension image-texte de haute pr\xe9cision."},"internvl3-latest":{"description":"Nous avons r\xe9cemment publi\xe9 un grand mod\xe8le multimodal, dot\xe9 de capacit\xe9s de compr\xe9hension d\'images et de textes plus puissantes, ainsi que d\'une compr\xe9hension d\'images sur de longues s\xe9quences, dont les performances rivalisent avec celles des meilleurs mod\xe8les ferm\xe9s. Il pointe par d\xe9faut vers notre dernier mod\xe8le de la s\xe9rie InternVL, actuellement vers internvl3-78b."},"irag-1.0":{"description":"ERNIE iRAG, mod\xe8le de g\xe9n\xe9ration am\xe9lior\xe9e par recherche d\'image, prenant en charge la recherche d\'image par image, la recherche image-texte et la g\xe9n\xe9ration de contenu."},"jamba-large":{"description":"Notre mod\xe8le le plus puissant et avanc\xe9, con\xe7u pour traiter des t\xe2ches complexes de niveau entreprise, offrant des performances exceptionnelles."},"jamba-mini":{"description":"Le mod\xe8le le plus efficace de sa cat\xe9gorie, alliant vitesse et qualit\xe9, avec un volume r\xe9duit."},"jina-deepsearch-v1":{"description":"La recherche approfondie combine la recherche sur le web, la lecture et le raisonnement pour mener des enqu\xeates compl\xe8tes. Vous pouvez la consid\xe9rer comme un agent qui prend en charge vos t\xe2ches de recherche - elle effectuera une recherche approfondie et it\xe9rative avant de fournir une r\xe9ponse. Ce processus implique une recherche continue, un raisonnement et une r\xe9solution de probl\xe8mes sous diff\xe9rents angles. Cela diff\xe8re fondamentalement des grands mod\xe8les standard qui g\xe9n\xe8rent des r\xe9ponses directement \xe0 partir de donn\xe9es pr\xe9-entra\xeen\xe9es et des syst\xe8mes RAG traditionnels qui d\xe9pendent d\'une recherche superficielle unique."},"kimi-k2":{"description":"Kimi-K2 est un mod\xe8le de base \xe0 architecture MoE lanc\xe9 par Moonshot AI, dot\xe9 de capacit\xe9s exceptionnelles en codage et agents, avec 1 000 milliards de param\xe8tres au total et 32 milliards activ\xe9s. Il surpasse les autres mod\xe8les open source majeurs dans les tests de performance sur les connaissances g\xe9n\xe9rales, la programmation, les math\xe9matiques et les agents."},"kimi-k2-0711-preview":{"description":"kimi-k2 est un mod\xe8le de base \xe0 architecture MoE dot\xe9 de capacit\xe9s exceptionnelles en code et Agent, avec un total de 1T de param\xe8tres et 32B de param\xe8tres activ\xe9s. Dans les tests de performance sur les principales cat\xe9gories telles que le raisonnement g\xe9n\xe9ral, la programmation, les math\xe9matiques et les Agents, le mod\xe8le K2 surpasse les autres mod\xe8les open source majeurs."},"kimi-k2-0905-preview":{"description":"Le mod\xe8le kimi-k2-0905-preview dispose d\'une longueur de contexte de 256k, offrant une capacit\xe9 renforc\xe9e de codage agentique, une meilleure esth\xe9tique et utilit\xe9 du code front-end, ainsi qu\'une compr\xe9hension contextuelle am\xe9lior\xe9e."},"kimi-k2-instruct":{"description":"Kimi K2 Instruct, mod\xe8le d\'inf\xe9rence officiel de Kimi, prenant en charge le long contexte, le code, les questions-r\xe9ponses et d\'autres sc\xe9narios."},"kimi-k2-turbo-preview":{"description":"kimi-k2 est un mod\xe8le de base \xe0 architecture MoE dot\xe9 de capacit\xe9s remarquables en programmation et en agents autonomes, avec 1T de param\xe8tres au total et 32B de param\xe8tres activ\xe9s. Dans les principaux tests de r\xe9f\xe9rence couvrant le raisonnement g\xe9n\xe9ral, la programmation, les math\xe9matiques et les agents, le mod\xe8le K2 surpasse les autres mod\xe8les open source majeurs."},"kimi-k2:1t":{"description":"Kimi K2 est un mod\xe8le de langage \xe0 experts mixtes \xe0 grande \xe9chelle (MoE) d\xe9velopp\xe9 par l\'IA de la face cach\xe9e de la Lune, avec un total de 1 000 milliards de param\xe8tres et 32 milliards de param\xe8tres activ\xe9s par passage avant. Il est optimis\xe9 pour les capacit\xe9s d\'agent, incluant l\'utilisation avanc\xe9e d\'outils, le raisonnement et la synth\xe8se de code."},"kimi-latest":{"description":"Le produit d\'assistant intelligent Kimi utilise le dernier mod\xe8le Kimi, qui peut inclure des fonctionnalit\xe9s encore instables. Il prend en charge la compr\xe9hension des images et choisit automatiquement le mod\xe8le de facturation 8k/32k/128k en fonction de la longueur du contexte de la demande."},"kimi-thinking-preview":{"description":"Le mod\xe8le kimi-thinking-preview, fourni par Moon\'s Dark Side, est un mod\xe8le de r\xe9flexion multimodal dot\xe9 de capacit\xe9s de raisonnement g\xe9n\xe9ral et multimodal. Il excelle dans le raisonnement approfondi, aidant \xe0 r\xe9soudre des probl\xe8mes plus complexes."},"learnlm-1.5-pro-experimental":{"description":"LearnLM est un mod\xe8le de langage exp\xe9rimental, sp\xe9cifique \xe0 des t\xe2ches, form\xe9 pour respecter les principes des sciences de l\'apprentissage, capable de suivre des instructions syst\xe9matiques dans des contextes d\'enseignement et d\'apprentissage, agissant comme un mentor expert, entre autres."},"learnlm-2.0-flash-experimental":{"description":"LearnLM est un mod\xe8le de langage exp\xe9rimental, sp\xe9cifique \xe0 des t\xe2ches, form\xe9 pour respecter les principes des sciences de l\'apprentissage, capable de suivre des instructions syst\xe9matiques dans des contextes d\'enseignement et d\'apprentissage, agissant comme un mentor expert, entre autres."},"lite":{"description":"Spark Lite est un mod\xe8le de langage l\xe9ger, offrant une latence extr\xeamement faible et une capacit\xe9 de traitement efficace, enti\xe8rement gratuit et ouvert, prenant en charge la recherche en temps r\xe9el. Sa capacit\xe9 de r\xe9ponse rapide le rend exceptionnel pour les applications d\'inf\xe9rence sur des appareils \xe0 faible puissance de calcul et pour le r\xe9glage des mod\xe8les, offrant aux utilisateurs un excellent rapport co\xfbt-efficacit\xe9 et une exp\xe9rience intelligente, en particulier dans les sc\xe9narios de questions-r\xe9ponses, de g\xe9n\xe9ration de contenu et de recherche."},"llama-3.1-70b-versatile":{"description":"Llama 3.1 70B offre une capacit\xe9 de raisonnement AI plus puissante, adapt\xe9e aux applications complexes, prenant en charge un traitement de calcul intensif tout en garantissant efficacit\xe9 et pr\xe9cision."},"llama-3.1-8b-instant":{"description":"Llama 3.1 8B est un mod\xe8le \xe0 haute performance, offrant une capacit\xe9 de g\xe9n\xe9ration de texte rapide, particuli\xe8rement adapt\xe9 aux sc\xe9narios d\'application n\xe9cessitant une efficacit\xe9 \xe0 grande \xe9chelle et un rapport co\xfbt-efficacit\xe9."},"llama-3.1-instruct":{"description":"Le mod\xe8le d\'instructions affin\xe9 Llama 3.1 est optimis\xe9 pour les sc\xe9narios de dialogue, surpassant de nombreux mod\xe8les de chat open source existants dans les tests de r\xe9f\xe9rence courants de l\'industrie."},"llama-3.2-11b-vision-instruct":{"description":"Capacit\xe9s d\'inf\xe9rence d\'image exceptionnelles sur des images haute r\xe9solution, adapt\xe9es aux applications de compr\xe9hension visuelle."},"llama-3.2-11b-vision-preview":{"description":"Llama 3.2 est con\xe7u pour traiter des t\xe2ches combinant des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches telles que la description d\'images et les questions-r\xe9ponses visuelles, comblant le foss\xe9 entre la g\xe9n\xe9ration de langage et le raisonnement visuel."},"llama-3.2-90b-vision-instruct":{"description":"Capacit\xe9s d\'inf\xe9rence d\'image avanc\xe9es pour les applications d\'agents de compr\xe9hension visuelle."},"llama-3.2-90b-vision-preview":{"description":"Llama 3.2 est con\xe7u pour traiter des t\xe2ches combinant des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches telles que la description d\'images et les questions-r\xe9ponses visuelles, comblant le foss\xe9 entre la g\xe9n\xe9ration de langage et le raisonnement visuel."},"llama-3.2-vision-instruct":{"description":"Le mod\xe8le Llama 3.2-Vision optimis\xe9 pour les instructions est sp\xe9cialis\xe9 dans la reconnaissance visuelle, le raisonnement sur images, la description d\'images et la r\xe9ponse aux questions g\xe9n\xe9rales li\xe9es aux images."},"llama-3.3-70b":{"description":"Llama 3.3 70B : un mod\xe8le Llama de taille moyenne \xe0 grande, \xe9quilibrant capacit\xe9s de raisonnement et d\xe9bit \xe9lev\xe9."},"llama-3.3-70b-versatile":{"description":"Le mod\xe8le de langage multilingue Llama 3.3 de Meta (LLM) est un mod\xe8le g\xe9n\xe9ratif pr\xe9-entra\xeen\xe9 et affin\xe9 par instructions avec 70B (entr\xe9e/sortie de texte). Le mod\xe8le Llama 3.3 affin\xe9 par instructions est optimis\xe9 pour les cas d\'utilisation de dialogue multilingue et surpasse de nombreux mod\xe8les de chat open-source et ferm\xe9s disponibles sur des benchmarks industriels courants."},"llama-3.3-instruct":{"description":"Le mod\xe8le d\'instructions affin\xe9 Llama 3.3 est optimis\xe9 pour les sc\xe9narios de dialogue, surpassant de nombreux mod\xe8les de chat open source existants dans les tests de r\xe9f\xe9rence courants de l\'industrie."},"llama-4-scout-17b-16e-instruct":{"description":"Llama 4 Scout : un mod\xe8le haute performance de la s\xe9rie Llama, con\xe7u pour les sc\xe9narios n\xe9cessitant un haut d\xe9bit et une faible latence."},"llama3-70b-8192":{"description":"Meta Llama 3 70B offre une capacit\xe9 de traitement de complexit\xe9 in\xe9gal\xe9e, sur mesure pour des projets exigeants."},"llama3-8b-8192":{"description":"Meta Llama 3 8B offre d\'excellentes performances de raisonnement, adapt\xe9es \xe0 des besoins d\'application vari\xe9s."},"llama3-groq-70b-8192-tool-use-preview":{"description":"Llama 3 Groq 70B Tool Use offre de puissantes capacit\xe9s d\'appel d\'outils, prenant en charge le traitement efficace de t\xe2ches complexes."},"llama3-groq-8b-8192-tool-use-preview":{"description":"Llama 3 Groq 8B Tool Use est un mod\xe8le optimis\xe9 pour une utilisation efficace des outils, prenant en charge un calcul parall\xe8le rapide."},"llama3.1":{"description":"Llama 3.1 est le mod\xe8le de pointe lanc\xe9 par Meta, prenant en charge jusqu\'\xe0 405B de param\xe8tres, applicable dans les domaines des dialogues complexes, de la traduction multilingue et de l\'analyse de donn\xe9es."},"llama3.1-8b":{"description":"Llama 3.1 8B : une variante l\xe9g\xe8re et \xe0 faible latence de Llama, adapt\xe9e aux sc\xe9narios d\'inf\xe9rence en ligne et d\'interaction l\xe9gers."},"llama3.1:405b":{"description":"Llama 3.1 est le mod\xe8le de pointe lanc\xe9 par Meta, prenant en charge jusqu\'\xe0 405B de param\xe8tres, applicable dans les domaines des dialogues complexes, de la traduction multilingue et de l\'analyse de donn\xe9es."},"llama3.1:70b":{"description":"Llama 3.1 est le mod\xe8le de pointe lanc\xe9 par Meta, prenant en charge jusqu\'\xe0 405B de param\xe8tres, applicable dans les domaines des dialogues complexes, de la traduction multilingue et de l\'analyse de donn\xe9es."},"llava":{"description":"LLaVA est un mod\xe8le multimodal combinant un encodeur visuel et Vicuna, utilis\xe9 pour une compr\xe9hension puissante du visuel et du langage."},"llava-v1.5-7b-4096-preview":{"description":"LLaVA 1.5 7B offre une capacit\xe9 de traitement visuel int\xe9gr\xe9e, g\xe9n\xe9rant des sorties complexes \xe0 partir d\'entr\xe9es d\'informations visuelles."},"llava:13b":{"description":"LLaVA est un mod\xe8le multimodal combinant un encodeur visuel et Vicuna, utilis\xe9 pour une compr\xe9hension puissante du visuel et du langage."},"llava:34b":{"description":"LLaVA est un mod\xe8le multimodal combinant un encodeur visuel et Vicuna, utilis\xe9 pour une compr\xe9hension puissante du visuel et du langage."},"magistral-medium-latest":{"description":"Magistral Medium 1.2 est un mod\xe8le d\'inf\xe9rence de pointe avec support visuel, publi\xe9 par Mistral AI en septembre 2025."},"magistral-small-2509":{"description":"Magistral Small 1.2 est un mod\xe8le d\'inf\xe9rence open source compact avec support visuel, publi\xe9 par Mistral AI en septembre 2025."},"mathstral":{"description":"MathΣtral est con\xe7u pour la recherche scientifique et le raisonnement math\xe9matique, offrant des capacit\xe9s de calcul efficaces et des interpr\xe9tations de r\xe9sultats."},"max-32k":{"description":"Spark Max 32K est \xe9quip\xe9 d\'une grande capacit\xe9 de traitement de contexte, avec une compr\xe9hension contextuelle et des capacit\xe9s de raisonnement logique renforc\xe9es, prenant en charge des entr\xe9es textuelles de 32K tokens, adapt\xe9 \xe0 la lecture de documents longs, aux questions-r\xe9ponses priv\xe9es et \xe0 d\'autres sc\xe9narios."},"megrez-3b-instruct":{"description":"Megrez 3B Instruct est un mod\xe8le efficace \xe0 faible nombre de param\xe8tres lanc\xe9 par Wuwen Xinqiong."},"meituan/longcat-flash-chat":{"description":"Mod\xe8le de base non r\xe9flexif open source de Meituan, optimis\xe9 pour les interactions conversationnelles et les t\xe2ches d\'agents intelligents, offrant d\'excellentes performances dans les appels d\'outils et les sc\xe9narios d\'interactions complexes \xe0 plusieurs tours."},"meta-llama-3-70b-instruct":{"description":"Un puissant mod\xe8le de 70 milliards de param\xe8tres excelling dans le raisonnement, le codage et les applications linguistiques larges."},"meta-llama-3-8b-instruct":{"description":"Un mod\xe8le polyvalent de 8 milliards de param\xe8tres optimis\xe9 pour les t\xe2ches de dialogue et de g\xe9n\xe9ration de texte."},"meta-llama-3.1-405b-instruct":{"description":"Les mod\xe8les textuels uniquement ajust\xe9s par instruction Llama 3.1 sont optimis\xe9s pour les cas d\'utilisation de dialogue multilingue et surpassent de nombreux mod\xe8les de chat open source et ferm\xe9s disponibles sur les benchmarks industriels courants."},"meta-llama-3.1-70b-instruct":{"description":"Les mod\xe8les textuels uniquement ajust\xe9s par instruction Llama 3.1 sont optimis\xe9s pour les cas d\'utilisation de dialogue multilingue et surpassent de nombreux mod\xe8les de chat open source et ferm\xe9s disponibles sur les benchmarks industriels courants."},"meta-llama-3.1-8b-instruct":{"description":"Les mod\xe8les textuels uniquement ajust\xe9s par instruction Llama 3.1 sont optimis\xe9s pour les cas d\'utilisation de dialogue multilingue et surpassent de nombreux mod\xe8les de chat open source et ferm\xe9s disponibles sur les benchmarks industriels courants."},"meta-llama/Llama-2-13b-chat-hf":{"description":"LLaMA-2 Chat (13B) offre d\'excellentes capacit\xe9s de traitement du langage et une exp\xe9rience interactive exceptionnelle."},"meta-llama/Llama-2-70b-hf":{"description":"LLaMA-2 offre d\'excellentes capacit\xe9s de traitement du langage et une exp\xe9rience d\'interaction exceptionnelle."},"meta-llama/Llama-3-70b-chat-hf":{"description":"LLaMA-3 Chat (70B) est un mod\xe8le de chat puissant, prenant en charge des besoins de dialogue complexes."},"meta-llama/Llama-3-8b-chat-hf":{"description":"LLaMA-3 Chat (8B) offre un support multilingue, couvrant un large \xe9ventail de connaissances."},"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo":{"description":"LLaMA 3.2 est con\xe7u pour traiter des t\xe2ches qui combinent des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches comme la description d\'image et le questionnement visuel, comblant le foss\xe9 entre g\xe9n\xe9ration de langage et raisonnement visuel."},"meta-llama/Llama-3.2-3B-Instruct-Turbo":{"description":"LLaMA 3.2 est con\xe7u pour traiter des t\xe2ches qui combinent des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches comme la description d\'image et le questionnement visuel, comblant le foss\xe9 entre g\xe9n\xe9ration de langage et raisonnement visuel."},"meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo":{"description":"LLaMA 3.2 est con\xe7u pour traiter des t\xe2ches qui combinent des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches comme la description d\'image et le questionnement visuel, comblant le foss\xe9 entre g\xe9n\xe9ration de langage et raisonnement visuel."},"meta-llama/Llama-3.3-70B-Instruct-Turbo":{"description":"Le mod\xe8le de langage multilingue Meta Llama 3.3 (LLM) est un mod\xe8le g\xe9n\xe9ratif pr\xe9-entra\xeen\xe9 et ajust\xe9 par instruction de 70B (entr\xe9e/sortie de texte). Le mod\xe8le de texte pur ajust\xe9 par instruction Llama 3.3 est optimis\xe9 pour les cas d\'utilisation de dialogue multilingue et surpasse de nombreux mod\xe8les de chat open source et ferm\xe9s sur des benchmarks industriels courants."},"meta-llama/Llama-Vision-Free":{"description":"LLaMA 3.2 est con\xe7u pour traiter des t\xe2ches qui combinent des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches comme la description d\'image et le questionnement visuel, comblant le foss\xe9 entre g\xe9n\xe9ration de langage et raisonnement visuel."},"meta-llama/Meta-Llama-3-70B-Instruct-Lite":{"description":"Llama 3 70B Instruct Lite est adapt\xe9 aux environnements n\xe9cessitant une haute performance et une faible latence."},"meta-llama/Meta-Llama-3-70B-Instruct-Turbo":{"description":"Llama 3 70B Instruct Turbo offre une compr\xe9hension et une g\xe9n\xe9ration de langage exceptionnelles, adapt\xe9 aux t\xe2ches de calcul les plus exigeantes."},"meta-llama/Meta-Llama-3-8B-Instruct-Lite":{"description":"Llama 3 8B Instruct Lite est adapt\xe9 aux environnements \xe0 ressources limit\xe9es, offrant un excellent \xe9quilibre de performance."},"meta-llama/Meta-Llama-3-8B-Instruct-Turbo":{"description":"Llama 3 8B Instruct Turbo est un mod\xe8le de langage \xe0 haute performance, prenant en charge une large gamme de sc\xe9narios d\'application."},"meta-llama/Meta-Llama-3.1-405B-Instruct":{"description":"LLaMA 3.1 405B est un mod\xe8le puissant pour le pr\xe9-entra\xeenement et l\'ajustement des instructions."},"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo":{"description":"Le mod\xe8le Llama 3.1 Turbo 405B offre un support de contexte de tr\xe8s grande capacit\xe9 pour le traitement de grandes donn\xe9es, se distinguant dans les applications d\'intelligence artificielle \xe0 tr\xe8s grande \xe9chelle."},"meta-llama/Meta-Llama-3.1-70B":{"description":"Llama 3.1 est le mod\xe8le de pointe lanc\xe9 par Meta, prenant en charge jusqu\'\xe0 405B de param\xe8tres, applicable aux dialogues complexes, \xe0 la traduction multilingue et \xe0 l\'analyse de donn\xe9es."},"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo":{"description":"Le mod\xe8le Llama 3.1 70B est finement ajust\xe9 pour des applications \xe0 forte charge, quantifi\xe9 en FP8 pour offrir une capacit\xe9 de calcul et une pr\xe9cision plus efficaces, garantissant des performances exceptionnelles dans des sc\xe9narios complexes."},"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo":{"description":"Le mod\xe8le Llama 3.1 8B utilise la quantification FP8, prenant en charge jusqu\'\xe0 131 072 jetons de contexte, se distinguant parmi les mod\xe8les open source, adapt\xe9 aux t\xe2ches complexes, surpassant de nombreux benchmarks industriels."},"meta-llama/llama-3-70b-instruct":{"description":"Llama 3 70B Instruct est optimis\xe9 pour des sc\xe9narios de dialogue de haute qualit\xe9, affichant d\'excellentes performances dans diverses \xe9valuations humaines."},"meta-llama/llama-3-8b-instruct":{"description":"Llama 3 8B Instruct optimise les sc\xe9narios de dialogue de haute qualit\xe9, avec des performances sup\xe9rieures \xe0 de nombreux mod\xe8les ferm\xe9s."},"meta-llama/llama-3.1-70b-instruct":{"description":"Llama 3.1 70B Instruct est con\xe7u pour des dialogues de haute qualit\xe9, se distinguant dans les \xe9valuations humaines, particuli\xe8rement adapt\xe9 aux sc\xe9narios d\'interaction \xe9lev\xe9e."},"meta-llama/llama-3.1-8b-instruct":{"description":"Llama 3.1 8B Instruct est la derni\xe8re version lanc\xe9e par Meta, optimis\xe9e pour des sc\xe9narios de dialogue de haute qualit\xe9, surpassant de nombreux mod\xe8les ferm\xe9s de premier plan."},"meta-llama/llama-3.1-8b-instruct:free":{"description":"LLaMA 3.1 offre un support multilingue et est l\'un des mod\xe8les g\xe9n\xe9ratifs les plus avanc\xe9s de l\'industrie."},"meta-llama/llama-3.2-11b-vision-instruct":{"description":"LLaMA 3.2 est con\xe7u pour traiter des t\xe2ches combinant des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches telles que la description d\'images et les questions-r\xe9ponses visuelles, comblant le foss\xe9 entre la g\xe9n\xe9ration de langage et le raisonnement visuel."},"meta-llama/llama-3.2-3b-instruct":{"description":"meta-llama/llama-3.2-3b-instruct"},"meta-llama/llama-3.2-90b-vision-instruct":{"description":"LLaMA 3.2 est con\xe7u pour traiter des t\xe2ches combinant des donn\xe9es visuelles et textuelles. Il excelle dans des t\xe2ches telles que la description d\'images et les questions-r\xe9ponses visuelles, comblant le foss\xe9 entre la g\xe9n\xe9ration de langage et le raisonnement visuel."},"meta-llama/llama-3.3-70b-instruct":{"description":"Llama 3.3 est le mod\xe8le de langage open source multilingue le plus avanc\xe9 de la s\xe9rie Llama, offrant des performances comparables \xe0 celles du mod\xe8le 405B \xe0 un co\xfbt tr\xe8s bas. Bas\xe9 sur une architecture Transformer, il am\xe9liore son utilit\xe9 et sa s\xe9curit\xe9 gr\xe2ce \xe0 un ajustement supervis\xe9 (SFT) et un apprentissage par renforcement avec retour humain (RLHF). Sa version optimis\xe9e pour les instructions est sp\xe9cialement con\xe7ue pour les dialogues multilingues et surpasse de nombreux mod\xe8les de chat open source et ferm\xe9s sur plusieurs benchmarks industriels. La date limite des connaissances est d\xe9cembre 2023."},"meta-llama/llama-3.3-70b-instruct:free":{"description":"Llama 3.3 est le mod\xe8le de langage open source multilingue le plus avanc\xe9 de la s\xe9rie Llama, offrant des performances comparables \xe0 celles du mod\xe8le 405B \xe0 un co\xfbt tr\xe8s bas. Bas\xe9 sur une architecture Transformer, il am\xe9liore son utilit\xe9 et sa s\xe9curit\xe9 gr\xe2ce \xe0 un ajustement supervis\xe9 (SFT) et un apprentissage par renforcement avec retour humain (RLHF). Sa version optimis\xe9e pour les instructions est sp\xe9cialement con\xe7ue pour les dialogues multilingues et surpasse de nombreux mod\xe8les de chat open source et ferm\xe9s sur plusieurs benchmarks industriels. La date limite des connaissances est d\xe9cembre 2023."},"meta.llama3-1-405b-instruct-v1:0":{"description":"Meta Llama 3.1 405B Instruct est le mod\xe8le le plus grand et le plus puissant du mod\xe8le Llama 3.1 Instruct. C\'est un mod\xe8le de g\xe9n\xe9ration de donn\xe9es de dialogue et de raisonnement hautement avanc\xe9, qui peut \xe9galement servir de base pour un pr\xe9-entra\xeenement ou un ajustement fin sp\xe9cialis\xe9 dans des domaines sp\xe9cifiques. Les mod\xe8les de langage multilingues (LLMs) fournis par Llama 3.1 sont un ensemble de mod\xe8les g\xe9n\xe9ratifs pr\xe9-entra\xeen\xe9s et ajust\xe9s par instructions, comprenant des tailles de 8B, 70B et 405B (entr\xe9e/sortie de texte). Les mod\xe8les de texte ajust\xe9s par instructions de Llama 3.1 (8B, 70B, 405B) sont optimis\xe9s pour des cas d\'utilisation de dialogue multilingue et ont surpass\xe9 de nombreux mod\xe8les de chat open source disponibles dans des benchmarks industriels courants. Llama 3.1 est con\xe7u pour des usages commerciaux et de recherche dans plusieurs langues. Les mod\xe8les de texte ajust\xe9s par instructions conviennent aux chats de type assistant, tandis que les mod\xe8les pr\xe9-entra\xeen\xe9s peuvent s\'adapter \xe0 diverses t\xe2ches de g\xe9n\xe9ration de langage naturel. Le mod\xe8le Llama 3.1 prend \xe9galement en charge l\'am\xe9lioration d\'autres mod\xe8les en utilisant sa sortie, y compris la g\xe9n\xe9ration de donn\xe9es synth\xe9tiques et le raffinement. Llama 3.1 est un mod\xe8le de langage autoregressif utilisant une architecture de transformateur optimis\xe9e. Les versions ajust\xe9es utilisent un ajustement fin supervis\xe9 (SFT) et un apprentissage par renforcement avec retour humain (RLHF) pour r\xe9pondre aux pr\xe9f\xe9rences humaines en mati\xe8re d\'utilit\xe9 et de s\xe9curit\xe9."},"meta.llama3-1-70b-instruct-v1:0":{"description":"Meta Llama 3.1 70B Instruct est une version mise \xe0 jour, incluant une longueur de contexte \xe9tendue de 128K, une multilinguisme et des capacit\xe9s de raisonnement am\xe9lior\xe9es. Les mod\xe8les de langage \xe0 grande \xe9chelle (LLMs) fournis par Llama 3.1 sont un ensemble de mod\xe8les g\xe9n\xe9ratifs pr\xe9-entra\xeen\xe9s et ajust\xe9s par instruction, comprenant des tailles de 8B, 70B et 405B (entr\xe9e/sortie de texte). Les mod\xe8les de texte ajust\xe9s par instruction de Llama 3.1 (8B, 70B, 405B) sont optimis\xe9s pour des cas d\'utilisation de dialogue multilingue et ont surpass\xe9 de nombreux mod\xe8les de chat open source disponibles dans des benchmarks industriels courants. Llama 3.1 est con\xe7u pour des usages commerciaux et de recherche dans plusieurs langues. Les mod\xe8les de texte ajust\xe9s par instruction sont adapt\xe9s aux chats de type assistant, tandis que les mod\xe8les pr\xe9-entra\xeen\xe9s peuvent s\'adapter \xe0 diverses t\xe2ches de g\xe9n\xe9ration de langage naturel. Le mod\xe8le Llama 3.1 prend \xe9galement en charge l\'utilisation de ses sorties pour am\xe9liorer d\'autres mod\xe8les, y compris la g\xe9n\xe9ration de donn\xe9es synth\xe9tiques et le raffinement. Llama 3.1 est un mod\xe8le de langage autoregressif utilisant une architecture de transformateur optimis\xe9e. La version ajust\xe9e utilise un affinement supervis\xe9 (SFT) et un apprentissage par renforcement avec retour humain (RLHF) pour r\xe9pondre aux pr\xe9f\xe9rences humaines en mati\xe8re d\'utilit\xe9 et de s\xe9curit\xe9."},"meta.llama3-1-8b-instruct-v1:0":{"description":"Meta Llama 3.1 8B Instruct est une version mise \xe0 jour, incluant une longueur de contexte \xe9tendue de 128K, une multilinguisme et des capacit\xe9s de raisonnement am\xe9lior\xe9es. Les mod\xe8les de langage \xe0 grande \xe9chelle (LLMs) fournis par Llama 3.1 sont un ensemble de mod\xe8les g\xe9n\xe9ratifs pr\xe9-entra\xeen\xe9s et ajust\xe9s par instruction, comprenant des tailles de 8B, 70B et 405B (entr\xe9e/sortie de texte). Les mod\xe8les de texte ajust\xe9s par instruction de Llama 3.1 (8B, 70B, 405B) sont optimis\xe9s pour des cas d\'utilisation de dialogue multilingue et ont surpass\xe9 de nombreux mod\xe8les de chat open source disponibles dans des benchmarks industriels courants. Llama 3.1 est con\xe7u pour des usages commerciaux et de recherche dans plusieurs langues. Les mod\xe8les de texte ajust\xe9s par instruction sont adapt\xe9s aux chats de type assistant, tandis que les mod\xe8les pr\xe9-entra\xeen\xe9s peuvent s\'adapter \xe0 diverses t\xe2ches de g\xe9n\xe9ration de langage naturel. Le mod\xe8le Llama 3.1 prend \xe9galement en charge l\'utilisation de ses sorties pour am\xe9liorer d\'autres mod\xe8les, y compris la g\xe9n\xe9ration de donn\xe9es synth\xe9tiques et le raffinement. Llama 3.1 est un mod\xe8le de langage autoregressif utilisant une architecture de transformateur optimis\xe9e. La version ajust\xe9e utilise un affinement supervis\xe9 (SFT) et un apprentissage par renforcement avec retour humain (RLHF) pour r\xe9pondre aux pr\xe9f\xe9rences humaines en mati\xe8re d\'utilit\xe9 et de s\xe9curit\xe9."},"meta.llama3-70b-instruct-v1:0":{"description":"Meta Llama 3 est un mod\xe8le de langage ouvert (LLM) destin\xe9 aux d\xe9veloppeurs, chercheurs et entreprises, con\xe7u pour les aider \xe0 construire, exp\xe9rimenter et \xe9tendre de mani\xe8re responsable leurs id\xe9es d\'IA g\xe9n\xe9rative. En tant que partie int\xe9grante d\'un syst\xe8me de base pour l\'innovation de la communaut\xe9 mondiale, il est particuli\xe8rement adapt\xe9 \xe0 la cr\xe9ation de contenu, \xe0 l\'IA de dialogue, \xe0 la compr\xe9hension du langage, \xe0 la recherche et aux applications d\'entreprise."},"meta.llama3-8b-instruct-v1:0":{"description":"Meta Llama 3 est un mod\xe8le de langage ouvert (LLM) destin\xe9 aux d\xe9veloppeurs, chercheurs et entreprises, con\xe7u pour les aider \xe0 construire, exp\xe9rimenter et \xe9tendre de mani\xe8re responsable leurs id\xe9es d\'IA g\xe9n\xe9rative. En tant que partie int\xe9grante d\'un syst\xe8me de base pour l\'innovation de la communaut\xe9 mondiale, il est particuli\xe8rement adapt\xe9 aux appareils \xe0 capacit\xe9 de calcul et de ressources limit\xe9es, ainsi qu\'\xe0 des temps d\'entra\xeenement plus rapides."},"meta/Llama-3.2-11B-Vision-Instruct":{"description":"Excellentes capacit\xe9s d\'inf\xe9rence d\'images haute r\xe9solution, adapt\xe9 aux applications de compr\xe9hension visuelle."},"meta/Llama-3.2-90B-Vision-Instruct":{"description":"Capacit\xe9s avanc\xe9es d\'inf\xe9rence d\'images pour applications d\'agents de compr\xe9hension visuelle."},"meta/Llama-3.3-70B-Instruct":{"description":"Llama 3.3 est le mod\xe8le open source multilingue le plus avanc\xe9 de la s\xe9rie Llama, offrant des performances comparables \xe0 un mod\xe8le de 405 milliards de param\xe8tres \xe0 tr\xe8s faible co\xfbt. Bas\xe9 sur l\'architecture Transformer, il est am\xe9lior\xe9 par un ajustement supervis\xe9 (SFT) et un apprentissage par renforcement avec retour humain (RLHF) pour une meilleure utilit\xe9 et s\xe9curit\xe9. Sa version optimis\xe9e pour les instructions est con\xe7ue pour les dialogues multilingues et surpasse de nombreux mod\xe8les de chat open source et propri\xe9taires sur plusieurs benchmarks industriels. Date de coupure des connaissances : d\xe9cembre 2023."},"meta/Meta-Llama-3-70B-Instruct":{"description":"Un puissant mod\xe8le de 70 milliards de param\xe8tres, excellent en inf\xe9rence, codage et applications linguistiques \xe9tendues."},"meta/Meta-Llama-3-8B-Instruct":{"description":"Un mod\xe8le polyvalent de 8 milliards de param\xe8tres, optimis\xe9 pour les t\xe2ches de dialogue et de g\xe9n\xe9ration de texte."},"meta/Meta-Llama-3.1-405B-Instruct":{"description":"Mod\xe8le textuel Llama 3.1 ajust\xe9 aux instructions, optimis\xe9 pour les cas d\'usage de dialogue multilingue, performant sur de nombreux benchmarks industriels parmi les mod\xe8les de chat open source et propri\xe9taires disponibles."},"meta/Meta-Llama-3.1-70B-Instruct":{"description":"Mod\xe8le textuel Llama 3.1 ajust\xe9 aux instructions, optimis\xe9 pour les cas d\'usage de dialogue multilingue, performant sur de nombreux benchmarks industriels parmi les mod\xe8les de chat open source et propri\xe9taires disponibles."},"meta/Meta-Llama-3.1-8B-Instruct":{"description":"Mod\xe8le textuel Llama 3.1 ajust\xe9 aux instructions, optimis\xe9 pour les cas d\'usage de dialogue multilingue, performant sur de nombreux benchmarks industriels parmi les mod\xe8les de chat open source et propri\xe9taires disponibles."},"meta/llama-3-70b":{"description":"Mod\xe8le open source de 70 milliards de param\xe8tres finement ajust\xe9 par Meta pour le suivi des instructions. Servi par Groq avec son mat\xe9riel personnalis\xe9 LPU pour un raisonnement rapide et efficace."},"meta/llama-3-8b":{"description":"Mod\xe8le open source de 8 milliards de param\xe8tres finement ajust\xe9 par Meta pour le suivi des instructions. Servi par Groq avec son mat\xe9riel personnalis\xe9 LPU pour un raisonnement rapide et efficace."},"meta/llama-3.1-405b-instruct":{"description":"LLM avanc\xe9, prenant en charge la g\xe9n\xe9ration de donn\xe9es synth\xe9tiques, la distillation de connaissances et le raisonnement, adapt\xe9 aux chatbots, \xe0 la programmation et aux t\xe2ches sp\xe9cifiques."},"meta/llama-3.1-70b":{"description":"Version mise \xe0 jour de Meta Llama 3 70B Instruct, incluant une longueur de contexte \xe9tendue \xe0 128K, multilingue et capacit\xe9s de raisonnement am\xe9lior\xe9es."},"meta/llama-3.1-70b-instruct":{"description":"Permet des dialogues complexes, avec une excellente compr\xe9hension du contexte, des capacit\xe9s de raisonnement et de g\xe9n\xe9ration de texte."},"meta/llama-3.1-8b":{"description":"Llama 3.1 8B supporte une fen\xeatre de contexte de 128K, id\xe9al pour les interfaces de dialogue en temps r\xe9el et l\'analyse de donn\xe9es, tout en offrant des \xe9conomies de co\xfbts significatives par rapport aux mod\xe8les plus grands. Servi par Groq avec son mat\xe9riel personnalis\xe9 LPU pour un raisonnement rapide et efficace."},"meta/llama-3.1-8b-instruct":{"description":"Mod\xe8le de pointe avanc\xe9, dot\xe9 de compr\xe9hension linguistique, d\'excellentes capacit\xe9s de raisonnement et de g\xe9n\xe9ration de texte."},"meta/llama-3.2-11b":{"description":"Mod\xe8le de g\xe9n\xe9ration d\'inf\xe9rence d\'image ajust\xe9 par instruction (entr\xe9e texte + image / sortie texte), optimis\xe9 pour la reconnaissance visuelle, l\'inf\xe9rence d\'image, la g\xe9n\xe9ration de l\xe9gendes et la r\xe9ponse aux questions g\xe9n\xe9rales sur les images."},"meta/llama-3.2-11b-vision-instruct":{"description":"Mod\xe8le visuel-linguistique de pointe, sp\xe9cialis\xe9 dans le raisonnement de haute qualit\xe9 \xe0 partir d\'images."},"meta/llama-3.2-1b":{"description":"Mod\xe8le uniquement textuel, supportant les cas d\'usage sur appareil tels que la recherche locale multilingue, le r\xe9sum\xe9 et la r\xe9\xe9criture."},"meta/llama-3.2-1b-instruct":{"description":"Mod\xe8le de langage de pointe de petite taille, dot\xe9 de compr\xe9hension linguistique, d\'excellentes capacit\xe9s de raisonnement et de g\xe9n\xe9ration de texte."},"meta/llama-3.2-3b":{"description":"Mod\xe8le uniquement textuel, finement ajust\xe9 pour supporter les cas d\'usage sur appareil tels que la recherche locale multilingue, le r\xe9sum\xe9 et la r\xe9\xe9criture."},"meta/llama-3.2-3b-instruct":{"description":"Mod\xe8le de langage de pointe de petite taille, dot\xe9 de compr\xe9hension linguistique, d\'excellentes capacit\xe9s de raisonnement et de g\xe9n\xe9ration de texte."},"meta/llama-3.2-90b":{"description":"Mod\xe8le de g\xe9n\xe9ration d\'inf\xe9rence d\'image ajust\xe9 par instruction (entr\xe9e texte + image / sortie texte), optimis\xe9 pour la reconnaissance visuelle, l\'inf\xe9rence d\'image, la g\xe9n\xe9ration de l\xe9gendes et la r\xe9ponse aux questions g\xe9n\xe9rales sur les images."},"meta/llama-3.2-90b-vision-instruct":{"description":"Mod\xe8le visuel-linguistique de pointe, sp\xe9cialis\xe9 dans le raisonnement de haute qualit\xe9 \xe0 partir d\'images."},"meta/llama-3.3-70b":{"description":"Combinaison parfaite de performance et d\'efficacit\xe9. Ce mod\xe8le supporte une IA conversationnelle haute performance, con\xe7u pour la cr\xe9ation de contenu, les applications d\'entreprise et la recherche, offrant des capacit\xe9s avanc\xe9es de compr\xe9hension du langage, incluant r\xe9sum\xe9 de texte, classification, analyse de sentiment et g\xe9n\xe9ration de code."},"meta/llama-3.3-70b-instruct":{"description":"LLM avanc\xe9, sp\xe9cialis\xe9 dans le raisonnement, les math\xe9matiques, le bon sens et les appels de fonction."},"meta/llama-4-maverick":{"description":"La collection de mod\xe8les Llama 4 est une IA multimodale native, supportant les exp\xe9riences textuelles et multimodales. Ces mod\xe8les utilisent une architecture d\'experts hybrides pour offrir des performances de pointe en compr\xe9hension de texte et d\'image. Llama 4 Maverick, un mod\xe8le de 17 milliards de param\xe8tres avec 128 experts, est servi par DeepInfra."},"meta/llama-4-scout":{"description":"La collection de mod\xe8les Llama 4 est une IA multimodale native, supportant les exp\xe9riences textuelles et multimodales. Ces mod\xe8les utilisent une architecture d\'experts hybrides pour offrir des performances de pointe en compr\xe9hension de texte et d\'image. Llama 4 Scout, un mod\xe8le de 17 milliards de param\xe8tres avec 16 experts, est servi par DeepInfra."},"microsoft/Phi-3-medium-128k-instruct":{"description":"M\xeame mod\xe8le Phi-3-medium, mais avec une taille de contexte plus grande, adapt\xe9 au RAG ou aux prompts courts."},"microsoft/Phi-3-medium-4k-instruct":{"description":"Mod\xe8le de 14 milliards de param\xe8tres, de meilleure qualit\xe9 que Phi-3-mini, ax\xe9 sur des donn\xe9es de haute qualit\xe9 et \xe0 forte intensit\xe9 d\'inf\xe9rence."},"microsoft/Phi-3-mini-128k-instruct":{"description":"M\xeame mod\xe8le Phi-3-mini, mais avec une taille de contexte plus grande, adapt\xe9 au RAG ou aux prompts courts."},"microsoft/Phi-3-mini-4k-instruct":{"description":"Le plus petit membre de la famille Phi-3, optimis\xe9 pour la qualit\xe9 et la faible latence."},"microsoft/Phi-3-small-128k-instruct":{"description":"M\xeame mod\xe8le Phi-3-small, mais avec une taille de contexte plus grande, adapt\xe9 au RAG ou aux prompts courts."},"microsoft/Phi-3-small-8k-instruct":{"description":"Mod\xe8le de 7 milliards de param\xe8tres, de meilleure qualit\xe9 que Phi-3-mini, ax\xe9 sur des donn\xe9es de haute qualit\xe9 et \xe0 forte intensit\xe9 d\'inf\xe9rence."},"microsoft/Phi-3.5-mini-instruct":{"description":"Version mise \xe0 jour du mod\xe8le Phi-3-mini."},"microsoft/Phi-3.5-vision-instruct":{"description":"Version mise \xe0 jour du mod\xe8le Phi-3-vision."},"microsoft/WizardLM-2-8x22B":{"description":"WizardLM 2 est un mod\xe8le de langage propos\xe9 par Microsoft AI, qui excelle dans les domaines des dialogues complexes, du multilinguisme, du raisonnement et des assistants intelligents."},"microsoft/wizardlm-2-8x22b":{"description":"WizardLM-2 8x22B est le mod\xe8le Wizard le plus avanc\xe9 de Microsoft AI, montrant des performances extr\xeamement comp\xe9titives."},"minicpm-v":{"description":"MiniCPM-V est un nouveau mod\xe8le multimodal de nouvelle g\xe9n\xe9ration lanc\xe9 par OpenBMB, offrant d\'excellentes capacit\xe9s de reconnaissance OCR et de compr\xe9hension multimodale, prenant en charge une large gamme d\'applications."},"minimax-m2":{"description":"MiniMax M2 est un mod\xe8le de langage de grande taille, efficace et con\xe7u pour les flux de travail en codage et en automatisation."},"minimax/minimax-m2":{"description":"Con\xe7u pour un codage efficace et des flux de travail d\'agents performants."},"minimaxai/minimax-m2":{"description":"MiniMax-M2 est un mod\xe8le MoE (Mixture of Experts) compact, rapide et \xe9conomique, dot\xe9 de 230 milliards de param\xe8tres totaux et de 10 milliards de param\xe8tres actifs. Il est con\xe7u pour offrir des performances optimales dans les t\xe2ches de codage et d\'agents, tout en maintenant une intelligence g\xe9n\xe9rale robuste. Ce mod\xe8le excelle dans l\'\xe9dition multi-fichiers, les boucles de codage-ex\xe9cution-correction, la v\xe9rification et la correction de tests, ainsi que dans les cha\xeenes d\'outils complexes \xe0 long terme, ce qui en fait un choix id\xe9al pour les flux de travail des d\xe9veloppeurs."},"ministral-3b-latest":{"description":"Ministral 3B est le mod\xe8le de pointe de Mistral sur le march\xe9."},"ministral-8b-latest":{"description":"Ministral 8B est un mod\xe8le \xe0 excellent rapport qualit\xe9-prix de Mistral."},"mistral":{"description":"Mistral est le mod\xe8le 7B lanc\xe9 par Mistral AI, adapt\xe9 aux besoins vari\xe9s de traitement du langage."},"mistral-ai/Mistral-Large-2411":{"description":"Le mod\xe8le phare de Mistral, adapt\xe9 aux t\xe2ches complexes n\xe9cessitant une inf\xe9rence \xe0 grande \xe9chelle ou une sp\xe9cialisation \xe9lev\xe9e (g\xe9n\xe9ration de texte synth\xe9tique, g\xe9n\xe9ration de code, RAG ou agents)."},"mistral-ai/Mistral-Nemo":{"description":"Mistral Nemo est un mod\xe8le de langage de pointe (LLM) offrant les meilleures performances en inf\xe9rence, connaissances mondiales et capacit\xe9s de codage dans sa cat\xe9gorie de taille."},"mistral-ai/mistral-small-2503":{"description":"Mistral Small est adapt\xe9 \xe0 toute t\xe2che linguistique n\xe9cessitant haute efficacit\xe9 et faible latence."},"mistral-large":{"description":"Mixtral Large est le mod\xe8le phare de Mistral, combinant des capacit\xe9s de g\xe9n\xe9ration de code, de math\xe9matiques et de raisonnement, prenant en charge une fen\xeatre de contexte de 128k."},"mistral-large-instruct":{"description":"Mistral-Large-Instruct-2407 est un mod\xe8le de langage avanc\xe9 (LLM) dense de grande taille, dot\xe9 de 123 milliards de param\xe8tres, offrant des capacit\xe9s de raisonnement, de connaissances et de codage \xe0 la pointe de la technologie."},"mistral-large-latest":{"description":"Mistral Large est le mod\xe8le phare, excellent pour les t\xe2ches multilingues, le raisonnement complexe et la g\xe9n\xe9ration de code, id\xe9al pour des applications haut de gamme."},"mistral-medium-latest":{"description":"Mistral Medium 3 offre des performances de pointe \xe0 un co\xfbt 8 fois inf\xe9rieur et simplifie fondamentalement le d\xe9ploiement en entreprise."},"mistral-nemo":{"description":"Mistral Nemo, d\xe9velopp\xe9 en collaboration entre Mistral AI et NVIDIA, est un mod\xe8le de 12B \xe0 performance efficace."},"mistral-nemo-instruct":{"description":"Mistral-Nemo-Instruct-2407 est un grand mod\xe8le de langage (LLM) qui est une version affin\xe9e par instructions de Mistral-Nemo-Base-2407."},"mistral-small":{"description":"Mistral Small peut \xeatre utilis\xe9 pour toute t\xe2che bas\xe9e sur le langage n\xe9cessitant une haute efficacit\xe9 et une faible latence."},"mistral-small-latest":{"description":"Mistral Small est une option rentable, rapide et fiable, adapt\xe9e aux cas d\'utilisation tels que la traduction, le r\xe9sum\xe9 et l\'analyse des sentiments."},"mistral/codestral":{"description":"Mistral Codestral 25.01 est un mod\xe8le de codage de pointe, optimis\xe9 pour les cas d\'usage \xe0 faible latence et haute fr\xe9quence. Ma\xeetrisant plus de 80 langages de programmation, il excelle dans les t\xe2ches de remplissage interm\xe9diaire (FIM), correction de code et g\xe9n\xe9ration de tests."},"mistral/codestral-embed":{"description":"Mod\xe8le d\'embedding de code pouvant \xeatre int\xe9gr\xe9 dans des bases de donn\xe9es et d\xe9p\xf4ts de code pour soutenir les assistants de codage."},"mistral/devstral-small":{"description":"Devstral est un grand mod\xe8le de langage agent pour les t\xe2ches d\'ing\xe9nierie logicielle, en faisant un excellent choix pour les agents en ing\xe9nierie logicielle."},"mistral/magistral-medium":{"description":"Pens\xe9e complexe soutenue par une compr\xe9hension profonde, avec un raisonnement transparent que vous pouvez suivre et v\xe9rifier. Ce mod\xe8le maintient un raisonnement fid\xe8le dans de nombreuses langues, m\xeame lors de changements de langue en cours de t\xe2che."},"mistral/magistral-small":{"description":"Pens\xe9e complexe soutenue par une compr\xe9hension profonde, avec un raisonnement transparent que vous pouvez suivre et v\xe9rifier. Ce mod\xe8le maintient un raisonnement fid\xe8le dans de nombreuses langues, m\xeame lors de changements de langue en cours de t\xe2che."},"mistral/ministral-3b":{"description":"Un mod\xe8le compact et efficace pour les t\xe2ches sur appareil telles qu\'assistants intelligents et analyses locales, offrant une faible latence."},"mistral/ministral-8b":{"description":"Un mod\xe8le plus puissant avec un raisonnement plus rapide et \xe9conome en m\xe9moire, id\xe9al pour les flux de travail complexes et les applications exigeantes en p\xe9riph\xe9rie."},"mistral/mistral-embed":{"description":"Mod\xe8le d\'embedding textuel universel pour la recherche s\xe9mantique, la similarit\xe9, le clustering et les workflows RAG."},"mistral/mistral-large":{"description":"Mistral Large est id\xe9al pour les t\xe2ches complexes n\xe9cessitant une grande capacit\xe9 de raisonnement ou une sp\xe9cialisation \xe9lev\xe9e, telles que la g\xe9n\xe9ration de texte synth\xe9tique, le codage, le RAG ou les agents."},"mistral/mistral-small":{"description":"Mistral Small est id\xe9al pour les t\xe2ches simples pouvant \xeatre trait\xe9es en lots, telles que la classification, le support client ou la g\xe9n\xe9ration de texte. Il offre d\'excellentes performances \xe0 un prix abordable."},"mistral/mixtral-8x22b-instruct":{"description":"Mod\xe8le 8x22b Instruct. 8x22b est un mod\xe8le open source \xe0 experts hybrides servi par Mistral."},"mistral/pixtral-12b":{"description":"Un mod\xe8le de 12 milliards de param\xe8tres avec capacit\xe9s de compr\xe9hension d\'image et de texte."},"mistral/pixtral-large":{"description":"Pixtral Large est le deuxi\xe8me mod\xe8le de notre famille multimodale, d\xe9montrant un niveau avanc\xe9 de compr\xe9hension d\'image. En particulier, il peut comprendre documents, graphiques et images naturelles, tout en conservant les capacit\xe9s de compr\xe9hension textuelle de pointe de Mistral Large 2."},"mistralai/Mistral-7B-Instruct-v0.1":{"description":"Mistral (7B) Instruct est r\xe9put\xe9 pour ses performances \xe9lev\xe9es, adapt\xe9 \xe0 diverses t\xe2ches linguistiques."},"mistralai/Mistral-7B-Instruct-v0.2":{"description":"Mistral 7B est un mod\xe8le fine-tun\xe9 \xe0 la demande, offrant des r\xe9ponses optimis\xe9es pour les t\xe2ches."},"mistralai/Mistral-7B-Instruct-v0.3":{"description":"Mistral (7B) Instruct v0.3 offre une capacit\xe9 de calcul efficace et une compr\xe9hension du langage naturel, adapt\xe9 \xe0 un large \xe9ventail d\'applications."},"mistralai/Mistral-7B-v0.1":{"description":"Mistral 7B est un mod\xe8le compact mais performant, excellent pour le traitement par lot et les t\xe2ches simples, comme la classification et la g\xe9n\xe9ration de texte, avec de bonnes capacit\xe9s d\'inf\xe9rence."},"mistralai/Mixtral-8x22B-Instruct-v0.1":{"description":"Mixtral-8x22B Instruct (141B) est un super grand mod\xe8le de langage, prenant en charge des besoins de traitement extr\xeamement \xe9lev\xe9s."},"mistralai/Mixtral-8x7B-Instruct-v0.1":{"description":"Mixtral 8x7B est un mod\xe8le de m\xe9lange d\'experts pr\xe9-entra\xeen\xe9, utilis\xe9 pour des t\xe2ches textuelles g\xe9n\xe9rales."},"mistralai/Mixtral-8x7B-v0.1":{"description":"Mixtral 8x7B est un mod\xe8le d\'experts clairsem\xe9s qui utilise de multiples param\xe8tres pour am\xe9liorer la vitesse d\'inf\xe9rence, adapt\xe9 au traitement des t\xe2ches multilingues et de g\xe9n\xe9ration de code."},"mistralai/mistral-nemo":{"description":"Mistral Nemo est un mod\xe8le de 7,3 milliards de param\xe8tres, offrant un support multilingue et une programmation haute performance."},"mixtral":{"description":"Mixtral est le mod\xe8le d\'expert de Mistral AI, avec des poids open source, offrant un soutien dans la g\xe9n\xe9ration de code et la compr\xe9hension du langage."},"mixtral-8x7b-32768":{"description":"Mixtral 8x7B offre une capacit\xe9 de calcul parall\xe8le \xe0 haute tol\xe9rance aux pannes, adapt\xe9e aux t\xe2ches complexes."},"mixtral:8x22b":{"description":"Mixtral est le mod\xe8le d\'expert de Mistral AI, avec des poids open source, offrant un soutien dans la g\xe9n\xe9ration de code et la compr\xe9hension du langage."},"moonshot-v1-128k":{"description":"Moonshot V1 128K est un mod\xe8le dot\xe9 d\'une capacit\xe9 de traitement de contexte ultra-long, adapt\xe9 \xe0 la g\xe9n\xe9ration de textes tr\xe8s longs, r\xe9pondant aux besoins de t\xe2ches de g\xe9n\xe9ration complexes, capable de traiter jusqu\'\xe0 128 000 tokens, id\xe9al pour la recherche, l\'acad\xe9mie et la g\xe9n\xe9ration de documents volumineux."},"moonshot-v1-128k-vision-preview":{"description":"Le mod\xe8le visuel Kimi (y compris moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, etc.) est capable de comprendre le contenu des images, y compris le texte des images, les couleurs des images et les formes des objets."},"moonshot-v1-32k":{"description":"Moonshot V1 32K offre une capacit\xe9 de traitement de contexte de longueur moyenne, capable de traiter 32 768 tokens, particuli\xe8rement adapt\xe9 \xe0 la g\xe9n\xe9ration de divers documents longs et de dialogues complexes, utilis\xe9 dans la cr\xe9ation de contenu, la g\xe9n\xe9ration de rapports et les syst\xe8mes de dialogue."},"moonshot-v1-32k-vision-preview":{"description":"Le mod\xe8le visuel Kimi (y compris moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, etc.) est capable de comprendre le contenu des images, y compris le texte des images, les couleurs des images et les formes des objets."},"moonshot-v1-8k":{"description":"Moonshot V1 8K est con\xe7u pour des t\xe2ches de g\xe9n\xe9ration de courts textes, avec des performances de traitement efficaces, capable de traiter 8 192 tokens, id\xe9al pour des dialogues courts, des prises de notes et une g\xe9n\xe9ration rapide de contenu."},"moonshot-v1-8k-vision-preview":{"description":"Le mod\xe8le visuel Kimi (y compris moonshot-v1-8k-vision-preview/moonshot-v1-32k-vision-preview/moonshot-v1-128k-vision-preview, etc.) est capable de comprendre le contenu des images, y compris le texte des images, les couleurs des images et les formes des objets."},"moonshot-v1-auto":{"description":"Moonshot V1 Auto peut choisir le mod\xe8le appropri\xe9 en fonction du nombre de tokens utilis\xe9s dans le contexte actuel."},"moonshotai/Kimi-Dev-72B":{"description":"Kimi-Dev-72B est un grand mod\xe8le de code open source, optimis\xe9 par un apprentissage par renforcement \xe0 grande \xe9chelle, capable de g\xe9n\xe9rer des correctifs robustes et directement exploitables en production. Ce mod\xe8le a atteint un nouveau score record de 60,4 % sur SWE-bench Verified, \xe9tablissant un nouveau standard pour les mod\xe8les open source dans les t\xe2ches d\'ing\xe9nierie logicielle automatis\xe9e telles que la correction de bugs et la revue de code."},"moonshotai/Kimi-K2-Instruct-0905":{"description":"Kimi K2-Instruct-0905 est la version la plus r\xe9cente et la plus puissante de Kimi K2. Il s\'agit d\'un mod\xe8le linguistique de pointe \xe0 experts mixtes (MoE), avec un total de 1 000 milliards de param\xe8tres et 32 milliards de param\xe8tres activ\xe9s. Les principales caract\xe9ristiques de ce mod\xe8le incluent : une intelligence de codage d\'agents am\xe9lior\xe9e, d\xe9montrant des performances significatives dans les tests de r\xe9f\xe9rence publics et les t\xe2ches r\xe9elles d\'agents de codage ; une exp\xe9rience de codage frontale am\xe9lior\xe9e, avec des progr\xe8s tant en esth\xe9tique qu\'en praticit\xe9 pour la programmation frontale."},"moonshotai/kimi-k2":{"description":"Kimi K2 est un mod\xe8le de langage \xe0 experts hybrides (MoE) \xe0 grande \xe9chelle d\xe9velopp\xe9 par Moonshot AI, avec un total de 1 000 milliards de param\xe8tres et 32 milliards de param\xe8tres activ\xe9s par passage avant. Il est optimis\xe9 pour les capacit\xe9s d\'agent, incluant l\'utilisation avanc\xe9e d\'outils, le raisonnement et la synth\xe8se de code."},"moonshotai/kimi-k2-0905":{"description":"Le mod\xe8le kimi-k2-0905-preview dispose d\'une longueur de contexte de 256k, offrant une capacit\xe9 renforc\xe9e de codage agentique, une meilleure esth\xe9tique et utilit\xe9 du code front-end, ainsi qu\'une compr\xe9hension contextuelle am\xe9lior\xe9e."},"moonshotai/kimi-k2-instruct-0905":{"description":"Le mod\xe8le kimi-k2-0905-preview dispose d\'une longueur de contexte de 256k, offrant une capacit\xe9 renforc\xe9e de codage agentique, une meilleure esth\xe9tique et utilit\xe9 du code front-end, ainsi qu\'une compr\xe9hension contextuelle am\xe9lior\xe9e."},"morph/morph-v3-fast":{"description":"Morph propose un mod\xe8le IA sp\xe9cialis\xe9 qui applique rapidement les modifications de code sugg\xe9r\xe9es par des mod\xe8les de pointe (comme Claude ou GPT-4o) \xe0 vos fichiers de code existants - RAPIDE - plus de 4500 tokens/seconde. Il agit comme la derni\xe8re \xe9tape dans le flux de travail de codage IA. Supporte 16k tokens d\'entr\xe9e et 16k tokens de sortie."},"morph/morph-v3-large":{"description":"Morph propose un mod\xe8le IA sp\xe9cialis\xe9 qui applique les modifications de code sugg\xe9r\xe9es par des mod\xe8les de pointe (comme Claude ou GPT-4o) \xe0 vos fichiers de code existants - RAPIDE - plus de 2500 tokens/seconde. Il agit comme la derni\xe8re \xe9tape dans le flux de travail de codage IA. Supporte 16k tokens d\'entr\xe9e et 16k tokens de sortie."},"nousresearch/hermes-2-pro-llama-3-8b":{"description":"Hermes 2 Pro Llama 3 8B est une version am\xe9lior\xe9e de Nous Hermes 2, int\xe9grant les derniers ensembles de donn\xe9es d\xe9velopp\xe9s en interne."},"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF":{"description":"Llama 3.1 Nemotron 70B est un mod\xe8le de langage \xe0 grande \xe9chelle personnalis\xe9 par NVIDIA, con\xe7u pour am\xe9liorer l\'aide fournie par les r\xe9ponses g\xe9n\xe9r\xe9es par LLM aux requ\xeates des utilisateurs. Ce mod\xe8le a excell\xe9 dans des tests de r\xe9f\xe9rence tels que Arena Hard, AlpacaEval 2 LC et GPT-4-Turbo MT-Bench, se classant premier dans les trois tests d\'alignement automatique au 1er octobre 2024. Le mod\xe8le utilise RLHF (en particulier REINFORCE), Llama-3.1-Nemotron-70B-Reward et HelpSteer2-Preference pour l\'entra\xeenement sur la base du mod\xe8le Llama-3.1-70B-Instruct."},"nvidia/llama-3.1-nemotron-51b-instruct":{"description":"Mod\xe8le de langage unique, offrant une pr\xe9cision et une efficacit\xe9 in\xe9gal\xe9es."},"nvidia/llama-3.1-nemotron-70b-instruct":{"description":"Llama-3.1-Nemotron-70B-Instruct est un mod\xe8le de langage de grande taille personnalis\xe9 par NVIDIA, con\xe7u pour am\xe9liorer l\'utilit\xe9 des r\xe9ponses g\xe9n\xe9r\xe9es par LLM."},"o1":{"description":"Ax\xe9 sur le raisonnement avanc\xe9 et la r\xe9solution de probl\xe8mes complexes, y compris les t\xe2ches math\xe9matiques et scientifiques. Id\xe9al pour les applications n\xe9cessitant une compr\xe9hension approfondie du contexte et des flux de travail d\'agent."},"o1-mini":{"description":"o1-mini est un mod\xe8le de raisonnement rapide et \xe9conomique con\xe7u pour les applications de programmation, de math\xe9matiques et de sciences. Ce mod\xe8le dispose d\'un contexte de 128K et d\'une date limite de connaissance en octobre 2023."},"o1-preview":{"description":"Ax\xe9 sur le raisonnement avanc\xe9 et la r\xe9solution de probl\xe8mes complexes, y compris des t\xe2ches math\xe9matiques et scientifiques. Particuli\xe8rement adapt\xe9 aux applications n\xe9cessitant une compr\xe9hension approfondie du contexte et des flux de travail autonomes."},"o1-pro":{"description":"La s\xe9rie de mod\xe8les o1 est entra\xeen\xe9e par apprentissage par renforcement, capable de r\xe9fl\xe9chir avant de r\xe9pondre et d\'ex\xe9cuter des t\xe2ches de raisonnement complexes. Le mod\xe8le o1-pro utilise plus de ressources de calcul pour une r\xe9flexion plus approfondie, fournissant ainsi des r\xe9ponses de qualit\xe9 sup\xe9rieure de mani\xe8re continue."},"o3":{"description":"o3 est un mod\xe8le polyvalent et puissant, performant dans de nombreux domaines. Il \xe9tablit de nouvelles normes pour les t\xe2ches de math\xe9matiques, de sciences, de programmation et de raisonnement visuel. Il excelle \xe9galement dans la r\xe9daction technique et le respect des instructions. Les utilisateurs peuvent l\'utiliser pour analyser des textes, du code et des images, et r\xe9soudre des probl\xe8mes complexes en plusieurs \xe9tapes."},"o3-2025-04-16":{"description":"o3 est le nouveau mod\xe8le de raisonnement d\'OpenAI, supportant les entr\xe9es texte et image et produisant du texte, adapt\xe9 aux t\xe2ches complexes n\xe9cessitant des connaissances g\xe9n\xe9rales \xe9tendues."},"o3-deep-research":{"description":"o3-deep-research est notre mod\xe8le de recherche approfondie le plus avanc\xe9, con\xe7u sp\xe9cialement pour g\xe9rer des t\xe2ches de recherche complexes en plusieurs \xe9tapes. Il peut rechercher et synth\xe9tiser des informations sur Internet, ainsi qu\'acc\xe9der et exploiter vos propres donn\xe9es via le connecteur MCP."},"o3-mini":{"description":"o3-mini est notre dernier mod\xe8le d\'inf\xe9rence compact, offrant une grande intelligence avec les m\xeames objectifs de co\xfbt et de latence que o1-mini."},"o3-pro":{"description":"Le mod\xe8le o3-pro utilise davantage de calculs pour r\xe9fl\xe9chir plus profond\xe9ment et fournir constamment de meilleures r\xe9ponses, uniquement disponible via l\'API Responses."},"o3-pro-2025-06-10":{"description":"o3 Pro est le nouveau mod\xe8le de raisonnement d\'OpenAI, supportant les entr\xe9es texte et image et produisant du texte, adapt\xe9 aux t\xe2ches complexes n\xe9cessitant des connaissances g\xe9n\xe9rales \xe9tendues."},"o4-mini":{"description":"o4-mini est notre dernier mod\xe8le de la s\xe9rie o de petite taille. Il est optimis\xe9 pour une inf\xe9rence rapide et efficace, offrant une grande efficacit\xe9 et performance dans les t\xe2ches de codage et visuelles."},"o4-mini-2025-04-16":{"description":"o4-mini est un mod\xe8le de raisonnement d\'OpenAI, supportant les entr\xe9es texte et image et produisant du texte, adapt\xe9 aux t\xe2ches complexes n\xe9cessitant des connaissances g\xe9n\xe9rales \xe9tendues. Ce mod\xe8le dispose d\'un contexte de 200 000 tokens."},"o4-mini-deep-research":{"description":"o4-mini-deep-research est notre mod\xe8le de recherche approfondie plus rapide et plus abordable — id\xe9al pour traiter des t\xe2ches de recherche complexes en plusieurs \xe9tapes. Il peut rechercher et synth\xe9tiser des informations sur Internet, ainsi qu\'acc\xe9der et exploiter vos propres donn\xe9es via le connecteur MCP."},"open-codestral-mamba":{"description":"Codestral Mamba est un mod\xe8le de langage Mamba 2 ax\xe9 sur la g\xe9n\xe9ration de code, offrant un soutien puissant pour des t\xe2ches avanc\xe9es de codage et de raisonnement."},"open-mistral-7b":{"description":"Mistral 7B est un mod\xe8le compact mais performant, excellent pour le traitement par lots et les t\xe2ches simples, telles que la classification et la g\xe9n\xe9ration de texte, avec de bonnes capacit\xe9s de raisonnement."},"open-mistral-nemo":{"description":"Mistral Nemo est un mod\xe8le de 12B d\xe9velopp\xe9 en collaboration avec Nvidia, offrant d\'excellentes performances de raisonnement et de codage, facile \xe0 int\xe9grer et \xe0 remplacer."},"open-mixtral-8x22b":{"description":"Mixtral 8x22B est un mod\xe8le d\'expert plus grand, ax\xe9 sur des t\xe2ches complexes, offrant d\'excellentes capacit\xe9s de raisonnement et un d\xe9bit plus \xe9lev\xe9."},"open-mixtral-8x7b":{"description":"Mixtral 8x7B est un mod\xe8le d\'expert \xe9pars, utilisant plusieurs param\xe8tres pour am\xe9liorer la vitesse de raisonnement, adapt\xe9 au traitement de t\xe2ches multilingues et de g\xe9n\xe9ration de code."},"openai/gpt-3.5-turbo":{"description":"Le mod\xe8le le plus performant et rentable de la s\xe9rie GPT-3.5 d\'OpenAI, optimis\xe9 pour le chat mais \xe9galement performant pour les t\xe2ches de compl\xe9tion traditionnelles."},"openai/gpt-3.5-turbo-instruct":{"description":"Capacit\xe9s similaires aux mod\xe8les de l\'\xe8re GPT-3. Compatible avec les points de terminaison de compl\xe9tion traditionnels, pas les compl\xe9tions de chat."},"openai/gpt-4-turbo":{"description":"gpt-4-turbo d\'OpenAI poss\xe8de une vaste connaissance g\xe9n\xe9rale et une expertise sectorielle, capable de suivre des instructions complexes en langage naturel et de r\xe9soudre pr\xe9cis\xe9ment des probl\xe8mes difficiles. Sa date de coupure des connaissances est avril 2023, avec une fen\xeatre de contexte de 128 000 tokens."},"openai/gpt-4.1":{"description":"GPT 4.1 est le mod\xe8le phare d\'OpenAI, adapt\xe9 aux t\xe2ches complexes. Il excelle dans la r\xe9solution de probl\xe8mes interdomaines."},"openai/gpt-4.1-mini":{"description":"GPT 4.1 mini \xe9quilibre intelligence, vitesse et co\xfbt, en faisant un mod\xe8le attractif pour de nombreux cas d\'usage."},"openai/gpt-4.1-nano":{"description":"GPT-4.1 nano est le mod\xe8le GPT 4.1 le plus rapide et le plus rentable."},"openai/gpt-4o":{"description":"GPT-4o d\'OpenAI poss\xe8de une vaste connaissance g\xe9n\xe9rale et une expertise sectorielle, capable de suivre des instructions complexes en langage naturel et de r\xe9soudre pr\xe9cis\xe9ment des probl\xe8mes difficiles. Il offre des performances \xe9quivalentes \xe0 GPT-4 Turbo avec une API plus rapide et moins co\xfbteuse."},"openai/gpt-4o-mini":{"description":"GPT-4o mini d\'OpenAI est leur mod\xe8le petit, avanc\xe9 et rentable. Il est multimodal (accepte texte ou image en entr\xe9e et produit du texte), plus intelligent que gpt-3.5-turbo tout en \xe9tant aussi rapide."},"openai/gpt-5":{"description":"GPT-5 est le mod\xe8le de langage phare d\'OpenAI, excellent en raisonnement complexe, vaste connaissance du monde r\xe9el, t\xe2ches intensives en code et agents multi-\xe9tapes."},"openai/gpt-5-mini":{"description":"GPT-5 mini est un mod\xe8le optimis\xe9 pour le co\xfbt, performant en raisonnement et t\xe2ches de chat. Il offre un \xe9quilibre optimal entre vitesse, co\xfbt et capacit\xe9."},"openai/gpt-5-nano":{"description":"GPT-5 nano est un mod\xe8le \xe0 haut d\xe9bit, performant pour les instructions simples ou les t\xe2ches de classification."},"openai/gpt-oss-120b":{"description":"Mod\xe8le de langage g\xe9n\xe9ral extr\xeamement performant avec des capacit\xe9s de raisonnement puissantes et contr\xf4lables."},"openai/gpt-oss-20b":{"description":"Mod\xe8le de langage compact avec poids open source, optimis\xe9 pour la faible latence et les environnements \xe0 ressources limit\xe9es, incluant le d\xe9ploiement local et en p\xe9riph\xe9rie."},"openai/o1":{"description":"o1 d\'OpenAI est un mod\xe8le de raisonnement phare, con\xe7u pour les probl\xe8mes complexes n\xe9cessitant une r\xe9flexion approfondie. Il offre un raisonnement puissant et une pr\xe9cision accrue pour les t\xe2ches complexes \xe0 plusieurs \xe9tapes."},"openai/o1-mini":{"description":"o1-mini est un mod\xe8le de raisonnement rapide et \xe9conomique con\xe7u pour les applications de programmation, de math\xe9matiques et de sciences. Ce mod\xe8le dispose d\'un contexte de 128K et d\'une date limite de connaissance en octobre 2023."},"openai/o1-preview":{"description":"o1 est le nouveau mod\xe8le de raisonnement d\'OpenAI, adapt\xe9 aux t\xe2ches complexes n\xe9cessitant une vaste connaissance g\xe9n\xe9rale. Ce mod\xe8le dispose d\'un contexte de 128K et d\'une date limite de connaissance en octobre 2023."},"openai/o3":{"description":"o3 d\'OpenAI est le mod\xe8le de raisonnement le plus puissant, \xe9tablissant de nouveaux standards en codage, math\xe9matiques, sciences et perception visuelle. Il excelle dans les requ\xeates complexes n\xe9cessitant une analyse multidimensionnelle, avec un avantage particulier pour l\'analyse d\'images, graphiques et diagrammes."},"openai/o3-mini":{"description":"o3-mini est le dernier petit mod\xe8le de raisonnement d\'OpenAI, offrant une intelligence \xe9lev\xe9e avec les m\xeames objectifs de co\xfbt et latence que o1-mini."},"openai/o3-mini-high":{"description":"o3-mini haute version de raisonnement, offrant une grande intelligence avec les m\xeames objectifs de co\xfbt et de latence que o1-mini."},"openai/o4-mini":{"description":"o4-mini d\'OpenAI offre un raisonnement rapide et rentable, avec des performances exceptionnelles pour sa taille, notamment en math\xe9matiques (meilleur sur le benchmark AIME), codage et t\xe2ches visuelles."},"openai/o4-mini-high":{"description":"Version \xe0 haut niveau d\'inf\xe9rence d\'o4-mini, optimis\xe9e pour une inf\xe9rence rapide et efficace, offrant une grande efficacit\xe9 et performance dans les t\xe2ches de codage et visuelles."},"openai/text-embedding-3-large":{"description":"Le mod\xe8le d\'embedding le plus performant d\'OpenAI, adapt\xe9 aux t\xe2ches en anglais et non anglaises."},"openai/text-embedding-3-small":{"description":"Version am\xe9lior\xe9e et plus performante du mod\xe8le d\'embedding ada d\'OpenAI."},"openai/text-embedding-ada-002":{"description":"Mod\xe8le d\'embedding textuel traditionnel d\'OpenAI."},"openrouter/auto":{"description":"En fonction de la longueur du contexte, du sujet et de la complexit\xe9, votre demande sera envoy\xe9e \xe0 Llama 3 70B Instruct, Claude 3.5 Sonnet (auto-r\xe9gul\xe9) ou GPT-4o."},"perplexity/sonar":{"description":"Produit l\xe9ger de Perplexity avec capacit\xe9 de recherche ancr\xe9e, plus rapide et moins cher que Sonar Pro."},"perplexity/sonar-pro":{"description":"Produit phare de Perplexity avec capacit\xe9 de recherche ancr\xe9e, supportant des requ\xeates avanc\xe9es et des suivis."},"perplexity/sonar-reasoning":{"description":"Mod\xe8le ax\xe9 sur le raisonnement, produisant des cha\xeenes de pens\xe9e (CoT) dans les r\xe9ponses, offrant des explications d\xe9taill\xe9es avec recherche ancr\xe9e."},"perplexity/sonar-reasoning-pro":{"description":"Mod\xe8le avanc\xe9 ax\xe9 sur le raisonnement, produisant des cha\xeenes de pens\xe9e (CoT) dans les r\xe9ponses, offrant des explications compl\xe8tes avec capacit\xe9s de recherche am\xe9lior\xe9es et multiples requ\xeates de recherche par demande."},"phi3":{"description":"Phi-3 est un mod\xe8le ouvert l\xe9ger lanc\xe9 par Microsoft, adapt\xe9 \xe0 une int\xe9gration efficace et \xe0 un raisonnement de connaissances \xe0 grande \xe9chelle."},"phi3:14b":{"description":"Phi-3 est un mod\xe8le ouvert l\xe9ger lanc\xe9 par Microsoft, adapt\xe9 \xe0 une int\xe9gration efficace et \xe0 un raisonnement de connaissances \xe0 grande \xe9chelle."},"pixtral-12b-2409":{"description":"Le mod\xe8le Pixtral montre de puissantes capacit\xe9s dans des t\xe2ches telles que la compr\xe9hension des graphiques et des images, le questionnement de documents, le raisonnement multimodal et le respect des instructions, capable d\'ing\xe9rer des images \xe0 r\xe9solution naturelle et \xe0 rapport d\'aspect, tout en traitant un nombre quelconque d\'images dans une fen\xeatre de contexte longue allant jusqu\'\xe0 128K tokens."},"pixtral-large-latest":{"description":"Pixtral Large est un mod\xe8le multimodal open source avec 124 milliards de param\xe8tres, bas\xe9 sur Mistral Large 2. C\'est notre deuxi\xe8me mod\xe8le de la famille multimodale, d\xe9montrant des capacit\xe9s de compr\xe9hension d\'image \xe0 la pointe de la technologie."},"pro-128k":{"description":"Spark Pro 128K est dot\xe9 d\'une capacit\xe9 de traitement de contexte tr\xe8s \xe9tendue, capable de g\xe9rer jusqu\'\xe0 128K d\'informations contextuelles, particuli\xe8rement adapt\xe9 pour l\'analyse compl\xe8te et le traitement des relations logiques \xe0 long terme dans des contenus longs, offrant une logique fluide et coh\xe9rente ainsi qu\'un soutien vari\xe9 pour les r\xe9f\xe9rences dans des communications textuelles complexes."},"pro-deepseek-r1":{"description":"Mod\xe8le d\xe9di\xe9 aux services d\'entreprise, incluant les services en parall\xe8le."},"pro-deepseek-v3":{"description":"Mod\xe8le d\xe9di\xe9 aux services d\'entreprise, incluant les services en parall\xe8le."},"qianfan-70b":{"description":"Qianfan 70B, un mod\xe8le chinois de grande taille, id\xe9al pour la g\xe9n\xe9ration de contenu de haute qualit\xe9 et les t\xe2ches de raisonnement complexes."},"qianfan-8b":{"description":"Qianfan 8B, un mod\xe8le g\xe9n\xe9raliste de taille moyenne, adapt\xe9 \xe0 la g\xe9n\xe9ration de texte et aux sc\xe9narios de questions-r\xe9ponses avec un bon \xe9quilibre entre co\xfbt et performance."},"qianfan-agent-intent-32k":{"description":"Qianfan Agent Intent 32K, un mod\xe8le con\xe7u pour la reconnaissance d’intention et l’orchestration d’agents, prenant en charge des contextes longs."},"qianfan-agent-lite-8k":{"description":"Qianfan Agent Lite 8K, un mod\xe8le d’agent l\xe9ger, adapt\xe9 aux dialogues multi-tours \xe0 faible co\xfbt et \xe0 l’orchestration de processus m\xe9tier."},"qianfan-agent-speed-32k":{"description":"Qianfan Agent Speed 32K, un mod\xe8le d’agent \xe0 haut d\xe9bit, con\xe7u pour des applications d’agents \xe0 grande \xe9chelle et multi-t\xe2ches."},"qianfan-agent-speed-8k":{"description":"Qianfan Agent Speed 8K, un mod\xe8le d’agent \xe0 haute concurrence, optimis\xe9 pour les dialogues courts et les r\xe9ponses rapides."},"qianfan-check-vl":{"description":"Qianfan Check VL, un mod\xe8le de v\xe9rification multimodale, prenant en charge la conformit\xe9 et la d\xe9tection dans les contenus image-texte."},"qianfan-composition":{"description":"Qianfan Composition, un mod\xe8le de cr\xe9ation multimodale, capable de comprendre et de g\xe9n\xe9rer des contenus combinant texte et image."},"qianfan-engcard-vl":{"description":"Qianfan EngCard VL, un mod\xe8le de reconnaissance multimodale sp\xe9cialis\xe9 pour les sc\xe9narios en anglais."},"qianfan-lightning-128b-a19b":{"description":"Qianfan Lightning 128B A19B, un mod\xe8le chinois haute performance, adapt\xe9 aux questions complexes et au raisonnement \xe0 grande \xe9chelle."},"qianfan-llama-vl-8b":{"description":"Qianfan Llama VL 8B, un mod\xe8le multimodal bas\xe9 sur Llama, con\xe7u pour les t\xe2ches g\xe9n\xe9rales de compr\xe9hension image-texte."},"qianfan-multipicocr":{"description":"Qianfan MultiPicOCR, un mod\xe8le OCR multi-images, capable de d\xe9tecter et reconna\xeetre du texte sur plusieurs images."},"qianfan-qi-vl":{"description":"Qianfan QI VL, un mod\xe8le de questions-r\xe9ponses multimodal, optimis\xe9 pour la recherche pr\xe9cise et les r\xe9ponses dans des contextes image-texte complexes."},"qianfan-singlepicocr":{"description":"Qianfan SinglePicOCR, un mod\xe8le OCR pour image unique, offrant une reconnaissance de caract\xe8res haute pr\xe9cision."},"qianfan-vl-70b":{"description":"Qianfan VL 70B, un mod\xe8le visuel-langagier de grande taille, adapt\xe9 aux sc\xe9narios complexes de compr\xe9hension image-texte."},"qianfan-vl-8b":{"description":"Qianfan VL 8B, un mod\xe8le visuel-langagier l\xe9ger, id\xe9al pour les questions-r\xe9ponses et analyses image-texte du quotidien."},"qvq-72b-preview":{"description":"Le mod\xe8le QVQ est un mod\xe8le de recherche exp\xe9rimental d\xe9velopp\xe9 par l\'\xe9quipe Qwen, ax\xe9 sur l\'am\xe9lioration des capacit\xe9s de raisonnement visuel, en particulier dans le domaine du raisonnement math\xe9matique."},"qvq-max":{"description":"Mod\xe8le de raisonnement visuel QVQ de Tongyi Qianwen, supportant l’entr\xe9e visuelle et la sortie en cha\xeene de pens\xe9e, d\xe9montrant des capacit\xe9s renforc\xe9es en math\xe9matiques, programmation, analyse visuelle, cr\xe9ation et t\xe2ches g\xe9n\xe9rales."},"qvq-plus":{"description":"Mod\xe8le de raisonnement visuel. Prend en charge les entr\xe9es visuelles et les sorties en cha\xeene de pens\xe9e. Version plus avanc\xe9e du mod\xe8le qvq-max, offrant une vitesse de raisonnement plus rapide et un meilleur \xe9quilibre entre performance et co\xfbt."},"qwen-3-32b":{"description":"Qwen 3 32B : un mod\xe8le de la s\xe9rie Qwen performant dans les t\xe2ches multilingues et de codage, adapt\xe9 \xe0 une utilisation en production \xe0 \xe9chelle moyenne."},"qwen-3-coder-480b":{"description":"Qwen 3 Coder 480B : un mod\xe8le \xe0 long contexte con\xe7u pour la g\xe9n\xe9ration de code et les t\xe2ches de programmation complexes."},"qwen-coder-plus":{"description":"Mod\xe8le de code Tongyi Qianwen."},"qwen-coder-turbo":{"description":"Mod\xe8le de code Tongyi Qianwen."},"qwen-coder-turbo-latest":{"description":"Le mod\xe8le de code Tongyi Qwen."},"qwen-flash":{"description":"La s\xe9rie Tongyi Qianwen propose les mod\xe8les les plus rapides et les plus \xe9conomiques, adapt\xe9s aux t\xe2ches simples."},"qwen-image":{"description":"Qwen-Image est un mod\xe8le polyvalent de g\xe9n\xe9ration d\'images, prenant en charge de nombreux styles artistiques et excelling particuli\xe8rement dans le rendu de textes complexes, notamment en chinois et en anglais. Le mod\xe8le g\xe8re les mises en page multi‑lignes, la g\xe9n\xe9ration de texte au niveau des paragraphes et le rendu de d\xe9tails fins, permettant de cr\xe9er des compositions complexes m\xealant texte et image."},"qwen-image-edit":{"description":"Qwen Image Edit est un mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir d\'images, permettant l\'\xe9dition et la modification d\'images bas\xe9es sur une image d\'entr\xe9e et des indications textuelles. Il peut ajuster pr\xe9cis\xe9ment et transformer de mani\xe8re cr\xe9ative l\'image originale selon les besoins de l\'utilisateur."},"qwen-long":{"description":"Qwen est un mod\xe8le de langage \xe0 grande \xe9chelle, prenant en charge un contexte de texte long, ainsi que des fonctionnalit\xe9s de dialogue bas\xe9es sur des documents longs et multiples."},"qwen-math-plus":{"description":"Mod\xe8le math\xe9matique Tongyi Qianwen sp\xe9cialement con\xe7u pour la r\xe9solution de probl\xe8mes math\xe9matiques."},"qwen-math-plus-latest":{"description":"Le mod\xe8le de langage Tongyi Qwen pour les math\xe9matiques, sp\xe9cialement con\xe7u pour r\xe9soudre des probl\xe8mes math\xe9matiques."},"qwen-math-turbo":{"description":"Mod\xe8le math\xe9matique Tongyi Qianwen sp\xe9cialement con\xe7u pour la r\xe9solution de probl\xe8mes math\xe9matiques."},"qwen-math-turbo-latest":{"description":"Le mod\xe8le de langage Tongyi Qwen pour les math\xe9matiques, sp\xe9cialement con\xe7u pour r\xe9soudre des probl\xe8mes math\xe9matiques."},"qwen-max":{"description":"Mod\xe8le de langage \xe0 grande \xe9chelle de niveau milliard Qwen, prenant en charge des entr\xe9es dans diff\xe9rentes langues telles que le chinois et l\'anglais, repr\xe9sentant actuellement le mod\xe8le API derri\xe8re la version 2.5 de Qwen."},"qwen-omni-turbo":{"description":"La s\xe9rie Qwen-Omni supporte l’entr\xe9e de donn\xe9es multimodales vari\xe9es, incluant vid\xe9o, audio, images et texte, et produit en sortie de l’audio et du texte."},"qwen-plus":{"description":"Version am\xe9lior\xe9e du mod\xe8le de langage \xe0 grande \xe9chelle Qwen, prenant en charge des entr\xe9es dans diff\xe9rentes langues telles que le chinois et l\'anglais."},"qwen-turbo":{"description":"Le mod\xe8le 通义千问 Turbo ne sera plus mis \xe0 jour ; il est recommand\xe9 de le remplacer par 通义千问 Flash. 通义千问 est un mod\xe8le de langage \xe0 tr\xe8s grande \xe9chelle, prenant en charge des entr\xe9es en chinois, en anglais et dans d\'autres langues."},"qwen-vl-chat-v1":{"description":"Qwen VL prend en charge des modes d\'interaction flexibles, y compris la capacit\xe9 de poser des questions \xe0 plusieurs images, des dialogues multi-tours, et plus encore."},"qwen-vl-max":{"description":"Mod\xe8le visuel-langage Tongyi Qianwen de tr\xe8s grande \xe9chelle. Par rapport \xe0 la version am\xe9lior\xe9e, il renforce encore les capacit\xe9s de raisonnement visuel et de conformit\xe9 aux instructions, offrant un niveau sup\xe9rieur de perception et de cognition visuelle."},"qwen-vl-max-latest":{"description":"Mod\xe8le de langage visuel \xe0 tr\xe8s grande \xe9chelle Tongyi Qianwen. Par rapport \xe0 la version am\xe9lior\xe9e, il am\xe9liore encore les capacit\xe9s de raisonnement visuel et de suivi des instructions, offrant un niveau de perception visuelle et de cognition plus \xe9lev\xe9."},"qwen-vl-ocr":{"description":"Tongyi Qianwen OCR est un mod\xe8le sp\xe9cialis\xe9 dans l’extraction de texte, focalis\xe9 sur les images de documents, tableaux, questions d’examen, \xe9criture manuscrite, etc. Il peut reconna\xeetre plusieurs langues, notamment : chinois, anglais, fran\xe7ais, japonais, cor\xe9en, allemand, russe, italien, vietnamien et arabe."},"qwen-vl-plus":{"description":"Version am\xe9lior\xe9e du grand mod\xe8le visuel-langage Tongyi Qianwen. Am\xe9lioration significative des capacit\xe9s de reconnaissance des d\xe9tails et de reconnaissance optique de caract\xe8res, supportant des images \xe0 r\xe9solution sup\xe9rieure \xe0 un million de pixels et des formats d’image de proportions arbitraires."},"qwen-vl-plus-latest":{"description":"Version am\xe9lior\xe9e du mod\xe8le de langage visuel \xe0 grande \xe9chelle Tongyi Qianwen. Am\xe9lioration significative des capacit\xe9s de reconnaissance des d\xe9tails et de reconnaissance de texte, prenant en charge des r\xe9solutions d\'image de plus d\'un million de pixels et des rapports d\'aspect de n\'importe quelle taille."},"qwen-vl-v1":{"description":"Initialis\xe9 avec le mod\xe8le de langage Qwen-7B, ajoutant un mod\xe8le d\'image, un mod\xe8le pr\xe9-entra\xeen\xe9 avec une r\xe9solution d\'entr\xe9e d\'image de 448."},"qwen/qwen-2-7b-instruct":{"description":"Qwen2 est la toute nouvelle s\xe9rie de mod\xe8les de langage de grande taille Qwen. Qwen2 7B est un mod\xe8le bas\xe9 sur le transformateur, qui excelle dans la compr\xe9hension du langage, les capacit\xe9s multilingues, la programmation, les math\xe9matiques et le raisonnement."},"qwen/qwen-2-7b-instruct:free":{"description":"Qwen2 est une toute nouvelle s\xe9rie de mod\xe8les de langage de grande taille, offrant des capacit\xe9s de compr\xe9hension et de g\xe9n\xe9ration plus puissantes."},"qwen/qwen-2-vl-72b-instruct":{"description":"Qwen2-VL est la derni\xe8re version it\xe9r\xe9e du mod\xe8le Qwen-VL, atteignant des performances de pointe dans les benchmarks de compr\xe9hension visuelle, y compris MathVista, DocVQA, RealWorldQA et MTVQA. Qwen2-VL peut comprendre des vid\xe9os de plus de 20 minutes pour des questions-r\xe9ponses, des dialogues et de la cr\xe9ation de contenu de haute qualit\xe9 bas\xe9s sur la vid\xe9o. Il poss\xe8de \xe9galement des capacit\xe9s de raisonnement et de d\xe9cision complexes, pouvant \xeatre int\xe9gr\xe9 \xe0 des appareils mobiles, des robots, etc., pour des op\xe9rations automatiques bas\xe9es sur l\'environnement visuel et des instructions textuelles. En plus de l\'anglais et du chinois, Qwen2-VL prend d\xe9sormais en charge la compr\xe9hension du texte dans diff\xe9rentes langues dans les images, y compris la plupart des langues europ\xe9ennes, le japonais, le cor\xe9en, l\'arabe et le vietnamien."},"qwen/qwen-2.5-72b-instruct":{"description":"Qwen2.5-72B-Instruct est l\'un des derniers mod\xe8les de langage de grande taille publi\xe9s par Alibaba Cloud. Ce mod\xe8le de 72B pr\xe9sente des capacit\xe9s significativement am\xe9lior\xe9es dans des domaines tels que le codage et les math\xe9matiques. Le mod\xe8le offre \xe9galement un support multilingue, couvrant plus de 29 langues, y compris le chinois et l\'anglais. Il a montr\xe9 des am\xe9liorations significatives dans le suivi des instructions, la compr\xe9hension des donn\xe9es structur\xe9es et la g\xe9n\xe9ration de sorties structur\xe9es (en particulier JSON)."},"qwen/qwen2.5-32b-instruct":{"description":"Qwen2.5-32B-Instruct est l\'un des derniers mod\xe8les de langage de grande taille publi\xe9s par Alibaba Cloud. Ce mod\xe8le de 32B pr\xe9sente des capacit\xe9s significativement am\xe9lior\xe9es dans des domaines tels que le codage et les math\xe9matiques. Le mod\xe8le offre un support multilingue, couvrant plus de 29 langues, y compris le chinois et l\'anglais. Il a montr\xe9 des am\xe9liorations significatives dans le suivi des instructions, la compr\xe9hension des donn\xe9es structur\xe9es et la g\xe9n\xe9ration de sorties structur\xe9es (en particulier JSON)."},"qwen/qwen2.5-7b-instruct":{"description":"LLM orient\xe9 vers le chinois et l\'anglais, ciblant des domaines tels que la langue, la programmation, les math\xe9matiques et le raisonnement."},"qwen/qwen2.5-coder-32b-instruct":{"description":"LLM avanc\xe9, prenant en charge la g\xe9n\xe9ration de code, le raisonnement et la correction, couvrant les langages de programmation courants."},"qwen/qwen2.5-coder-7b-instruct":{"description":"Mod\xe8le de code puissant de taille moyenne, prenant en charge une longueur de contexte de 32K, sp\xe9cialis\xe9 dans la programmation multilingue."},"qwen/qwen3-14b":{"description":"Qwen3-14B est un mod\xe8le de langage causal dense de 14 milliards de param\xe8tres dans la s\xe9rie Qwen3, con\xe7u pour un raisonnement complexe et des dialogues efficaces. Il permet un passage sans effort entre un mode de pens\xe9e pour des t\xe2ches telles que les math\xe9matiques, la programmation et le raisonnement logique, et un mode non-pensant pour des dialogues g\xe9n\xe9raux. Ce mod\xe8le a \xe9t\xe9 affin\xe9 pour le suivi des instructions, l\'utilisation d\'outils d\'agents, l\'\xe9criture cr\xe9ative et des t\xe2ches multilingues dans plus de 100 langues et dialectes. Il g\xe8re nativement un contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via une extension bas\xe9e sur YaRN."},"qwen/qwen3-14b:free":{"description":"Qwen3-14B est un mod\xe8le de langage causal dense de 14 milliards de param\xe8tres dans la s\xe9rie Qwen3, con\xe7u pour un raisonnement complexe et des dialogues efficaces. Il permet un passage sans effort entre un mode de pens\xe9e pour des t\xe2ches telles que les math\xe9matiques, la programmation et le raisonnement logique, et un mode non-pensant pour des dialogues g\xe9n\xe9raux. Ce mod\xe8le a \xe9t\xe9 affin\xe9 pour le suivi des instructions, l\'utilisation d\'outils d\'agents, l\'\xe9criture cr\xe9ative et des t\xe2ches multilingues dans plus de 100 langues et dialectes. Il g\xe8re nativement un contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via une extension bas\xe9e sur YaRN."},"qwen/qwen3-235b-a22b":{"description":"Qwen3-235B-A22B est un mod\xe8le de m\xe9lange d\'experts (MoE) de 235 milliards de param\xe8tres d\xe9velopp\xe9 par Qwen, activant 22 milliards de param\xe8tres \xe0 chaque passage avant. Il permet un passage sans effort entre un mode de pens\xe9e pour des t\xe2ches complexes de raisonnement, de math\xe9matiques et de code, et un mode non-pensant pour une efficacit\xe9 dans les dialogues g\xe9n\xe9raux. Ce mod\xe8le d\xe9montre de solides capacit\xe9s de raisonnement, un support multilingue (plus de 100 langues et dialectes), un suivi avanc\xe9 des instructions et des capacit\xe9s d\'appel d\'outils d\'agents. Il g\xe8re nativement une fen\xeatre de contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via une extension bas\xe9e sur YaRN."},"qwen/qwen3-235b-a22b:free":{"description":"Qwen3-235B-A22B est un mod\xe8le de m\xe9lange d\'experts (MoE) de 235 milliards de param\xe8tres d\xe9velopp\xe9 par Qwen, activant 22 milliards de param\xe8tres \xe0 chaque passage avant. Il permet un passage sans effort entre un mode de pens\xe9e pour des t\xe2ches complexes de raisonnement, de math\xe9matiques et de code, et un mode non-pensant pour une efficacit\xe9 dans les dialogues g\xe9n\xe9raux. Ce mod\xe8le d\xe9montre de solides capacit\xe9s de raisonnement, un support multilingue (plus de 100 langues et dialectes), un suivi avanc\xe9 des instructions et des capacit\xe9s d\'appel d\'outils d\'agents. Il g\xe8re nativement une fen\xeatre de contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via une extension bas\xe9e sur YaRN."},"qwen/qwen3-30b-a3b":{"description":"Qwen3 est la derni\xe8re g\xe9n\xe9ration de la s\xe9rie de mod\xe8les de langage Qwen, dot\xe9e d\'une architecture dense et de m\xe9lange d\'experts (MoE), offrant d\'excellentes performances en mati\xe8re de raisonnement, de support multilingue et de t\xe2ches avanc\xe9es d\'agent. Sa capacit\xe9 unique \xe0 passer sans effort entre un mode de pens\xe9e pour le raisonnement complexe et un mode non-pensant pour des dialogues efficaces garantit des performances polyvalentes et de haute qualit\xe9.\\n\\nQwen3 surpasse de mani\xe8re significative les mod\xe8les pr\xe9c\xe9dents tels que QwQ et Qwen2.5, offrant des capacit\xe9s exceptionnelles en math\xe9matiques, en codage, en raisonnement de bon sens, en \xe9criture cr\xe9ative et en dialogue interactif. La variante Qwen3-30B-A3B contient 30,5 milliards de param\xe8tres (3,3 milliards de param\xe8tres activ\xe9s), 48 couches, 128 experts (8 activ\xe9s par t\xe2che) et prend en charge un contexte allant jusqu\'\xe0 131K tokens (utilisant YaRN), \xe9tablissant une nouvelle norme pour les mod\xe8les open source."},"qwen/qwen3-30b-a3b:free":{"description":"Qwen3 est la derni\xe8re g\xe9n\xe9ration de la s\xe9rie de mod\xe8les de langage Qwen, dot\xe9e d\'une architecture dense et de m\xe9lange d\'experts (MoE), offrant d\'excellentes performances en mati\xe8re de raisonnement, de support multilingue et de t\xe2ches avanc\xe9es d\'agent. Sa capacit\xe9 unique \xe0 passer sans effort entre un mode de pens\xe9e pour le raisonnement complexe et un mode non-pensant pour des dialogues efficaces garantit des performances polyvalentes et de haute qualit\xe9.\\n\\nQwen3 surpasse de mani\xe8re significative les mod\xe8les pr\xe9c\xe9dents tels que QwQ et Qwen2.5, offrant des capacit\xe9s exceptionnelles en math\xe9matiques, en codage, en raisonnement de bon sens, en \xe9criture cr\xe9ative et en dialogue interactif. La variante Qwen3-30B-A3B contient 30,5 milliards de param\xe8tres (3,3 milliards de param\xe8tres activ\xe9s), 48 couches, 128 experts (8 activ\xe9s par t\xe2che) et prend en charge un contexte allant jusqu\'\xe0 131K tokens (utilisant YaRN), \xe9tablissant une nouvelle norme pour les mod\xe8les open source."},"qwen/qwen3-32b":{"description":"Qwen3-32B est un mod\xe8le de langage causal dense de 32 milliards de param\xe8tres dans la s\xe9rie Qwen3, optimis\xe9 pour un raisonnement complexe et des dialogues efficaces. Il permet un passage sans effort entre un mode de pens\xe9e pour des t\xe2ches telles que les math\xe9matiques, le codage et le raisonnement logique, et un mode non-pensant pour des dialogues plus rapides et g\xe9n\xe9raux. Ce mod\xe8le montre de solides performances dans le suivi des instructions, l\'utilisation d\'outils d\'agents, l\'\xe9criture cr\xe9ative et des t\xe2ches multilingues dans plus de 100 langues et dialectes. Il g\xe8re nativement un contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via une extension bas\xe9e sur YaRN."},"qwen/qwen3-32b:free":{"description":"Qwen3-32B est un mod\xe8le de langage causal dense de 32 milliards de param\xe8tres dans la s\xe9rie Qwen3, optimis\xe9 pour un raisonnement complexe et des dialogues efficaces. Il permet un passage sans effort entre un mode de pens\xe9e pour des t\xe2ches telles que les math\xe9matiques, le codage et le raisonnement logique, et un mode non-pensant pour des dialogues plus rapides et g\xe9n\xe9raux. Ce mod\xe8le montre de solides performances dans le suivi des instructions, l\'utilisation d\'outils d\'agents, l\'\xe9criture cr\xe9ative et des t\xe2ches multilingues dans plus de 100 langues et dialectes. Il g\xe8re nativement un contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via une extension bas\xe9e sur YaRN."},"qwen/qwen3-8b:free":{"description":"Qwen3-8B est un mod\xe8le de langage causal dense de 8 milliards de param\xe8tres dans la s\xe9rie Qwen3, con\xe7u pour des t\xe2ches intensives en raisonnement et des dialogues efficaces. Il permet un passage sans effort entre un mode de pens\xe9e pour les math\xe9matiques, le codage et le raisonnement logique, et un mode non-pensant pour des dialogues g\xe9n\xe9raux. Ce mod\xe8le a \xe9t\xe9 affin\xe9 pour le suivi des instructions, l\'int\xe9gration d\'agents, l\'\xe9criture cr\xe9ative et l\'utilisation multilingue dans plus de 100 langues et dialectes. Il prend en charge nativement une fen\xeatre de contexte de 32K tokens et peut \xeatre \xe9tendu \xe0 131K tokens via YaRN."},"qwen2":{"description":"Qwen2 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle d\'Alibaba, offrant d\'excellentes performances pour des besoins d\'application diversifi\xe9s."},"qwen2.5":{"description":"Qwen2.5 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle de Alibaba, offrant d\'excellentes performances pour r\xe9pondre \xe0 des besoins d\'application diversifi\xe9s."},"qwen2.5-14b-instruct":{"description":"Le mod\xe8le de 14B de Tongyi Qwen 2.5, open source."},"qwen2.5-14b-instruct-1m":{"description":"Le mod\xe8le de 72B de Qwen2.5 est ouvert au public."},"qwen2.5-32b-instruct":{"description":"Le mod\xe8le de 32B de Tongyi Qwen 2.5, open source."},"qwen2.5-72b-instruct":{"description":"Le mod\xe8le de 72B de Tongyi Qwen 2.5, open source."},"qwen2.5-7b-instruct":{"description":"Qwen2.5 7B Instruct, un mod\xe8le open source mature bas\xe9 sur des instructions, adapt\xe9 aux dialogues et \xe0 la g\xe9n\xe9ration dans divers contextes."},"qwen2.5-coder-1.5b-instruct":{"description":"Version open-source du mod\xe8le de code Qwen."},"qwen2.5-coder-14b-instruct":{"description":"Version open source du mod\xe8le de code Tongyi Qianwen."},"qwen2.5-coder-32b-instruct":{"description":"Version open source du mod\xe8le de code Qwen universel."},"qwen2.5-coder-7b-instruct":{"description":"Version open source du mod\xe8le de code Tongyi Qwen."},"qwen2.5-coder-instruct":{"description":"Qwen2.5-Coder est le dernier mod\xe8le de langage de grande taille sp\xe9cialis\xe9 dans le code de la s\xe9rie Qwen (anciennement connu sous le nom de CodeQwen)."},"qwen2.5-instruct":{"description":"Qwen2.5 est la derni\xe8re s\xe9rie de mod\xe8les de langage \xe0 grande \xe9chelle Qwen. Pour Qwen2.5, nous avons publi\xe9 plusieurs mod\xe8les de langage de base et des mod\xe8les de langage affin\xe9s par instruction, avec des param\xe8tres allant de 0,5 \xe0 72 milliards."},"qwen2.5-math-1.5b-instruct":{"description":"Le mod\xe8le Qwen-Math poss\xe8de de puissantes capacit\xe9s de r\xe9solution de probl\xe8mes math\xe9matiques."},"qwen2.5-math-72b-instruct":{"description":"Le mod\xe8le Qwen-Math poss\xe8de de puissantes capacit\xe9s de r\xe9solution de probl\xe8mes math\xe9matiques."},"qwen2.5-math-7b-instruct":{"description":"Le mod\xe8le Qwen-Math poss\xe8de de puissantes capacit\xe9s de r\xe9solution de probl\xe8mes math\xe9matiques."},"qwen2.5-omni-7b":{"description":"La s\xe9rie de mod\xe8les Qwen-Omni prend en charge l\'entr\xe9e de donn\xe9es multimodales, y compris des vid\xe9os, de l\'audio, des images et du texte, et produit de l\'audio et du texte en sortie."},"qwen2.5-vl-32b-instruct":{"description":"Qwen2.5 VL 32B Instruct, un mod\xe8le multimodal open source, adapt\xe9 au d\xe9ploiement priv\xe9 et \xe0 des applications vari\xe9es."},"qwen2.5-vl-72b-instruct":{"description":"Am\xe9lioration globale des capacit\xe9s de suivi des instructions, math\xe9matiques, r\xe9solution de probl\xe8mes et code, am\xe9lioration des capacit\xe9s de reconnaissance, support de divers formats pour un positionnement pr\xe9cis des \xe9l\xe9ments visuels, compr\xe9hension de fichiers vid\xe9o longs (jusqu\'\xe0 10 minutes) et localisation d\'\xe9v\xe9nements en temps r\xe9el, capable de comprendre l\'ordre temporel et la vitesse, supportant le contr\xf4le d\'agents OS ou Mobile bas\xe9 sur des capacit\xe9s d\'analyse et de localisation, avec une forte capacit\xe9 d\'extraction d\'informations cl\xe9s et de sortie au format Json. Cette version est la version 72B, la plus puissante de cette s\xe9rie."},"qwen2.5-vl-7b-instruct":{"description":"Qwen2.5 VL 7B Instruct, un mod\xe8le multimodal l\xe9ger, \xe9quilibrant co\xfbt de d\xe9ploiement et capacit\xe9 de reconnaissance."},"qwen2.5-vl-instruct":{"description":"Qwen2.5-VL est la derni\xe8re version du mod\xe8le de langage visuel de la famille de mod\xe8les Qwen."},"qwen2.5:0.5b":{"description":"Qwen2.5 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle de Alibaba, offrant d\'excellentes performances pour r\xe9pondre \xe0 des besoins d\'application diversifi\xe9s."},"qwen2.5:1.5b":{"description":"Qwen2.5 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle de Alibaba, offrant d\'excellentes performances pour r\xe9pondre \xe0 des besoins d\'application diversifi\xe9s."},"qwen2.5:72b":{"description":"Qwen2.5 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle de Alibaba, offrant d\'excellentes performances pour r\xe9pondre \xe0 des besoins d\'application diversifi\xe9s."},"qwen2:0.5b":{"description":"Qwen2 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle d\'Alibaba, offrant d\'excellentes performances pour des besoins d\'application diversifi\xe9s."},"qwen2:1.5b":{"description":"Qwen2 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle d\'Alibaba, offrant d\'excellentes performances pour des besoins d\'application diversifi\xe9s."},"qwen2:72b":{"description":"Qwen2 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle d\'Alibaba, offrant d\'excellentes performances pour des besoins d\'application diversifi\xe9s."},"qwen3":{"description":"Qwen3 est le nouveau mod\xe8le de langage \xe0 grande \xe9chelle d\'Alibaba, offrant d\'excellentes performances pour r\xe9pondre \xe0 des besoins d\'application diversifi\xe9s."},"qwen3-0.6b":{"description":"Qwen3 0.6B, un mod\xe8le d’entr\xe9e de gamme, adapt\xe9 au raisonnement simple et aux environnements \xe0 ressources tr\xe8s limit\xe9es."},"qwen3-1.7b":{"description":"Qwen3 1.7B, un mod\xe8le ultra-l\xe9ger, facile \xe0 d\xe9ployer sur les dispositifs en p\xe9riph\xe9rie ou terminaux."},"qwen3-14b":{"description":"Qwen3 14B, un mod\xe8le de taille moyenne, adapt\xe9 aux questions-r\xe9ponses multilingues et \xe0 la g\xe9n\xe9ration de texte."},"qwen3-235b-a22b":{"description":"Qwen3 235B A22B, un grand mod\xe8le g\xe9n\xe9raliste, con\xe7u pour une vari\xe9t\xe9 de t\xe2ches complexes."},"qwen3-235b-a22b-instruct-2507":{"description":"Qwen3 235B A22B Instruct 2507, un mod\xe8le Instruct phare, adapt\xe9 \xe0 la g\xe9n\xe9ration et au raisonnement dans divers sc\xe9narios."},"qwen3-235b-a22b-thinking-2507":{"description":"Qwen3 235B A22B Thinking 2507, un mod\xe8le de raisonnement \xe0 tr\xe8s grande \xe9chelle, con\xe7u pour les t\xe2ches de r\xe9flexion avanc\xe9e."},"qwen3-30b-a3b":{"description":"Qwen3 30B A3B, un mod\xe8le g\xe9n\xe9raliste de taille moyenne \xe0 grande, \xe9quilibrant co\xfbt et performance."},"qwen3-30b-a3b-instruct-2507":{"description":"Qwen3 30B A3B Instruct 2507, un mod\xe8le Instruct de taille moyenne \xe0 grande, adapt\xe9 \xe0 la g\xe9n\xe9ration de haute qualit\xe9 et aux questions-r\xe9ponses."},"qwen3-30b-a3b-thinking-2507":{"description":"Qwen3 30B A3B Thinking 2507, un mod\xe8le de r\xe9flexion de taille moyenne \xe0 grande, alliant pr\xe9cision et ma\xeetrise des co\xfbts."},"qwen3-32b":{"description":"Qwen3 32B, un mod\xe8le g\xe9n\xe9raliste adapt\xe9 aux t\xe2ches n\xe9cessitant une compr\xe9hension approfondie."},"qwen3-4b":{"description":"Qwen3 4B, adapt\xe9 aux applications de petite \xe0 moyenne taille et aux sc\xe9narios d’inf\xe9rence locale."},"qwen3-8b":{"description":"Qwen3 8B, un mod\xe8le l\xe9ger et flexible, id\xe9al pour les services \xe0 haute concurrence."},"qwen3-coder-30b-a3b-instruct":{"description":"Version open source du mod\xe8le de g\xe9n\xe9ration de code Qwen. Le dernier qwen3-coder-30b-a3b-instruct, bas\xe9 sur Qwen3, est un mod\xe8le puissant de g\xe9n\xe9ration de code dot\xe9 de capacit\xe9s avanc\xe9es d’agent de codage. Il excelle dans l’appel d’outils et l’interaction avec l’environnement, permettant une programmation autonome tout en conservant des comp\xe9tences g\xe9n\xe9rales solides."},"qwen3-coder-480b-a35b-instruct":{"description":"Qwen3 Coder 480B A35B Instruct, un mod\xe8le de codage de niveau phare, prenant en charge la programmation multilingue et la compr\xe9hension de code complexe."},"qwen3-coder-flash":{"description":"Mod\xe8le de code Tongyi Qianwen. La derni\xe8re s\xe9rie de mod\xe8les Qwen3-Coder est bas\xe9e sur Qwen3 pour la g\xe9n\xe9ration de code, avec une puissante capacit\xe9 d\'agent de codage, ma\xeetrisant l\'appel d\'outils et l\'interaction avec l\'environnement, capable de programmation autonome, alliant excellence en codage et polyvalence."},"qwen3-coder-plus":{"description":"Mod\xe8le de code Tongyi Qianwen. La derni\xe8re s\xe9rie de mod\xe8les Qwen3-Coder est bas\xe9e sur Qwen3 pour la g\xe9n\xe9ration de code, avec une puissante capacit\xe9 d\'agent de codage, ma\xeetrisant l\'appel d\'outils et l\'interaction avec l\'environnement, capable de programmation autonome, alliant excellence en codage et polyvalence."},"qwen3-coder:480b":{"description":"Mod\xe8le \xe0 contexte long haute performance d\'Alibaba, con\xe7u pour les t\xe2ches d\'agents et de codage."},"qwen3-max":{"description":"La s\xe9rie Max de Tongyi Qianwen 3 offre une am\xe9lioration significative par rapport \xe0 la s\xe9rie 2.5 en termes de capacit\xe9s g\xe9n\xe9rales. Elle renforce notablement la compr\xe9hension du texte en chinois et en anglais, la capacit\xe9 \xe0 suivre des instructions complexes, les t\xe2ches ouvertes subjectives, les comp\xe9tences multilingues et l\'appel d\'outils ; le mod\xe8le pr\xe9sente \xe9galement moins d\'hallucinations de connaissances. La derni\xe8re version du mod\xe8le qwen3-max, compar\xe9e \xe0 la version preview, a b\xe9n\xe9fici\xe9 d\'une mise \xe0 niveau sp\xe9cifique en programmation d\'agents et en appel d\'outils. La version officielle publi\xe9e atteint un niveau SOTA dans son domaine, adapt\xe9e aux besoins plus complexes des agents intelligents."},"qwen3-max-preview":{"description":"Le mod\xe8le le plus performant de la s\xe9rie Tongyi Qianwen, adapt\xe9 aux t\xe2ches complexes et \xe0 \xe9tapes multiples. La version de pr\xe9visualisation prend d\xe9j\xe0 en charge la r\xe9flexion."},"qwen3-next-80b-a3b-instruct":{"description":"Mod\xe8le open source de nouvelle g\xe9n\xe9ration en mode non r\xe9flexif bas\xe9 sur Qwen3, offrant une meilleure compr\xe9hension du texte en chinois, des capacit\xe9s de raisonnement logique renforc\xe9es et de meilleures performances dans les t\xe2ches de g\xe9n\xe9ration de texte par rapport \xe0 la version pr\xe9c\xe9dente (Tongyi Qianwen 3-235B-A22B-Instruct-2507)."},"qwen3-next-80b-a3b-thinking":{"description":"Qwen3 Next 80B A3B Thinking, une version de raisonnement phare con\xe7ue pour les t\xe2ches complexes."},"qwen3-omni-flash":{"description":"Le mod\xe8le Qwen-Omni accepte des entr\xe9es combin\xe9es de texte, image, audio et vid\xe9o, et g\xe9n\xe8re des r\xe9ponses sous forme de texte ou de voix. Il propose plusieurs voix synth\xe9tiques r\xe9alistes, prend en charge plusieurs langues et dialectes, et peut \xeatre utilis\xe9 dans des sc\xe9narios tels que la cr\xe9ation de contenu, la reconnaissance visuelle et les assistants vocaux."},"qwen3-vl-235b-a22b-instruct":{"description":"Qwen3 VL 235B A22B Instruct, un mod\xe8le multimodal phare, destin\xe9 aux sc\xe9narios exigeants de compr\xe9hension et de cr\xe9ation."},"qwen3-vl-235b-a22b-thinking":{"description":"Qwen3 VL 235B A22B Thinking, version de r\xe9flexion phare, con\xe7ue pour le raisonnement et la planification multimodale complexes."},"qwen3-vl-30b-a3b-instruct":{"description":"Qwen3 VL 30B A3B Instruct, un grand mod\xe8le multimodal, \xe9quilibrant pr\xe9cision et performance de raisonnement."},"qwen3-vl-30b-a3b-thinking":{"description":"Qwen3 VL 30B A3B Thinking, version de r\xe9flexion approfondie pour les t\xe2ches multimodales complexes."},"qwen3-vl-32b-instruct":{"description":"Qwen3 VL 32B Instruct, un mod\xe8le multimodal affin\xe9 par instruction, adapt\xe9 aux questions-r\xe9ponses image-texte de haute qualit\xe9 et \xe0 la cr\xe9ation."},"qwen3-vl-32b-thinking":{"description":"Qwen3 VL 32B Thinking, version de r\xe9flexion multimodale, renfor\xe7ant le raisonnement complexe et l’analyse \xe0 long terme."},"qwen3-vl-8b-instruct":{"description":"Qwen3 VL 8B Instruct, un mod\xe8le multimodal l\xe9ger, id\xe9al pour les questions-r\xe9ponses visuelles quotidiennes et l’int\xe9gration applicative."},"qwen3-vl-8b-thinking":{"description":"Qwen3 VL 8B Thinking, un mod\xe8le de cha\xeene de pens\xe9e multimodale, adapt\xe9 au raisonnement d\xe9taill\xe9 sur les informations visuelles."},"qwen3-vl-flash":{"description":"Qwen3 VL Flash : version all\xe9g\xe9e \xe0 inf\xe9rence rapide, id\xe9ale pour les sc\xe9narios sensibles \xe0 la latence ou les requ\xeates en grand volume."},"qwen3-vl-plus":{"description":"Tongyi Qianwen VL est un mod\xe8le de g\xe9n\xe9ration de texte dot\xe9 de capacit\xe9s de compr\xe9hension visuelle (images). Il peut non seulement effectuer de l\'OCR (reconnaissance de texte sur images), mais aussi r\xe9sumer et raisonner davantage, par exemple extraire des attributs \xe0 partir de photos de produits ou r\xe9soudre des exercices \xe0 partir d\'images."},"qwq":{"description":"QwQ est un mod\xe8le de recherche exp\xe9rimental, ax\xe9 sur l\'am\xe9lioration des capacit\xe9s de raisonnement de l\'IA."},"qwq-32b":{"description":"Le mod\xe8le d\'inf\xe9rence QwQ, entra\xeen\xe9 sur le mod\xe8le Qwen2.5-32B, a consid\xe9rablement am\xe9lior\xe9 ses capacit\xe9s d\'inf\xe9rence gr\xe2ce \xe0 l\'apprentissage par renforcement. Les indicateurs cl\xe9s du mod\xe8le, tels que le code math\xe9matique (AIME 24/25, LiveCodeBench) ainsi que certains indicateurs g\xe9n\xe9raux (IFEval, LiveBench, etc.), atteignent le niveau de la version compl\xe8te de DeepSeek-R1, avec des performances nettement sup\xe9rieures \xe0 celles de DeepSeek-R1-Distill-Qwen-32B, \xe9galement bas\xe9 sur Qwen2.5-32B."},"qwq-32b-preview":{"description":"Le mod\xe8le QwQ est un mod\xe8le de recherche exp\xe9rimental d\xe9velopp\xe9 par l\'\xe9quipe Qwen, ax\xe9 sur l\'am\xe9lioration des capacit\xe9s de raisonnement de l\'IA."},"qwq-plus":{"description":"Mod\xe8le d’inf\xe9rence QwQ entra\xeen\xe9 sur la base du mod\xe8le Qwen2.5, avec un renforcement par apprentissage qui am\xe9liore consid\xe9rablement les capacit\xe9s de raisonnement. Les indicateurs cl\xe9s en math\xe9matiques et code (AIME 24/25, LiveCodeBench) ainsi que certains indicateurs g\xe9n\xe9raux (IFEval, LiveBench, etc.) atteignent le niveau complet de DeepSeek-R1."},"qwq_32b":{"description":"Mod\xe8le de raisonnement de taille moyenne de la s\xe9rie Qwen. Compar\xe9 aux mod\xe8les d\'ajustement d\'instructions traditionnels, QwQ, avec ses capacit\xe9s de r\xe9flexion et de raisonnement, peut consid\xe9rablement am\xe9liorer les performances dans les t\xe2ches en aval, en particulier lors de la r\xe9solution de probl\xe8mes difficiles."},"r1-1776":{"description":"R1-1776 est une version du mod\xe8le DeepSeek R1, apr\xe8s un entra\xeenement suppl\xe9mentaire, fournissant des informations factuelles non filtr\xe9es et impartiales."},"solar-mini":{"description":"Solar Mini est un LLM compact, offrant des performances sup\xe9rieures \xe0 celles de GPT-3.5, avec de puissantes capacit\xe9s multilingues, prenant en charge l\'anglais et le cor\xe9en, et fournissant une solution efficace et compacte."},"solar-mini-ja":{"description":"Solar Mini (Ja) \xe9tend les capacit\xe9s de Solar Mini, se concentrant sur le japonais tout en maintenant une efficacit\xe9 et des performances exceptionnelles dans l\'utilisation de l\'anglais et du cor\xe9en."},"solar-pro":{"description":"Solar Pro est un LLM hautement intelligent lanc\xe9 par Upstage, ax\xe9 sur la capacit\xe9 de suivi des instructions sur un seul GPU, avec un score IFEval sup\xe9rieur \xe0 80. Actuellement, il supporte l\'anglais, et la version officielle est pr\xe9vue pour novembre 2024, avec une extension du support linguistique et de la longueur du contexte."},"sonar":{"description":"Produit de recherche l\xe9ger bas\xe9 sur le contexte de recherche, plus rapide et moins cher que Sonar Pro."},"sonar-deep-research":{"description":"Deep Research effectue des recherches approfondies de niveau expert et les synth\xe9tise en rapports accessibles et exploitables."},"sonar-pro":{"description":"Produit de recherche avanc\xe9 prenant en charge le contexte de recherche, avec des requ\xeates avanc\xe9es et un suivi."},"sonar-reasoning":{"description":"Nouveau produit API soutenu par le mod\xe8le de raisonnement DeepSeek."},"sonar-reasoning-pro":{"description":"Nouveau produit API soutenu par le mod\xe8le de raisonnement DeepSeek."},"stable-diffusion-3-medium":{"description":"Le dernier grand mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte lanc\xe9 par Stability AI. Cette version am\xe9liore significativement la qualit\xe9 d\'image, la compr\xe9hension du texte et la diversit\xe9 des styles, tout en h\xe9ritant des avantages des versions pr\xe9c\xe9dentes. Il interpr\xe8te plus pr\xe9cis\xe9ment les invites en langage naturel complexes et g\xe9n\xe8re des images plus pr\xe9cises et vari\xe9es."},"stable-diffusion-3.5-large":{"description":"stable-diffusion-3.5-large est un mod\xe8le de g\xe9n\xe9ration d\'images \xe0 partir de texte multimodal \xe0 base de transformateur de diffusion (MMDiT) avec 800 millions de param\xe8tres, offrant une qualit\xe9 d\'image exceptionnelle et une correspondance pr\xe9cise aux invites, capable de g\xe9n\xe9rer des images haute r\xe9solution jusqu\'\xe0 1 million de pixels, tout en fonctionnant efficacement sur du mat\xe9riel grand public."},"stable-diffusion-3.5-large-turbo":{"description":"stable-diffusion-3.5-large-turbo est un mod\xe8le bas\xe9 sur stable-diffusion-3.5-large utilisant la technique de distillation par diffusion antagoniste (ADD), offrant une vitesse accrue."},"stable-diffusion-v1.5":{"description":"stable-diffusion-v1.5 est initialis\xe9 avec les poids du checkpoint stable-diffusion-v1.2 et affin\xe9 pendant 595k \xe9tapes \xe0 une r\xe9solution de 512x512 sur \\"laion-aesthetics v2 5+\\", avec une r\xe9duction de 10 % de la condition textuelle pour am\xe9liorer l\'\xe9chantillonnage guid\xe9 sans classificateur."},"stable-diffusion-xl":{"description":"stable-diffusion-xl apporte des am\xe9liorations majeures par rapport \xe0 la version v1.5, avec des performances comparables au mod\xe8le open source SOTA midjourney. Les am\xe9liorations incluent un backbone unet trois fois plus grand, un module de raffinement pour am\xe9liorer la qualit\xe9 des images g\xe9n\xe9r\xe9es, et des techniques d\'entra\xeenement plus efficaces."},"stable-diffusion-xl-base-1.0":{"description":"Grand mod\xe8le open source de g\xe9n\xe9ration d\'images \xe0 partir de texte d\xe9velopp\xe9 par Stability AI, avec des capacit\xe9s cr\xe9atives de premier plan dans l\'industrie. Il poss\xe8de une excellente compr\xe9hension des instructions et supporte la d\xe9finition de prompts invers\xe9s pour une g\xe9n\xe9ration pr\xe9cise du contenu."},"step-1-128k":{"description":"\xc9quilibre entre performance et co\xfbt, adapt\xe9 \xe0 des sc\xe9narios g\xe9n\xe9raux."},"step-1-256k":{"description":"Capacit\xe9 de traitement de contexte ultra long, particuli\xe8rement adapt\xe9 \xe0 l\'analyse de documents longs."},"step-1-32k":{"description":"Prend en charge des dialogues de longueur moyenne, adapt\xe9 \xe0 divers sc\xe9narios d\'application."},"step-1-8k":{"description":"Mod\xe8le de petite taille, adapt\xe9 aux t\xe2ches l\xe9g\xe8res."},"step-1-flash":{"description":"Mod\xe8le \xe0 haute vitesse, adapt\xe9 aux dialogues en temps r\xe9el."},"step-1.5v-mini":{"description":"Ce mod\xe8le poss\xe8de de puissantes capacit\xe9s de compr\xe9hension vid\xe9o."},"step-1o-turbo-vision":{"description":"Ce mod\xe8le poss\xe8de de puissantes capacit\xe9s de compr\xe9hension d\'image, surpassant le 1o dans les domaines math\xe9matiques et de codage. Le mod\xe8le est plus petit que le 1o et offre une vitesse de sortie plus rapide."},"step-1o-vision-32k":{"description":"Ce mod\xe8le poss\xe8de de puissantes capacit\xe9s de compr\xe9hension d\'image. Par rapport \xe0 la s\xe9rie de mod\xe8les step-1v, il offre des performances visuelles sup\xe9rieures."},"step-1v-32k":{"description":"Prend en charge les entr\xe9es visuelles, am\xe9liorant l\'exp\xe9rience d\'interaction multimodale."},"step-1v-8k":{"description":"Mod\xe8le visuel compact, adapt\xe9 aux t\xe2ches de base en texte et image."},"step-1x-edit":{"description":"Ce mod\xe8le est sp\xe9cialis\xe9 dans les t\xe2ches d\'\xe9dition d\'images, capable de modifier et d\'am\xe9liorer des images selon les descriptions textuelles et les images fournies par l\'utilisateur. Il supporte plusieurs formats d\'entr\xe9e, comprenant descriptions textuelles et images d\'exemple. Le mod\xe8le comprend l\'intention de l\'utilisateur et g\xe9n\xe8re des r\xe9sultats d\'\xe9dition conformes aux exigences."},"step-1x-medium":{"description":"Ce mod\xe8le poss\xe8de de puissantes capacit\xe9s de g\xe9n\xe9ration d\'images, supportant les descriptions textuelles comme entr\xe9e. Il offre un support natif du chinois, permettant une meilleure compr\xe9hension et traitement des descriptions textuelles en chinois, capturant plus pr\xe9cis\xe9ment la s\xe9mantique pour la transformer en caract\xe9ristiques d\'image, r\xe9alisant ainsi une g\xe9n\xe9ration d\'images plus pr\xe9cise. Le mod\xe8le g\xe9n\xe8re des images haute r\xe9solution et de haute qualit\xe9, avec une certaine capacit\xe9 de transfert de style."},"step-2-16k":{"description":"Prend en charge des interactions contextuelles \xe0 grande \xe9chelle, adapt\xe9 aux sc\xe9narios de dialogue complexes."},"step-2-16k-exp":{"description":"Version exp\xe9rimentale du mod\xe8le step-2, contenant les derni\xe8res fonctionnalit\xe9s, en cours de mise \xe0 jour. Non recommand\xe9 pour une utilisation en production officielle."},"step-2-mini":{"description":"Un mod\xe8le de grande taille ultra-rapide bas\xe9 sur la nouvelle architecture d\'attention auto-d\xe9velopp\xe9e MFA, atteignant des r\xe9sultats similaires \xe0 ceux de step1 \xe0 un co\xfbt tr\xe8s bas, tout en maintenant un d\xe9bit plus \xe9lev\xe9 et un temps de r\xe9ponse plus rapide. Capable de traiter des t\xe2ches g\xe9n\xe9rales, avec des comp\xe9tences particuli\xe8res en mati\xe8re de codage."},"step-2x-large":{"description":"Mod\xe8le de nouvelle g\xe9n\xe9ration Step Star, sp\xe9cialis\xe9 dans la g\xe9n\xe9ration d\'images, capable de cr\xe9er des images de haute qualit\xe9 \xe0 partir de descriptions textuelles fournies par l\'utilisateur. Le nouveau mod\xe8le produit des images avec une texture plus r\xe9aliste et une meilleure capacit\xe9 de g\xe9n\xe9ration de texte en chinois et en anglais."},"step-3":{"description":"Ce mod\xe8le dispose d\'une puissante perception visuelle et d\'une capacit\xe9 de raisonnement complexe. Il peut accomplir avec pr\xe9cision la compr\xe9hension de connaissances complexes inter-domaines, l\'analyse crois\xe9e d\'informations math\xe9matiques et visuelles, ainsi que divers probl\xe8mes d\'analyse visuelle rencontr\xe9s dans la vie quotidienne."},"step-r1-v-mini":{"description":"Ce mod\xe8le est un grand mod\xe8le de raisonnement avec de puissantes capacit\xe9s de compr\xe9hension d\'image, capable de traiter des informations visuelles et textuelles, produisant du texte apr\xe8s une r\xe9flexion approfondie. Ce mod\xe8le se distingue dans le domaine du raisonnement visuel, tout en poss\xe9dant des capacit\xe9s de raisonnement math\xe9matique, de code et de texte de premier plan. La longueur du contexte est de 100k."},"step3":{"description":"Step3 est un mod\xe8le multimodal d\xe9velopp\xe9 par StepStar, dot\xe9 de puissantes capacit\xe9s de compr\xe9hension visuelle."},"stepfun-ai/step3":{"description":"Step3 est un mod\xe8le de raisonnement multimodal de pointe publi\xe9 par StepFun (阶跃星辰). Il est construit sur une architecture Mixture-of-Experts (MoE) comportant 321 milliards de param\xe8tres au total et 38 milliards de param\xe8tres d\'activation. Le mod\xe8le adopte une conception bout en bout visant \xe0 minimiser le co\xfbt de d\xe9codage tout en offrant des performances de premier plan en raisonnement visuel et linguistique. Gr\xe2ce \xe0 la conception synergique de l\'attention par d\xe9composition multi-matrice (MFA) et du d\xe9couplage attention‑FFN (AFD), Step3 conserve une grande efficacit\xe9 aussi bien sur des acc\xe9l\xe9rateurs haut de gamme que sur des acc\xe9l\xe9rateurs d\'entr\xe9e de gamme. Lors de la pr\xe9‑entra\xeenement, Step3 a trait\xe9 plus de 20 000 milliards de tokens textuels et 4 000 milliards de tokens mixtes image‑texte, couvrant une dizaine de langues. Le mod\xe8le atteint des niveaux de r\xe9f\xe9rence parmi les meilleurs des mod\xe8les open source sur plusieurs benchmarks, notamment en math\xe9matiques, en code et en multimodalit\xe9."},"taichu_llm":{"description":"Le mod\xe8le de langage Taichu Zidong poss\xe8de une forte capacit\xe9 de compr\xe9hension linguistique ainsi que des comp\xe9tences en cr\xe9ation de texte, questions-r\xe9ponses, programmation, calcul math\xe9matique, raisonnement logique, analyse des sentiments, et r\xe9sum\xe9 de texte. Il combine de mani\xe8re innovante le pr\xe9-entra\xeenement sur de grandes donn\xe9es avec des connaissances riches provenant de multiples sources, en perfectionnant continuellement la technologie algorithmique et en int\xe9grant de nouvelles connaissances sur le vocabulaire, la structure, la grammaire et le sens \xe0 partir de vastes ensembles de donn\xe9es textuelles, offrant aux utilisateurs des informations et des services plus pratiques ainsi qu\'une exp\xe9rience plus intelligente."},"taichu_o1":{"description":"taichu_o1 est un nouveau mod\xe8le de raisonnement de grande taille, r\xe9alisant une cha\xeene de pens\xe9e semblable \xe0 celle des humains gr\xe2ce \xe0 des interactions multimodales et \xe0 l\'apprentissage par renforcement, prenant en charge des d\xe9ductions de d\xe9cisions complexes, tout en montrant des chemins de raisonnement mod\xe9lis\xe9s avec une sortie de haute pr\xe9cision, adapt\xe9 \xe0 des sc\xe9narios d\'analyse strat\xe9gique et de r\xe9flexion approfondie."},"taichu_vl":{"description":"Int\xe8gre des capacit\xe9s de compr\xe9hension d\'image, de transfert de connaissances et de raisonnement logique, se distinguant dans le domaine des questions-r\xe9ponses textuelles et visuelles."},"tencent/Hunyuan-A13B-Instruct":{"description":"Hunyuan-A13B-Instruct compte 80 milliards de param\xe8tres, avec seulement 13 milliards activ\xe9s pour rivaliser avec des mod\xe8les plus grands, supportant un raisonnement hybride \xab pens\xe9e rapide/pens\xe9e lente \xbb ; compr\xe9hension stable des textes longs ; valid\xe9 par BFCL-v3 et τ-Bench, ses capacit\xe9s d’agent sont en avance ; combinant GQA et plusieurs formats de quantification, il r\xe9alise un raisonnement efficace."},"tencent/Hunyuan-MT-7B":{"description":"Le mod\xe8le de traduction Hunyuan est compos\xe9 du mod\xe8le Hunyuan-MT-7B et du mod\xe8le int\xe9gr\xe9 Hunyuan-MT-Chimera. Hunyuan-MT-7B est un mod\xe8le de traduction l\xe9ger avec 7 milliards de param\xe8tres, con\xe7u pour traduire des textes sources vers des langues cibles. Il prend en charge la traduction entre 33 langues ainsi que 5 langues des minorit\xe9s ethniques chinoises. Lors du concours international de traduction automatique WMT25, Hunyuan-MT-7B a obtenu la premi\xe8re place dans 30 des 31 cat\xe9gories linguistiques auxquelles il a particip\xe9, d\xe9montrant ses capacit\xe9s de traduction exceptionnelles. Pour les sc\xe9narios de traduction, Tencent Hunyuan a propos\xe9 un paradigme d\'entra\xeenement complet allant de la pr\xe9-formation \xe0 l\'ajustement supervis\xe9, puis au renforcement par traduction et \xe0 l\'int\xe9gration renforc\xe9e, atteignant des performances de pointe parmi les mod\xe8les de taille \xe9quivalente. Ce mod\xe8le est efficace en calcul, facile \xe0 d\xe9ployer et adapt\xe9 \xe0 divers cas d\'utilisation."},"text-embedding-3-large":{"description":"Le mod\xe8le de vectorisation le plus puissant, adapt\xe9 aux t\xe2ches en anglais et non-anglais."},"text-embedding-3-small":{"description":"Un mod\xe8le d\'Embedding de nouvelle g\xe9n\xe9ration, efficace et \xe9conomique, adapt\xe9 \xe0 la recherche de connaissances, aux applications RAG, etc."},"thudm/glm-4-32b":{"description":"GLM-4-32B-0414 est un mod\xe8le de langage \xe0 poids ouvert de 32B bilingue (chinois-anglais), optimis\xe9 pour la g\xe9n\xe9ration de code, les appels de fonctions et les t\xe2ches d\'agents. Il a \xe9t\xe9 pr\xe9-entra\xeen\xe9 sur 15T de donn\xe9es de haute qualit\xe9 et de r\xe9inf\xe9rence, et perfectionn\xe9 par un alignement des pr\xe9f\xe9rences humaines, un \xe9chantillonnage de rejet et un apprentissage par renforcement. Ce mod\xe8le excelle dans le raisonnement complexe, la g\xe9n\xe9ration d\'artefacts et les t\xe2ches de sortie structur\xe9e, atteignant des performances comparables \xe0 celles de GPT-4o et DeepSeek-V3-0324 dans plusieurs tests de r\xe9f\xe9rence."},"thudm/glm-4-32b:free":{"description":"GLM-4-32B-0414 est un mod\xe8le de langage \xe0 poids ouvert de 32B bilingue (chinois-anglais), optimis\xe9 pour la g\xe9n\xe9ration de code, les appels de fonctions et les t\xe2ches d\'agents. Il a \xe9t\xe9 pr\xe9-entra\xeen\xe9 sur 15T de donn\xe9es de haute qualit\xe9 et de r\xe9inf\xe9rence, et perfectionn\xe9 par un alignement des pr\xe9f\xe9rences humaines, un \xe9chantillonnage de rejet et un apprentissage par renforcement. Ce mod\xe8le excelle dans le raisonnement complexe, la g\xe9n\xe9ration d\'artefacts et les t\xe2ches de sortie structur\xe9e, atteignant des performances comparables \xe0 celles de GPT-4o et DeepSeek-V3-0324 dans plusieurs tests de r\xe9f\xe9rence."},"thudm/glm-4-9b-chat":{"description":"Version open source de la derni\xe8re g\xe9n\xe9ration de mod\xe8les pr\xe9-entra\xeen\xe9s de la s\xe9rie GLM-4 publi\xe9e par Zhizhu AI."},"thudm/glm-z1-32b":{"description":"GLM-Z1-32B-0414 est une variante de raisonnement am\xe9lior\xe9e de GLM-4-32B, construite pour r\xe9soudre des probl\xe8mes de math\xe9matiques profondes, de logique et orient\xe9s code. Il applique un apprentissage par renforcement \xe9tendu (sp\xe9cifique \xe0 la t\xe2che et bas\xe9 sur des pr\xe9f\xe9rences par paires g\xe9n\xe9rales) pour am\xe9liorer les performances sur des t\xe2ches complexes \xe0 plusieurs \xe9tapes. Par rapport au mod\xe8le de base GLM-4-32B, Z1 am\xe9liore consid\xe9rablement les capacit\xe9s de raisonnement structur\xe9 et de domaine formel.\\n\\nCe mod\xe8le prend en charge l\'ex\xe9cution des \xe9tapes de \'pens\xe9e\' via l\'ing\xe9nierie des invites et offre une coh\xe9rence am\xe9lior\xe9e pour les sorties au format long. Il est optimis\xe9 pour les flux de travail d\'agents et prend en charge un long contexte (via YaRN), des appels d\'outils JSON et une configuration d\'\xe9chantillonnage de granularit\xe9 fine pour un raisonnement stable. Id\xe9al pour les cas d\'utilisation n\xe9cessitant une r\xe9flexion approfondie, un raisonnement \xe0 plusieurs \xe9tapes ou une d\xe9duction formelle."},"thudm/glm-z1-rumination-32b":{"description":"THUDM : GLM Z1 Rumination 32B est un mod\xe8le de raisonnement profond de 32 milliards de param\xe8tres dans la s\xe9rie GLM-4-Z1, optimis\xe9 pour des t\xe2ches complexes et ouvertes n\xe9cessitant une r\xe9flexion prolong\xe9e. Il est construit sur la base de glm-4-32b-0414, ajoutant une phase d\'apprentissage par renforcement suppl\xe9mentaire et une strat\xe9gie d\'alignement multi-\xe9tapes, introduisant une capacit\xe9 de \\"r\xe9flexion\\" destin\xe9e \xe0 simuler un traitement cognitif \xe9tendu. Cela inclut un raisonnement it\xe9ratif, une analyse multi-sauts et des flux de travail am\xe9lior\xe9s par des outils, tels que la recherche, la r\xe9cup\xe9ration et la synth\xe8se consciente des citations.\\n\\nCe mod\xe8le excelle dans l\'\xe9criture de recherche, l\'analyse comparative et les questions complexes. Il prend en charge les appels de fonction pour les primitives de recherche et de navigation (`search`, `click`, `open`, `finish`), permettant son utilisation dans des pipelines d\'agents. Le comportement de r\xe9flexion est fa\xe7onn\xe9 par un contr\xf4le cyclique multi-tours avec des r\xe9compenses bas\xe9es sur des r\xe8gles et un m\xe9canisme de d\xe9cision diff\xe9r\xe9e, et est \xe9talonn\xe9 sur des cadres de recherche approfondie tels que la pile d\'alignement interne d\'OpenAI. Cette variante est adapt\xe9e aux sc\xe9narios n\xe9cessitant de la profondeur plut\xf4t que de la vitesse."},"tngtech/deepseek-r1t-chimera:free":{"description":"DeepSeek-R1T-Chimera est cr\xe9\xe9 en combinant DeepSeek-R1 et DeepSeek-V3 (0324), alliant la capacit\xe9 de raisonnement de R1 et les am\xe9liorations d\'efficacit\xe9 des tokens de V3. Il est bas\xe9 sur l\'architecture DeepSeek-MoE Transformer et optimis\xe9 pour des t\xe2ches g\xe9n\xe9rales de g\xe9n\xe9ration de texte.\\n\\nCe mod\xe8le fusionne les poids pr\xe9-entra\xeen\xe9s des deux mod\xe8les sources pour \xe9quilibrer les performances en raisonnement, en efficacit\xe9 et en suivi des instructions. Il est publi\xe9 sous la licence MIT, destin\xe9 \xe0 un usage de recherche et commercial."},"togethercomputer/StripedHyena-Nous-7B":{"description":"StripedHyena Nous (7B) offre une capacit\xe9 de calcul am\xe9lior\xe9e gr\xe2ce \xe0 des strat\xe9gies et une architecture de mod\xe8le efficaces."},"tts-1":{"description":"Le dernier mod\xe8le de synth\xe8se vocale, optimis\xe9 pour la vitesse dans des sc\xe9narios en temps r\xe9el."},"tts-1-hd":{"description":"Le dernier mod\xe8le de synth\xe8se vocale, optimis\xe9 pour la qualit\xe9."},"upstage/SOLAR-10.7B-Instruct-v1.0":{"description":"Upstage SOLAR Instruct v1 (11B) est adapt\xe9 aux t\xe2ches d\'instructions d\xe9taill\xe9es, offrant d\'excellentes capacit\xe9s de traitement du langage."},"us.anthropic.claude-3-5-sonnet-20241022-v2:0":{"description":"Claude 3.5 Sonnet \xe9l\xe8ve les normes de l\'industrie, surpassant les mod\xe8les concurrents et Claude 3 Opus, avec d\'excellentes performances dans une large gamme d\'\xe9valuations, tout en offrant la vitesse et le co\xfbt de nos mod\xe8les de niveau interm\xe9diaire."},"us.anthropic.claude-3-7-sonnet-20250219-v1:0":{"description":"Claude 3.7 sonnet est le mod\xe8le de prochaine g\xe9n\xe9ration le plus rapide d\'Anthropic. Par rapport \xe0 Claude 3 Haiku, Claude 3.7 Sonnet a am\xe9lior\xe9 ses comp\xe9tences dans divers domaines et a surpass\xe9 le plus grand mod\xe8le de la g\xe9n\xe9ration pr\xe9c\xe9dente, Claude 3 Opus, dans de nombreux tests de r\xe9f\xe9rence intellectuels."},"us.anthropic.claude-haiku-4-5-20251001-v1:0":{"description":"Claude Haiku 4.5 est le mod\xe8le Haiku le plus rapide et le plus intelligent d’Anthropic, offrant une vitesse fulgurante et une capacit\xe9 de raisonnement \xe9tendue."},"us.anthropic.claude-sonnet-4-5-20250929-v1:0":{"description":"Claude Sonnet 4.5 est le mod\xe8le le plus intelligent jamais con\xe7u par Anthropic."},"v0-1.0-md":{"description":"Le mod\xe8le v0-1.0-md est une version ancienne propos\xe9e via l\'API v0"},"v0-1.5-lg":{"description":"Le mod\xe8le v0-1.5-lg est adapt\xe9 aux t\xe2ches de r\xe9flexion avanc\xe9e ou de raisonnement"},"v0-1.5-md":{"description":"Le mod\xe8le v0-1.5-md convient aux t\xe2ches quotidiennes et \xe0 la g\xe9n\xe9ration d\'interfaces utilisateur (UI)"},"vercel/v0-1.0-md":{"description":"Acc\xe8s au mod\xe8le derri\xe8re v0 pour g\xe9n\xe9rer, r\xe9parer et optimiser des applications Web modernes, avec raisonnement sp\xe9cifique aux frameworks et connaissances \xe0 jour."},"vercel/v0-1.5-md":{"description":"Acc\xe8s au mod\xe8le derri\xe8re v0 pour g\xe9n\xe9rer, r\xe9parer et optimiser des applications Web modernes, avec raisonnement sp\xe9cifique aux frameworks et connaissances \xe0 jour."},"wan2.2-t2i-flash":{"description":"Version ultra-rapide Wanxiang 2.2, le mod\xe8le le plus r\xe9cent \xe0 ce jour. Am\xe9liorations globales en cr\xe9ativit\xe9, stabilit\xe9 et r\xe9alisme, avec une vitesse de g\xe9n\xe9ration rapide et un excellent rapport qualit\xe9-prix."},"wan2.2-t2i-plus":{"description":"Version professionnelle Wanxiang 2.2, le mod\xe8le le plus r\xe9cent \xe0 ce jour. Am\xe9liorations globales en cr\xe9ativit\xe9, stabilit\xe9 et r\xe9alisme, avec des d\xe9tails de g\xe9n\xe9ration riches."},"wanx-v1":{"description":"Mod\xe8le de base de g\xe9n\xe9ration d\'images \xe0 partir de texte, correspondant au mod\xe8le g\xe9n\xe9ral 1.0 officiel de Tongyi Wanxiang."},"wanx2.0-t2i-turbo":{"description":"Sp\xe9cialis\xe9 dans les portraits r\xe9alistes, vitesse moyenne et co\xfbt r\xe9duit. Correspond au mod\xe8le ultra-rapide 2.0 officiel de Tongyi Wanxiang."},"wanx2.1-t2i-plus":{"description":"Version enti\xe8rement am\xe9lior\xe9e. G\xe9n\xe8re des images avec des d\xe9tails plus riches, vitesse l\xe9g\xe8rement plus lente. Correspond au mod\xe8le professionnel 2.1 officiel de Tongyi Wanxiang."},"wanx2.1-t2i-turbo":{"description":"Version enti\xe8rement am\xe9lior\xe9e. Vitesse de g\xe9n\xe9ration rapide, r\xe9sultats complets, excellent rapport qualit\xe9-prix. Correspond au mod\xe8le ultra-rapide 2.1 officiel de Tongyi Wanxiang."},"whisper-1":{"description":"Mod\xe8le universel de reconnaissance vocale, prenant en charge la reconnaissance vocale multilingue, la traduction vocale et la reconnaissance de langue."},"wizardlm2":{"description":"WizardLM 2 est un mod\xe8le de langage propos\xe9 par Microsoft AI, particuli\xe8rement performant dans les domaines des dialogues complexes, du multilinguisme, du raisonnement et des assistants intelligents."},"wizardlm2:8x22b":{"description":"WizardLM 2 est un mod\xe8le de langage propos\xe9 par Microsoft AI, particuli\xe8rement performant dans les domaines des dialogues complexes, du multilinguisme, du raisonnement et des assistants intelligents."},"x-ai/grok-4-fast":{"description":"Nous sommes ravis de pr\xe9senter Grok 4 Fast, notre derni\xe8re avanc\xe9e en mati\xe8re de mod\xe8les de raisonnement rentables."},"x-ai/grok-code-fast-1":{"description":"Nous sommes heureux de lancer grok-code-fast-1, un mod\xe8le de raisonnement rapide et \xe9conomique, particuli\xe8rement performant pour le codage assist\xe9 par agent."},"x1":{"description":"Le mod\xe8le Spark X1 sera mis \xe0 niveau, et sur la base de ses performances d\xe9j\xe0 leaders dans les t\xe2ches math\xe9matiques, il atteindra des r\xe9sultats comparables dans des t\xe2ches g\xe9n\xe9rales telles que le raisonnement, la g\xe9n\xe9ration de texte et la compr\xe9hension du langage, en se mesurant \xe0 OpenAI o1 et DeepSeek R1."},"xai/grok-2":{"description":"Grok 2 est un mod\xe8le de langage de pointe avec des capacit\xe9s de raisonnement avanc\xe9es. Il excelle en chat, codage et raisonnement, surpassant Claude 3.5 Sonnet et GPT-4-Turbo dans le classement LMSYS."},"xai/grok-2-vision":{"description":"Le mod\xe8le visuel Grok 2 excelle dans les t\xe2ches bas\xe9es sur la vision, offrant des performances de pointe en raisonnement math\xe9matique visuel (MathVista) et en questions-r\xe9ponses bas\xe9es sur documents (DocVQA). Il peut traiter diverses informations visuelles, y compris documents, graphiques, diagrammes, captures d\'\xe9cran et photos."},"xai/grok-3":{"description":"Mod\xe8le phare de xAI, performant pour les cas d\'usage d\'entreprise tels que l\'extraction de donn\xe9es, le codage et le r\xe9sum\xe9 de texte. Il poss\xe8de une expertise approfondie dans les domaines financier, m\xe9dical, juridique et scientifique."},"xai/grok-3-fast":{"description":"Mod\xe8le phare de xAI, performant pour les cas d\'usage d\'entreprise tels que l\'extraction de donn\xe9es, le codage et le r\xe9sum\xe9 de texte. La variante rapide est servie sur une infrastructure plus rapide, offrant des temps de r\xe9ponse bien sup\xe9rieurs au standard, au co\xfbt accru par token de sortie."},"xai/grok-3-mini":{"description":"Mod\xe8le l\xe9ger de xAI, r\xe9fl\xe9chissant avant de r\xe9pondre. Id\xe9al pour les t\xe2ches simples ou logiques ne n\xe9cessitant pas une expertise approfondie. La trajectoire de pens\xe9e brute est accessible."},"xai/grok-3-mini-fast":{"description":"Mod\xe8le l\xe9ger de xAI, r\xe9fl\xe9chissant avant de r\xe9pondre. Id\xe9al pour les t\xe2ches simples ou logiques ne n\xe9cessitant pas une expertise approfondie. La trajectoire de pens\xe9e brute est accessible. La variante rapide est servie sur une infrastructure plus rapide, offrant des temps de r\xe9ponse bien sup\xe9rieurs au standard, au co\xfbt accru par token de sortie."},"xai/grok-4":{"description":"Le dernier et meilleur mod\xe8le phare de xAI, offrant des performances in\xe9gal\xe9es en langage naturel, math\xe9matiques et raisonnement — un v\xe9ritable mod\xe8le polyvalent."},"yi-large":{"description":"Un mod\xe8le de nouvelle g\xe9n\xe9ration avec des milliards de param\xe8tres, offrant des capacit\xe9s de question-r\xe9ponse et de g\xe9n\xe9ration de texte exceptionnelles."},"yi-large-fc":{"description":"Bas\xe9 sur le mod\xe8le yi-large, il prend en charge et renforce les capacit\xe9s d\'appel d\'outils, adapt\xe9 \xe0 divers sc\xe9narios d\'affaires n\xe9cessitant la cr\xe9ation d\'agents ou de workflows."},"yi-large-preview":{"description":"Version pr\xe9liminaire, il est recommand\xe9 d\'utiliser yi-large (nouvelle version)."},"yi-large-rag":{"description":"Un service de haut niveau bas\xe9 sur le mod\xe8le yi-large, combinant des techniques de recherche et de g\xe9n\xe9ration pour fournir des r\xe9ponses pr\xe9cises, avec un service de recherche d\'informations en temps r\xe9el sur le web."},"yi-large-turbo":{"description":"Un excellent rapport qualit\xe9-prix avec des performances exceptionnelles. Optimis\xe9 pour un \xe9quilibre de haute pr\xe9cision en fonction des performances, de la vitesse de raisonnement et des co\xfbts."},"yi-lightning":{"description":"Mod\xe8le haute performance dernier cri, garantissant une sortie de haute qualit\xe9 tout en am\xe9liorant consid\xe9rablement la vitesse d\'inf\xe9rence."},"yi-lightning-lite":{"description":"Version all\xe9g\xe9e, l\'utilisation de yi-lightning est recommand\xe9e."},"yi-medium":{"description":"Mod\xe8le de taille moyenne, optimis\xe9 et ajust\xe9, offrant un \xe9quilibre de capacit\xe9s et un bon rapport qualit\xe9-prix. Optimisation approfondie des capacit\xe9s de suivi des instructions."},"yi-medium-200k":{"description":"Fen\xeatre de contexte ultra longue de 200K, offrant une compr\xe9hension et une g\xe9n\xe9ration de texte en profondeur."},"yi-spark":{"description":"Petit mais puissant, un mod\xe8le l\xe9ger et rapide. Offre des capacit\xe9s renforc\xe9es en calcul math\xe9matique et en r\xe9daction de code."},"yi-vision":{"description":"Mod\xe8le pour des t\xe2ches visuelles complexes, offrant des capacit\xe9s de compr\xe9hension et d\'analyse d\'images de haute performance."},"yi-vision-v2":{"description":"Mod\xe8le pour des t\xe2ches visuelles complexes, offrant des capacit\xe9s de compr\xe9hension et d\'analyse de haute performance bas\xe9es sur plusieurs images."},"z-ai/glm-4.6":{"description":"GLM-4.6, le tout dernier mod\xe8le phare de Zhipu, surpasse son pr\xe9d\xe9cesseur dans les domaines du codage avanc\xe9, du traitement de longs textes, du raisonnement et des capacit\xe9s d\'agents intelligents."},"zai-org/GLM-4.5":{"description":"GLM-4.5 est un mod\xe8le de base con\xe7u pour les applications d\'agents intelligents, utilisant une architecture Mixture-of-Experts (MoE). Il est profond\xe9ment optimis\xe9 pour l\'appel d\'outils, la navigation web, l\'ing\xe9nierie logicielle et la programmation front-end, supportant une int\xe9gration transparente avec des agents de code tels que Claude Code et Roo Code. GLM-4.5 utilise un mode d\'inf\xe9rence hybride, adapt\xe9 \xe0 des sc\xe9narios vari\xe9s allant du raisonnement complexe \xe0 l\'usage quotidien."},"zai-org/GLM-4.5-Air":{"description":"GLM-4.5-Air est un mod\xe8le de base con\xe7u pour les applications d\'agents intelligents, utilisant une architecture Mixture-of-Experts (MoE). Il est profond\xe9ment optimis\xe9 pour l\'appel d\'outils, la navigation web, l\'ing\xe9nierie logicielle et la programmation front-end, supportant une int\xe9gration transparente avec des agents de code tels que Claude Code et Roo Code. GLM-4.5 utilise un mode d\'inf\xe9rence hybride, adapt\xe9 \xe0 des sc\xe9narios vari\xe9s allant du raisonnement complexe \xe0 l\'usage quotidien."},"zai-org/GLM-4.5V":{"description":"GLM-4.5V est la derni\xe8re g\xe9n\xe9ration de mod\xe8le langage-visuel (VLM) publi\xe9e par Zhipu AI. Ce mod\xe8le est construit sur le mod\xe8le texte phare GLM-4.5-Air, qui compte 106 milliards de param\xe8tres au total et 12 milliards de param\xe8tres d\'activation, et adopte une architecture de mixture d\'experts (MoE) afin d\'obtenir des performances excellentes \xe0 un co\xfbt d\'inf\xe9rence r\xe9duit. Sur le plan technique, GLM-4.5V prolonge la lign\xe9e de GLM-4.1V-Thinking et introduit des innovations telles que l\'encodage de position rotatif en 3D (3D-RoPE), renfor\xe7ant de fa\xe7on significative la perception et le raisonnement des relations spatiales tridimensionnelles. Gr\xe2ce aux optimisations apport\xe9es lors des phases de pr\xe9-entra\xeenement, d\'affinage supervis\xe9 et d\'apprentissage par renforcement, ce mod\xe8le est capable de traiter divers contenus visuels, notamment des images, des vid\xe9os et des documents longs, et atteint un niveau de pointe parmi les mod\xe8les open source de la m\xeame cat\xe9gorie sur 41 benchmarks multimodaux publics. De plus, le mod\xe8le int\xe8gre un interrupteur \xab mode r\xe9flexion \xbb permettant aux utilisateurs de choisir de mani\xe8re flexible entre r\xe9ponses rapides et raisonnement approfondi, pour \xe9quilibrer efficacit\xe9 et qualit\xe9."},"zai-org/GLM-4.6":{"description":"Par rapport \xe0 GLM-4.5, GLM-4.6 apporte plusieurs am\xe9liorations cl\xe9s. Sa fen\xeatre contextuelle s’\xe9tend de 128K \xe0 200K tokens, permettant au mod\xe8le de g\xe9rer des t\xe2ches d’agents plus complexes. Le mod\xe8le obtient de meilleurs scores aux benchmarks de code et montre des performances renforc\xe9es dans des applications telles que Claude Code, Cline, Roo Code et Kilo Code, notamment dans la g\xe9n\xe9ration de pages frontales visuellement raffin\xe9es. GLM-4.6 am\xe9liore nettement les performances d’inf\xe9rence et supporte l’utilisation d’outils durant l’inf\xe9rence, offrant ainsi une capacit\xe9 globale renforc\xe9e. Il excelle dans l’utilisation d’outils et les agents bas\xe9s sur la recherche, et s’int\xe8gre plus efficacement dans les cadres d’agents. En \xe9criture, ce mod\xe8le correspond davantage aux pr\xe9f\xe9rences humaines en termes de style et de lisibilit\xe9, et se comporte de mani\xe8re plus naturelle dans les sc\xe9narios de jeu de r\xf4le."},"zai/glm-4.5":{"description":"La s\xe9rie de mod\xe8les GLM-4.5 est con\xe7ue sp\xe9cifiquement pour les agents. Le mod\xe8le phare GLM-4.5 int\xe8gre 355 milliards de param\xe8tres totaux (32 milliards actifs), unifiant raisonnement, codage et capacit\xe9s d\'agent pour r\xe9pondre \xe0 des besoins applicatifs complexes. En tant que syst\xe8me de raisonnement hybride, il offre deux modes d\'op\xe9ration."},"zai/glm-4.5-air":{"description":"GLM-4.5 et GLM-4.5-Air sont nos derniers mod\xe8les phares, con\xe7us comme mod\xe8les de base pour les applications d\'agents. Les deux utilisent une architecture d\'experts hybrides (MoE). GLM-4.5 compte 355 milliards de param\xe8tres totaux avec 32 milliards actifs par passage avant, tandis que GLM-4.5-Air adopte une conception plus simplifi\xe9e avec 106 milliards de param\xe8tres totaux et 12 milliards actifs."},"zai/glm-4.5v":{"description":"GLM-4.5V est construit sur le mod\xe8le de base GLM-4.5-Air, h\xe9ritant des techniques \xe9prouv\xe9es de GLM-4.1V-Thinking, tout en r\xe9alisant une mise \xe0 l\'\xe9chelle efficace gr\xe2ce \xe0 une puissante architecture MoE de 106 milliards de param\xe8tres."}}')}}]);